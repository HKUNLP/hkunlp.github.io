<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Dream-Coder 7B | HKU NLP Group </title> <meta name="author" content="HKU NLP Group "> <meta name="description" content="Introducing Dream-Coder 7B, the most powerful open diffusion large language model for code to date."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hkunlp.github.io/blog/2025/dream-coder/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"]],tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Dream-Coder 7B",
      "description": "Introducing Dream-Coder 7B, the most powerful open diffusion large language model for code to date.",
      "published": "July 15, 2025",
      "authors": [
        {
          "author": "Zhihui Xie",
          "authorURL": "https://zhxie.site/",
          "affiliations": [
            {
              "name": "University of Hong Kong",
              "url": ""
            }
          ]
        }
        
      ]
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">HKU NLP Group¬†</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/seminar/">HKUNLP Seminar</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Dream-Coder 7B</h1> <p>Introducing Dream-Coder 7B, the most powerful open diffusion large language model for code to date.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introducing-dream-coder-7b">Introducing Dream-Coder 7B</a></div> <div><a href="#features">Features</a></div> <div><a href="#adaptation">Adaptation</a></div> <div><a href="#post-training">Post-training</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <p><strong>Team</strong>: Zhihui Xie*, Jiacheng Ye*, Lin Zheng*, Jiahui Gao*, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li‚Ä†, and Lingpeng Kong‚Ä†</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Huawei Noah‚Äôs Ark Lab</p> <p>(*Equal contribution. ‚Ä†Core advising. )</p> <h2 id="introducing-dream-coder-7b">Introducing Dream-Coder 7B</h2> <p>In a joint effort with Huawei Noah‚Äôs Ark Lab, we release <strong>Dream-Coder 7B</strong>, the first fully open-source <strong>diffusion LLM for code</strong> that provides complete transparency throughout its development pipeline, with all components publicly available ‚Äì data processing scripts, implementation code, and model weights.</p> <p>Text diffusion models represent a fundamental shift away from autoregressive LLMs. This emerging direction has attracted significant attention across academia and industry<d-cite key="dream2025,nie2025large,khanna2025mercury,geminidiffusion"></d-cite>, with startups like <a href="https://www.inceptionlabs.ai/introducing-mercury" rel="external nofollow noopener" target="_blank">Mercury</a> pioneering diffusion LLMs for code generation. Compared to autoregressive models, diffusion-based approaches offer greater generation diversity, improved robustness, and better capture of complex, multi-modal code structures. As a diffusion-based language model demonstrating competitive performance with autoregressive code LLMs at the same scale, Dream-Coder 7B Instruct achieves <strong>21.4% pass@1 on LiveCodeBench</strong> (2410-2505), a remarkable result given that it was <strong>trained exclusively on publicly available datasets</strong>.</p> <p><a href="https://github.com/DreamLM/Dream-Coder" rel="external nofollow noopener" target="_blank"><strong>üë®‚Äçüíª¬†Github</strong></a> <a href="https://huggingface.co/Dream-org/Dream-Coder-v0-Instruct-7B" rel="external nofollow noopener" target="_blank"><strong>ü§ó¬†HuggingFace</strong></a></p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/image.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Instruct model performance comparison on coding benchmarks. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-6"> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure1"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 1. Sketch-First Generation (from LiveCodeBench)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure4"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 4. Variable-Length Code Infilling I</figcaption> </figure> </div> </div> <div class="col-sm-6 "> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure2"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2. Left-to-Right Generation (from BigCodeBench)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure3"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 3. Interleaved Reasoning Generation (from CRUXEval)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure5"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 5. Variable-Length Code Infilling II</figcaption> </figure> </div> </div> </div> <h2 id="features">Features</h2> <h3 id="flexible-code-generation"><strong>Flexible Code Generation</strong></h3> <p>We observe Dream-Coder 7B exhibits emergent any-order generation that adaptively determines its decoding style based on the coding task. For example, Dream-Coder 7B Instruct displays patterns such as:</p> <ul> <li> <strong>Sketch-first generation</strong>: For problems that read inputs from standard input and write outputs to standard output (<a href="#figure1">Figure 1</a>), Dream-Coder 7B Instruct begins by sketching the entire entry-point scaffold, then works backward to implement and refine helper functions and core logic.</li> <li> <strong>Left-to-right generation</strong>: For single-function completions (<a href="#figure2">Figure 2</a>), Dream-Coder 7B Instruct writes almost linearly‚Äîstarting at the def header and moving left-to-right.</li> <li> <strong>Interleaved reasoning generation</strong>: For code reasoning tasks that require predicting output from code and input (<a href="#figure3">Figure 3</a>), Dream-Coder 7B Instruct first echoes the given input, then walks through the program step by step, jotting down each calculation and filling in output lines as soon as it figures them out.</li> </ul> <p>These demos were collected using consistent sampling parameters: <code class="language-plaintext highlighter-rouge">temperature=0.1, diffusion_steps=512, max_new_tokens=512, alg="entropy", top_p=1.0, alg_temp=0.0, and pad_penalty=3.0</code>.</p> <h3 id="variable-length-code-infilling"><strong>Variable-Length Code Infilling</strong></h3> <p>One of the biggest challenges for diffusion LLMs is their lack of natural capability to generate variable-length sequences. This limitation is particularly problematic for infilling‚Äîgenerating code that seamlessly fits between existing snippets. We introduce an infilling variant, <strong>DreamOn-7B</strong> , that naturally adjusts the length of masked spans during generation by introducing two special tokens, <code class="language-plaintext highlighter-rouge">&lt;|expand|&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;|delete|&gt;</code>, which dynamically expand or contract the mask region to match the required infill length (<a href="#figure4">Figure 4</a> and <a href="#figure5">Figure 5</a>). This capability allows the model to handle variable-length code infilling tasks more effectively, without prior knowledge of the target sequence length.</p> <p>For more details, please refer to our accompanying blog post for our variable-length generation method <a href="https://www.notion.so/228be544bdbb80cc991ef540e7805bd7?pvs=21" rel="external nofollow noopener" target="_blank">DreamOn</a>.</p> <h2 id="adaptation">Adaptation</h2> <p>Dream-Coder 7B belongs to the family of discrete diffusion models <d-cite key="zheng2023reparameterized"></d-cite> that generate tokens through denoising from mask tokens. Building on our previous work <d-cite key="dream2025,gong2024scaling"></d-cite>, we adapt from Qwen2.5-Coder 7B base <d-cite key="hui2024qwen2"></d-cite> using 322B training tokens. Our training data comprises a carefully curated mixture of code, math, and general datasets, including <a href="https://huggingface.co/collections/OpenCoder-LLM/opencoder-datasets-672e6db6a0fed24bd69ef1c2" rel="external nofollow noopener" target="_blank">OpenCoder</a>, <a href="https://huggingface.co/datasets/HuggingFaceTB/stack-edu" rel="external nofollow noopener" target="_blank">Stack-Edu</a>, <a href="https://huggingface.co/datasets/allenai/dolmino-mix-1124" rel="external nofollow noopener" target="_blank">Dolmino</a>, and¬†<a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0" rel="external nofollow noopener" target="_blank">DCLM-Baseline</a>. We apply Context-adaptive Token-level Noise Rescheduling introduced in Dream <d-cite key="dream2025"></d-cite> to dynamically adjust noise levels based on context complexity. Dream-Coder 7B is able to achieve top-tier coding performance among open autoregressive and diffusion language models, while possessing general language understanding, math, and science reasoning abilities.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/image1.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Base model performance comparison on coding benchmarks. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/image2.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Performance comparison of code base models regarding general, math and science reasoning abilities. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <h2 id="post-training">Post-training</h2> <p>Our post-training recipe consists of supervised fine-tuning and reinforcement learning from verifiable rewards.</p> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>We use 5 million high-quality training samples from <a href="https://huggingface.co/datasets/inclusionAI/Ling-Coder-SFT" rel="external nofollow noopener" target="_blank">Ling-Coder-SFT</a>, which includes a diverse range of programming tasks across multiple languages and corpora.</p> <p>In our early experiments, we observed low sample efficiency and generation instability issues due to losses on [PAD] tokens (used as end-of-sequence markers). Specifically, when applying a simple max-length padding strategy, we observed:</p> <ul> <li> <strong>Low sample efficiency</strong>: A large portion of compute is wasted on [PAD] tokens, dominating the loss and causing overfitting while slowing effective token learning.</li> <li> <strong>Generation instability</strong>: Since responses are all padded with [PAD], the model tended to produce short outputs during inference.</li> </ul> <p>To address these issues, we implement <strong>Random Truncation</strong> and <strong>[PAD] penalty</strong>. As illustrated below, we randomly select a sample from the batch and truncate responses based on its length during training. This improves sample efficiency and avoids over-padded outputs. During inference, we apply a penalty to the logits of the [PAD] token to prevent its premature generation. This penalty term is gradually annealed as decoding progresses. Through this mechanism, the model initially prioritizes generating meaningful tokens and considers termination in the later decoding stage. Additionally, as in adaptation training, we apply Context-adaptive Token-level Noise Rescheduling to dynamically adjust noise based on context complexity to improve training efficacy.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/image3.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Illustration of the random truncation technique. P: prompt; R: response; white areas: [PAD] tokens.</figcaption> </figure> </div> </div> <h3 id="reinforcement-learning-with-verifiable-rewards">Reinforcement Learning with Verifiable Rewards</h3> <p>Inspired by the success of RL with verifiable rewards <d-cite key="jaech2024openai,guo2025deepseek"></d-cite>, we further conduct RL training with open-source code datasets. We curate a high-quality training set from <a href="https://huggingface.co/datasets/KodCode/KodCode-V1" rel="external nofollow noopener" target="_blank">KodCode-V1</a>, <a href="https://huggingface.co/datasets/agentica-org/DeepCoder-Preview-Dataset" rel="external nofollow noopener" target="_blank">DeepCoder-Preview-Dataset</a>, and <a href="https://huggingface.co/datasets/LLM360/guru-RL-92k" rel="external nofollow noopener" target="_blank">guru-RL-92k</a>. We use Qwen2.5-Coder 7B Instruct to exclude prompts with entirely correct responses (out of 8 per prompt). To prevent reward hacking and ensure prompt diversity, we filter out samples with fewer than 5 unit tests and deduplicate similar prompts. The final training set contains 17k balanced prompts across function calling, standard I/O, and simulation tasks.</p> <p>We use the GRPO algorithm <d-cite key="shao2024deepseekmath"></d-cite> with several notable improvements inspired by prior work <d-cite key="yu2025dapo,gong2025diffucoder,Polaris2025"></d-cite>:</p> <ul> <li> <strong>No entropy &amp; KL loss</strong>: We eliminate entropy loss as it often results in training instability. Likewise, KL loss prevents exploration and requires additional computation for the reference policy.</li> <li> <strong>Clip-higher</strong>: We use an asymmetric clipping range for importance sampling ratios in policy updates, raising the upper bound to encourage exploration of low-probability tokens.</li> <li> <strong>Coupled sampling</strong>: For each batch, we sample complementary masks to estimate token-level log-likelihoods, which increases sample efficiency and reduces variance.</li> <li> <strong>Intra-batch informative substitution</strong>: For each batch, we randomly duplicate samples with nonzero advantages to replace those yielding zero advantage, ensuring every batch provides informative learning signals.</li> </ul> <p>To speed up training, we adopt Fast-dLLM <d-cite key="wu2025fastdllmtrainingfreeaccelerationdiffusion"></d-cite> for rollout generation. We use binary rewards of whether the generated code is formatted correctly and passes all unit tests, leveraging deployed sandboxes <d-cite key="liu2024fullstack"></d-cite> that execute code snippets in parallel for verification.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-1400.webp"></source> <img src="/assets/img/2025-07-15-dream-coder-imgs/image4.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"></figcaption> </figure> </div> </div> <p>Our final model, Dream-Coder 7B Instruct, delivers strong performance across standard benchmarks, including HumanEval, MBPP, BigCodeBench, LiveCodeBench, and CRUXEval. Notably, trained solely on publicly available data, Dream-Coder 7B Instruct outperforms OpenCoder 8B Instruct <d-cite key="Huang2024OpenCoderTO"></d-cite>, highlighting the competitiveness of diffusion LLMs over the autoregressive approach in coding tasks. On LiveCodeBench (2410-2505), our model achieves 21.4% pass@1, approaching the performance of proprietary models like Mercury Coder Small (22.9%).</p> <h2 id="conclusion">Conclusion</h2> <p>Dream-Coder 7B represents a continuation of our efforts to enhance open-source diffusion LLMs, with particular focus on post-training improvements. Trained entirely on open-source data, it delivers competitive performance in code generation. Future efforts will explore context extension and improved data curation to further boost Dream models‚Äô capabilities.</p> <h2 id="citation"><strong>Citation</strong></h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">dreamcoder2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Dream-Coder 7B}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/dream-coder}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Zhihui and Ye, Jiacheng and Zheng, Lin and Gao, Jiahui and Dong, Jingwei and Wu, Zirui and Zhao, Xueliang and Gong, Shansan and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 HKU NLP Group . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <d-bibliography src="/assets/bibliography/2025-07-15-dream-coder.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>