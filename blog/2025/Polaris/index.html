<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <hr> <p>layout: distill title: ‚ÄúPolaris‚Äù date: 2025-06-20 description: Introducing Polaris-4B-Preview and Polaris-7B-Preview, the most powerful open-recipe reasoning models to date. tags: reasoning-models scaling-RL</p> <h1 id="categories-sample-posts-external-services">categories: sample-posts external-services</h1> <p>authors:</p> <ul> <li>name: Chenxin An url: ‚Äúhttps://chenxinan-fdu.github.io/‚Äù affiliations: name: University of Hong Kong</li> </ul> <p>bibliography: 2025-06-20-Polaris.bib</p> <h1 id="optionally-you-can-add-a-table-of-contents-to-your-post">Optionally, you can add a table of contents to your post.</h1> <h1 id="notes">NOTES:</h1> <h1 id="--make-sure-that-toc-names-match-the-actual-section-names">- make sure that TOC names match the actual section names</h1> <h1 id="for-hyperlinks-within-the-post-to-work-correctly">for hyperlinks within the post to work correctly.</h1> <h1 id="--we-may-want-to-automate-toc-generation-in-the-future-using">- we may want to automate TOC generation in the future using</h1> <h1 id="jekyll-toc-plugin-httpsgithubcomtoshimarujekyll-toc">jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).</h1> <p>toc:</p> <ul> <li>name: 1. Data Difficulty</li> <li>name: 2. Diversity-based Rollout Sampling</li> <li>name: 3. Inference-Time Length Scaling</li> <li>name: 4. Exploration Efficiency</li> <li>name: 5. From DAPO and GRPO+</li> <li>name: 6. Reward Function</li> <li>name: Evaluation</li> </ul> <h1 id="below-is-an-example-of-injecting-additional-post-specific-styles">Below is an example of injecting additional post-specific styles.</h1> <h1 id="if-you-use-this-post-as-a-template-delete-this-_styles-block">If you use this post as a template, delete this _styles block.</h1> <h1 id="_styles-">_styles: &gt;</h1> <h1 id="fake-img-">.fake-img {</h1> <h1 id="background-bbb">background: #bbb;</h1> <h1 id="border-1px-solid-rgba0-0-0-01">border: 1px solid rgba(0, 0, 0, 0.1);</h1> <h1 id="box-shadow-0-0px-4px-rgba0-0-0-01">box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);</h1> <h1 id="margin-bottom-12px">margin-bottom: 12px;</h1> <h1>}</h1> <h1 id="fake-img-p-">.fake-img p {</h1> <h1 id="font-family-monospace">font-family: monospace;</h1> <h1 id="color-white">color: white;</h1> <h1 id="text-align-left">text-align: left;</h1> <h1 id="margin-12px-0">margin: 12px 0;</h1> <h1 id="text-align-center">text-align: center;</h1> <h1 id="font-size-16px">font-size: 16px;</h1> <h1 id="-1">}</h1> <hr> <h1 id="polaris-a-post-training-recipe-for-scaling-reinforcement-learning-on-advanced-reasoning-models">POLARIS: A POst-training recipe for scaling reinforcement Learning on Advanced ReasonIng modelS</h1> <p><strong>Team</strong>: Chenxin An *, Zhihui Xie‚Ä†, Xiaonan Li‚Ä†, Lei Li‚Ä†, Jun Zhang, Shansan Gong, Ming Zhong</p> <p>Jingjing Xu *, Xipeng Qiu, Mingxuan Wang, Lingpeng Kong</p> <p>*: Project Leads; ‚Ä†: Significant Contributor</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Bytedance Seed, Fudan University</p> <aside style="background-color: #f5f5f5; padding: 1em; border-radius: 8px;"> <p> We are thrilled to unveil our latest breakthroughs, <em>POLARIS-7B-Preview</em> and <em>POLARIS-4B-Preview</em>, which mark a new frontier in open‚Äêrecipe reasoning models developed using academic‚Äêlevel resources. <br> <em>POLARIS-4B-Preview</em> is fine-tuned from <em>Qwen3-4B</em> and <em>POLARIS-7B-Preview</em> is fine-tuned from <em>Deepseek-R1-Distill-Qwen-7B</em>. <br> Our 4B model achieves an impressive <strong>81.2% Pass@1 accuracy on AIME24</strong> and <strong>79.4% Pass@1 accuracy on AIME25</strong>, outperforming state-of-the-art commercial models like <em>Claude-4-Opus</em>, <em>Grok-3-Beta</em>, and <em>o3-mini-high(2025/01/31)</em> via scaling reinforcement learning on open-source data. On AIME25, POLARIS astonishingly achieves comparable performance to <em>Qwen3-235B-A22B</em> while using less than <strong>2%</strong> of its parameters and can be deployed on consumer-grade GPUs. </p> <p> To foster progress in scaling RL on advanced reasoning models, we are open-sourcing our dataset, code, and training details for the research community. </p> <p> üë®‚Äçüíª¬†<a href="https://github.com/HKUNLP/POLARIS" rel="external nofollow noopener" target="_blank">Github</a> | ü§ó¬†<a href="https://huggingface.co/POLARIS-HKU/Polaris-4B-Preview" rel="external nofollow noopener" target="_blank">HF Model</a> | ü§ó¬†<a href="https://huggingface.co/datasets/POLARIS-HKU/Polaris-Dataset-53K" rel="external nofollow noopener" target="_blank">HF Dataset</a> | üìñ <a href="comming%20soon">paper</a> | üîé¬†<a href="https://github.com/HKUNLP/POLARIS" rel="external nofollow noopener" target="_blank">Evaluation results</a> </p> </aside> <p><img src="assets/img/polaris-imgs/image.png" alt="image.png"></p> <p><em>‚úÖ Takeaways for post-training of advanced reasoning models</em></p> <blockquote> <ul> <li> <strong>Data Difficulty:</strong> Before training, Polaris analyzes and maps the distribution of data difficulty. The dataset should not be overwhelmed by either overly difficult or trivially easy problems. We recommend using a data distribution with a slight bias toward challenging problems, which typically exhibits a mirrored J-shaped distribution.</li> <li> <strong>Diversity-Based Rollout:</strong> We leverage the <em>diversity among rollouts</em> to initialize the sampling temperature, which is then progressively increased throughout the RL training stages.</li> <li> <strong>Inference-Time Length:</strong> Polaris incorporates length extrapolation techniques for generating longer CoT at inference stage. This enables a <em>‚Äútrain-short, generate-long‚Äù</em> paradigm for CoT reasoning, mitigating the computational burden of training with excessively long rollouts .</li> <li> <strong>Exploration Efficiency:</strong> Exploration efficiency in Polaris is enhanced through multi-stage training. However, reducing the model‚Äôs response length in the first stage poses potential risks. A more conservative approach would be to directly allow the model to ‚Äúthink longer‚Äù from the beginning.</li> </ul> </blockquote> <p><strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> was trained for 28 days on 32 H800 GPUs, using a batch size of 128, a rollout size of 8, and a dataset of 30K open-source examples. We have 3 training stages with sequence lengths scaling up (40K, 48K, 52K) across 700 steps. For complete training details, please refer to the <a href="https://www.notion.so/POLARIS-A-POst-training-recipe-for-scaling-reinforcement-Learning-on-Advanced-ReasonIng-modelS-1dfa954ff7c38094923ec7772bf447a1?pvs=21" rel="external nofollow noopener" target="_blank">released training scripts</a>.</p> <hr> <h1 id="polariss-recipe">POLARIS‚Äôs Recipe</h1> <p>Current work (e.g., <a href="https://www.notion.so/19681902c1468005bed8ca303013a4e2?pvs=21" rel="external nofollow noopener" target="_blank">DeepscaleR</a>) demonstrates that a small model (e.g., 1.5B parameters) can achieve surprising improvements in reasoning tasks through scaling RL training. However, when we apply their recipe to train more advanced reasoning models, we observe marginal improvements even decline during the RL training of <code class="language-plaintext highlighter-rouge">Qwen3</code>. This suggests a critical gap in the open-source community‚Äôs understanding of how to further scale RL on advanced reasoning models. To address this, we introduce <strong>POLARIS</strong>‚Äîa post-training recipe centered on calibrated data difficulty, enhanced data diversity, inference-time length scaling, and efficient training.</p> <p>We are committed to transparency and will be open-sourcing our trained models, training code, and data to foster community progress.</p> <h2 id="1-data-difficulty"><strong><em>1. Data Difficulty</em></strong></h2> <p>Our POLARIS recipe builds upon a deep investigation on the training data difficulty. Specifically, we conduct controlled experiments regarding data difficulty measured by model pass rate, and choose public available training datasets to enable better reproducibility.</p> <h3 id="balanced-data-difficulty-matters">Balanced Data Difficulty Matters</h3> <p>Our initial experiments involve training models of different scales on the public <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset" rel="external nofollow noopener" target="_blank">DeepScaleR dataset</a>. While a 1.5B model shows significant performance gains as expected, a 7B model trained on the same data exhibits only marginal improvements. We observe that the 7B model‚Äôs average reward quickly surpasses 0.7, indicating that the training set is too simple to drive further improvements.</p> <p>This leads us to a core hypothesis: <strong>For effective RL training, the difficulty of the data must be carefully calibrated to the model‚Äôs scale and capability.</strong></p> <p>To validate this, we analyze the difficulty distribution of the 40,000 samples in the DeepScaleR training set. We use <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> and its <code class="language-plaintext highlighter-rouge">1.5B</code> version to perform an offline evaluation, generating 8 solutions for each problem with a sampling temperature of 0.6. The percentage of correct solutions serves as a proxy for the difficulty of each sample.</p> <p>The results, shown in the figure below, are revealing.</p> <p><img src="assets/img/polaris-imgs/image%200.png" alt="image 0.png"></p> <blockquote> <p>Across both model scales, we observe that most problems are either very easy (8/8 correct solutions) or very hard (0/8 correct solutions).</p> </blockquote> <p>Crucially, we note a ‚Äúmirror effect‚Äù between the two models:</p> <ul> <li>The <strong>1.5B model</strong> shows a mirrored J-shaped (·Ç±) distribution, with most problems being extremely difficult (0/8 correct).</li> <li>The <strong>7B model</strong> shows a standard J-shaped distribution, with the vast majority of problems being far too easy (8/8 correct).</li> </ul> <p>This stark contrast confirms that the dataset, while challenging for a 1.5B model, is not sufficiently difficult to train a 7B model effectively. This insight motivates our work in curating a new, more challenging dataset tailored for advanced models.</p> <p>To confirm our hypothesis, we conduct an ablation study on the <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset" rel="external nofollow noopener" target="_blank">DeepScaleR 40K dataset</a> using the <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> model. We create distinct training sets by systematically altering the difficulty distribution:</p> <ol> <li> <strong>Full Dataset (40K samples):</strong> The dataset exhibits the original J-shaped distribution, dominated by easy samples (8/8 correct solutions).</li> <li> <strong>Removal of Perfect Scores (26K samples):</strong> We remove all samples with 8/8 correct solutions, creating a mirrored J-shaped distribution.</li> <li> <strong>Aggressive Filtering (19K samples):</strong> We filter out all samples with a pass rate greater than 4/8, resulting in an distribution that focuses only on the hardest problems.</li> </ol> <div align="center"> ![Figure 1: Model performance across the three above mentioned conditions](assets/img/polaris-imgs/image%201.png) <br> <sub>Figure 1: Model performance across the three above mentioned conditions</sub> </div> <p>The results, as shown in Figure¬†1, clearly demonstrate that removing the easiest samples leads to consistent performance improvements. In contrast, both the unfiltered dataset (which lacks sufficient challenge) and the aggressively filtered dataset (which is overly saturated with difficult problems) hinder training progress.</p> <p>These findings confirm that optimal RL training requires <strong>a balanced difficulty distribution</strong>‚Äîone that provides enough challenging samples to drive learning while avoiding both trivial problems and overwhelming difficulty.</p> <h3 id="polariss-data-curation-strategy"><strong>POLARIS‚Äôs Data Curation Strategy</strong></h3> <p>Motivated by these findings, the POLARIS recipe curates a <strong>mirrored J-shape difficulty distribution</strong> by filtering high-quality public datasets, including DeepScaleR-40K, and <a href="https://huggingface.co/datasets/inclusionAI/AReaL-boba-Data" rel="external nofollow noopener" target="_blank">AReaL-boba-106k</a>.</p> <p>Our data engineering process is as follows:</p> <ol> <li> <strong>Offline Difficulty Estimation:</strong> We use the specific model being trained to generate <code class="language-plaintext highlighter-rouge">8 rollouts</code> for each potential training problem. The pass rate determines the problem‚Äôs difficulty relative to that model.</li> <li> <strong>Targeted Filtering:</strong> To create the desired mirrored J-shape distribution, we remove all samples that the model solves perfectly (8/8 correct).</li> <li> <strong>Dataset Assembly and Calibration:</strong> <ul> <li>For training <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we applied this filtering to DeepScaleR and AReaL, creating a final training set of <strong>53K samples</strong> (26K from DeepScaleR and 27K from AReaL).</li> <li>To train the the <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> model, we performed an additional filtering pass on this 53K set, resulting in a <strong>30K sample</strong> dataset specifically calibrated to its difficulty level.</li> </ul> </li> </ol> <p>This model-specific calibration of data difficulty is a cornerstone of the POLARIS recipe, ensuring that the training process remains challenging and effective for any given model.</p> <h3 id="dynamically-drop-easy-data-during-training"><strong>Dynamically drop easy data during training</strong></h3> <p>As the RL training process progresses, the model‚Äôs capabilities will grow, and the proportion of difficult questions will decrease. Therefore, in addition to initially adjusting the difficulty distribution, we also adjust the training data distribution during the training process. Here is the figure showing the distribution shift of data difficulty during training.</p> <div align="center"> ![Figure 2: Data Difficulty Distribution Shifts (Left: Before Training, Right: After-Training; Top: Qwen3-4B, Bottom: Deepseek-R1-distill-Qwen-7B) ](assets/img/polaris-imgs/image%202.png) <br> <sub>Figure 2: Data Difficulty Distribution Shifts (Left: Before Training, Right: After-Training; Top: Qwen3-4B, Bottom: Deepseek-R1-distill-Qwen-7B) </sub> </div> <p>We observe that the sample difficulty distribution consistently shifts from a mirrored J-shape (·Ç±) to a J-shape. This evolution reinforces our motivation to start with a ·Ç±-shaped distribution, allowing for a smooth transition to the J-shape.</p> <p>Additionally, since data difficulty changes dynamically during training, using the initial dataset throughout is suboptimal. To maintain an ideal difficulty distribution, we introduce <strong>dynamic difficulty distribution updates</strong>: during training with a rollout size of n=8, we update each sample‚Äôs accuracy after reward computation (initial difficulty determined by offline filtering accuracy). <strong>At the end of each training phase, we remove samples with <code class="language-plaintext highlighter-rouge">Accuracy &gt; 0.9</code> to preserve the initial distribution shape and prevent skewing towards a J-shape<em>.</em></strong></p> <p>This dynamic filtering ensures that the model continues to face appropriately challenging samples, preventing the learning signal from degrading due to an overabundance of mastered samples.</p> <hr> <h2 id="2-diversity-based-rollout-sampling"><em>2. Diversity-based Rollout Sampling</em></h2> <p><a href="https://arxiv.org/abs/2501.12948" rel="external nofollow noopener" target="_blank">In GRPO training</a>, diversity is an essential factor. GRPO‚Äôs key is to contrast positive and negative trajectories, increasing the probability of positive ones. The diversity of sampled trajectories is crucial, encompassing two aspects:</p> <ol> <li>High diversity encourages the model to generate both positive and negative trajectories in a single rollout, enhancing trajectory contrast.</li> <li>High diversity allows the model to explore a wider range of potential reasoning paths, which helps prevent the model from quickly becoming overconfident in a narrow set of patterns. We also explore the method to increase the sampling diversity. Our approach aims to achieve the best diversity while ensuring performance.</li> </ol> <p>During the rollout phase, the primary hyperparameters affecting diversity are top‚Äëp,top‚Äëk, and the sampling temperature. In previous open‚Äësource projects, the default settings are typically a top‚Äëp value of 1.0 and a top‚Äëkvalue of ‚Äì1, which together yield maximum diversity. Only the sampling temperature remains adjustable. Temperature is usually established either by following the decoding temperature recommended on the official website (e.g., 0.6) or by setting it as a hyperparameter at 1.0. Therefore, in this section, we focus on temperature to analyze how the temperature affects the RL training performance and propose adjusting the temperature during training to match the base model‚Äôs diversity.</p> <p>We start from a probing experiment to explore the relationship between sampling temperature $t$, performance (mean@32), and diversity among rollouts. To quantify the diversity of the sampled trajectories, we use the distinct N-gram metric. This metric evaluates lexical diversity by measuring the ratio of unique N-grams (contiguous sequences of n words) to the total number of N-grams across all generated outputs. In our experiments we set N=4. A score closer to 1.0 indicates higher diversity, meaning the all trajectories contain a wide variety of phrases with little repetition. Conversely, a score closer to 0 indicates low diversity, suggesting the generated outputs are highly similar or repetitive.</p> <p><strong>Diversity vs. Sampling Temperature:</strong> Higher temperatures bring better diversity. To use more diverse trajectories for training, it is recommended to increase the sampling temperature. At the same temperature, there are significant differences in diversity performance across models. For instance, <code class="language-plaintext highlighter-rouge">Qwen3</code> has fewer unique n-grams and a more concentrated output distribution.</p> <div align="center"> ![Figure 3: Rollout diversity with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes](assets/img/polaris-imgs/image%203.png) &lt;/br&gt; <sub> Figure 3: Rollout diversity with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes </sub> </div> <p><strong>Performance vs. Sampling Temperature:</strong> While pursuing diverse trajectories, it is also necessary to ensure the model‚Äôs performance. When we increase the temperature from 0 to higher, all the tested models‚Äô average accuracy exhibits a low-high-low trend. We also notice that each model has significant differences in their zone spans, highlighting that there is no one-size-fits-all temperature setting . The optimal temperature for achieving a desired level of diversity is highly dependent on the specific model being used.</p> <div align="center"> ![Figure 4: Model performance with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes](assets/img/polaris-imgs/image%204.png) <br> <sub> Figure 4: Model performance with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes </sub> </div> <h3 id="definition-temperature-zone"> <strong>Definition</strong>: Temperature Zone</h3> <p>According to the trends, we can get the following sampling temperature zone empirically:</p> <ol> <li> <strong>Robust Generation Zone (RGZ)</strong>: RGZ defines the zone where the model‚Äôs performance is both <code class="language-plaintext highlighter-rouge">optimal</code>and <code class="language-plaintext highlighter-rouge">stable</code><strong>,</strong> without significant increases or decreases. The suggested decoding temperature is typically from RGZ.</li> <li> <strong>Controlled Exploration Zone(CEZ):</strong> Temperature in CEZ leads to slight performance degradation compared with RGZone, but the degradation level is acceptable because it leads to the increased rollout diversity.</li> <li> <p><strong>Performance Collapse Zone (PCZ)</strong>: In PCZ, the model tends to output noisy tokens and thus the performance will be extremely low.</p> <p><img src="assets/img/polaris-imgs/image%205.png" alt="image.png"></p> </li> </ol> <p>For illustration, We show <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>‚Äôs accuracy on AIME24 to demonstrate the differences among these three areas in the following figure. When temperature is 0.6~1.4, the model achieves the optimal and stable performance curve. When temperature is 1.4-1.55, the performance slightly degrades but has better rollout diversity. Then increasing temperature from 1.55 to 1.7 causes significant model performance collapse, which indicates the PCZone starts from temperature=1.55. Noise begins to appear, making it unsuitable for both training and decoding.</p> <hr> <h3 id="temperature-initialization-on-controlled-exploration-zone"><strong>Temperature initialization on Controlled Exploration Zone</strong></h3> <p>Our probing experiments reveal that sampling temperature significantly impacts rollout diversity, and its optimal setting varies across base models. The recommended test temperatures are usually from <strong><code class="language-plaintext highlighter-rouge">Robust Generation Zone</code></strong> , which usually result in low diversity. An overly deterministic sampling temperature restricts the model‚Äôs ability to explore better pattern spaces. In Polaris, we propose initializing the sampling temperature based on the model‚Äôs <strong><code class="language-plaintext highlighter-rouge">Controlled Exploration Zone</code></strong> to achieve comparable performance with improved diversity.</p> <p>We recommend using the sampling temperature at the point where model performance begins to decline while maximizing diversity. For Qwen3-4B and <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we set the initial sampling temperatures to 1.4 and 0.7, respectively.</p> <p>The comparison of different temperature initialization settings shows that the most common setting, <code class="language-plaintext highlighter-rouge">t= 0.6 / 1.0</code>, causes a decline in model performance, as it is too low to allow the model to explore better trajectories. In contrast, the temperature within the Controlled Exploration Zone demonstrates the best RL performance.</p> <div align="center"> ![ Figure 5: The performance trend of different sampling temperature initialization](assets/img/polaris-imgs/image%206.png) <br> <sub> Figure 5: The performance trend of different sampling temperature initialization </sub> </div> <h3 id="dynamic-temperature-reset-across-training-stages"><strong>Dynamic Temperature Reset Across Training Stages</strong></h3> <p>We also find that the model‚Äôs Robust Generation Zone and Controlled Exploration Zone shift during RL training (as shown in the following figure). Since reinforcement learning increases the probability of positive expression patterns, the model‚Äôs entropy tends to decrease and its exploration space becomes narrower, which is manifested by the convergence of N-grams in different trajectories.</p> <div align="center"> ![Figure 6: The RGZ and CEZ shift towards the high-temperature region following 800 steps of RL training.](assets/img/polaris-imgs/image%207.png) <br> <sub> Figure 6: The RGZ and CEZ shift towards the high-temperature region following 800 steps of RL training. </sub> </div> <div align="center"> ![Figure 7: Model performance and rollout diversity as a function of RL training steps. The experiment uses a training/testing temperature of 0.7 and a sampling size of 32 for testing.](assets/img/polaris-imgs/image%208.png) <br> <sub> Figure 7: Model performance and rollout diversity as a function of RL training steps. The experiment uses a training/testing temperature of 0.7 and a sampling size of 32 for testing. </sub> </div> <blockquote> <p>üìå Since the diversity of sample trajectories is critical for RL training, using the same sampling temperature throughout the training process can result in insufficient diversity at the later training stages and limit the potential for performance gains.</p> </blockquote> <p>Therefore, we propose dynamically updating the temperature during RL training. As the model converges on high-quality patterns, we will increase the sampling temperature to encourage further exploration.</p> <p><strong>After each training stage, we will use a higher sampling temperature to maintain the model‚Äôs previous diversity score.</strong></p> <p>Specifically, we test various sampling temperatures and select the one that achieves the desired diversity score. Our experiments suggest setting the temperature interval based on the previous stage‚Äôs entropy decrease. If entropy decreases slightly, we recommend a 0.05 interval for the sampling temperatures. If entropy decreases significantly, we will use a larger interval. We show the sampling temperatures of each stage of Polaris in this Table:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Stage-1</strong></th> <th><strong>Stage-2</strong></th> <th><strong>Stage-3</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-7B-Preview</code></strong></td> <td>0.7</td> <td>1.0</td> <td>1.1</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong></td> <td>1.4</td> <td>1.45</td> <td>1.5</td> </tr> </tbody> </table> <p>To verify the effectiveness of temperature increase, we conduct a baseline with the same temperature across the whole training. As we can see, the multi-stage with increased temperature leads to better RL training and further expanding the model‚Äôs thought depth by expanding the response length.</p> <p><img src="assets/img/polaris-imgs/image%209.png" alt="image.png"></p> <hr> <h2 id="3-inference-time-length-scaling"><em><strong>3. Inference-Time Length S</strong>caling</em></h2> <h3 id="insufficient-long-context-training-in-rl-stage"> <strong>insufficient</strong> long-context training in RL stage</h3> <p>A significant challenge in developing advanced reasoning models is the cost of long-context training. For instance, our model, based on <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, has a pre-training context length of <strong>32K</strong>. While we increase the maximum training length to 52K during RL, our experiments reveal a critical limitation. The <code class="language-plaintext highlighter-rouge">clip_ratio</code>, which measures the proportion of training samples that reach the maximum sequence length, remained below 10%. This indicates that very few samples are actually trained at the 52K length. During the RL training process, the inference time for rollouts often consumes a significant amount of resources. Within a single batch, if no additional optimizations are made, shorter samples must wait for longer samples to finish decoding, leading to wasted training time and resources. Therefore, it is not efficient to directly use a large training length. We also try some train-short, test-long methods to increase the inference length given limited training budgets.</p> <h3 id="performance-degradation-beyond-pre-training-length">Performance degradation beyond pre-training length</h3> <p>To quantify the effective Chain-of-Thought (CoT) length of <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> , we conduct an analysis using 60 problems from the AIME 2024/25 datasets. There are 32 rollouts for each problem (for a total of 1,920 rollouts) and grouped them by the length of the response:</p> <ul> <li> <strong>Short Rollouts Group</strong>: Responses with a length of less than <strong>16K</strong>.</li> <li> <strong>Mid-Length Rollouts Group</strong>: Responses with a length between 16K and 32K.</li> <li> <strong>Long Rollouts Group:</strong> Responses with a length exceeding the <strong>32K</strong> pre-training limit.</li> </ul> <p>The <em>Accuracy</em> for each group is calculated using the following formula:</p> <p>$\text{Accuracy} = \frac{\text{Number of Correct Rollouts}}{\text{Total Rollouts in Group}}$</p> <p>The results were striking (blue bars). We observe a dramatic performance drop for responses in the <strong>Long Rollouts Group</strong>, which achieved an accuracy of only 26%.</p> <p><img src="assets/img/polaris-imgs/image%2010.png" alt="image.png"></p> <p>This finding supports our hypothesis: due to the inefficiencies of long-context RL training, the model struggles to generate effective and accurate long CoTs beyond its original pre-training length, even though the RL training length is set to 52K.</p> <h3 id="training-free-length-extrapolation">Training-free Length Extrapolation</h3> <p>To address this, we introduce length extrapolation technique in long reasoning traces generation, which follows the principle of ‚Äútrain shorter, test longer.‚Äù By adjusting the model‚Äôs <a href="https://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">Rotary Position Embeddings (RoPE)</a>, this method allows the model to <strong>maintain its performance</strong> on sequences <strong>longer than</strong> those seen during training, effectively compensating for insufficient long-context training.</p> <p>For ease of implementation, we adopt the Yarn method with a scaling factor of <code class="language-plaintext highlighter-rouge">1.5</code>. While Yarn recommends adjusting the attention temperature during extrapolation, we find that this modification‚Äîthough beneficial for long-context retrieval tasks‚Äîis detrimental for generating long reasoning sequences.</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"rope_scaling"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"attn_factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.5</span><span class="p">,</span><span class="w">
    </span><span class="nl">"rope_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yarn"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p><img src="assets/img/polaris-imgs/image%2011.png" alt="image.png"></p> <p>By applying Yarn at inference time‚Äîwith no retraining required‚Äîwe boost the accuracy on responses longer than 32K from <strong>26% to over 50%!</strong></p> <p>This discovery suggests that CoT extrapolation is a powerful tool for the <strong>later stages</strong> of training advanced reasoning models, especially when increasing rollout length becomes unaffordable. We also note that the accuracy improvements are concentrated on the more difficult problems in the dataset.</p> <h3 id="extrapolation-benefits-inference-time-scaling">Extrapolation Benefits Inference-Time Scaling</h3> <p>The graph below illustrates the inference-time scaling capabilities unlocked by Yarn on the AIME 24/25 datasets. The blue line represents <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> with Yarn, while the orange line shows the baseline performance without it.</p> <p>As the chart demonstrates, Polaris-4B-Preview with Yarn (blue line) significantly outperforms its base model, Qwen3-4B, once the context length exceeds 48K. Its performance continues to grow as the length increases toward 96K. In contrast, the model without Yarn (yellow line) shows its performance plateauing after 64K with almost no further gains.</p> <p>This confirms that applying an extrapolation technique like Yarn at inference time unlocks the model‚Äôs potential to scale its reasoning abilities to much longer contexts, overcoming the limitations imposed by practical RL training constraints.</p> <p><img src="assets/img/polaris-imgs/image%2012.png" alt="image.png"></p> <hr> <h2 id="4-exploration-efficiency"><em>4. Exploration Efficiency</em></h2> <p>The success of long CoT training hinges on efficient exploration at the frontier of reward sparsity. In Polaris, exploration efficiency is enhanced through multi-stage training, accompanied with techniques that address response-level and sample reward sparsity. Specifically, we found that:</p> <ul> <li>the previously proposed <strong>‚ÄúThink Shorter, then Longer‚Äù</strong> paradigm does not generalize to all reasoning models; directly training with longer responses can often yield better performance;</li> <li>dynamical sampling can be done easy with the proposed <strong>Rollout Rescue Mechanism</strong> and <strong>Intra-Batch Informative Substitution</strong> techniques.</li> </ul> <h3 id="multi-stage-training">Multi-Stage Training</h3> <p>One of the biggest challenges in optimizing long CoT models with RL is the excessively long output, which results in slow training. To improve training efficiency, we incorporate multi-stage training in all our released models. Specifically, we use shorter context windows in earlier stages. Once the model‚Äôs performance converges, we increase the length of the context windows in the next stage.</p> <p><img src="assets/img/polaris-imgs/image%2013.png" alt="image.png"></p> <h3 id="is-think-shorter-then-longer-necessary"><strong>Is ‚ÄúThink Shorter, then Longer‚Äù necessary?</strong></h3> <p>While effective, for multi-stage training it is critical to select the appropriate response length at the first stage:</p> <ul> <li> <p>Not all models are both equally token-efficient: We found that training at a small response length works well for <code class="language-plaintext highlighter-rouge">DeepSeek-R1-Distill-Qwen-7B</code> but not for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>. Specifically, we observe drastic performance drop for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> even at a response length of 24K and response clip ratio of &lt;15%. Such performance degeneration is irreversible at later stages.</p> <p><img src="assets/img/polaris-imgs/image%2014.png" alt="image.png"></p> </li> <li> <p>It is usually safer to directly allow the model to ‚Äúthink longer‚Äù from the beginning: For <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, we observed steadily increasing performance with a 40K response length from scratch, in stark contrast with 24K and 24K‚Üí40K schemes.</p> <p><img src="assets/img/polaris-imgs/image%2015.png" alt="image.png"></p> </li> </ul> <p><strong>Takeaway</strong>: When computational resources allow, start directly with the maximum decoding length suggested by the official repository.</p> <h3 id="rollout-rescue-mechanism">Rollout Rescue Mechanism</h3> <p>POLARIS uses a small rollout size (8) for cost savings, but this raises the chance of zero-reward batches on hard prompts. To balance positive examples with minimal engineering, we maintain a per-example offline buffer (‚Äúsink‚Äù):</p> <ol> <li>If all 8 rollouts fail (accuracy 0/8) and a correct rollout was observed in earlier epochs, store that response in the sink (evicting the previous one).</li> <li>In later epochs, whenever a new batch yields 0/8 for that example, randomly swap one failed rollout with the buffered response.</li> </ol> <p>This lightweight strategy reduces zero-reward data dramatically and speeds up convergence, without retry loops.</p> <div align="center"> ![ An illustration of Rollout Rescue Mechanism ](assets/img/polaris-imgs/image%2016.png) <br> <sub> An illustration of Rollout Rescue Mechanism </sub> </div> <h3 id="intra-batch-informative-substitution">Intra-Batch Informative Substitution</h3> <p>In GRPO, examples with all-correct or all-incorrect rollouts produce no advantage. Rather than complex dynamic sampling, we apply a simple in-batch swap:</p> <ol> <li>Within each batch, select samples that have a mix of correct and incorrect rollouts (nonzero advantage).</li> <li>Randomly duplicate these informative samples to replace those that yield zero advantage.</li> </ol> <p>This ensures every training example contributes a learning signal, matching DAPO‚Äôs benefits but requiring only a few tensor index operations‚Äîno extra rollouts or data-pipeline changes.</p> <h2 id="5-from-dapo-and-grpo"><strong><em>5. From DAPO and GRPO+</em></strong></h2> <p>We‚Äôve incorporated several key strategies from <a href="https://dapo-sia.github.io/" rel="external nofollow noopener" target="_blank">DAPO</a> and <a href="https://www.notion.so/1cf81902c14680b3bee5eb349a512a51?pvs=21" rel="external nofollow noopener" target="_blank">GRPO+</a> into our training process for the following reasons:</p> <ul> <li> <strong>No Entropy Loss (from GRPO+):</strong> We remove the entropy loss term to prevent training instability. While intend to encourage exploration, we note it can cause entropy to grow uncontrollably, leading to a training collapse. Our primary motivation is to ensure a more stable and reliable training process.</li> <li> <strong>No KL Loss (from DAPO):</strong> We eliminate the KL loss to allow our model to explore beyond the constraints of the original SFT model. This also speeds up training, as we no longer need to compute log probabilities for a reference model.</li> <li> <strong>Clip High (from DAPO):</strong> We increase the upper clipping bound in the surrogate loss function to encourage more aggressive exploration. This adjustment helps stabilize entropy and has been shown to improve model performance by allowing the policy to take larger, more beneficial update steps.</li> </ul> <h2 id="6-reward-function"><em>6. Reward Function</em></h2> <p>The reward function used in this work is the same as DeepscaleR, we employ an Outcome Reward Model (ORM) which returns:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">1</code> - If the LLM‚Äôs answer passes basic LaTeX/Sympy checks.</li> <li> <code class="language-plaintext highlighter-rouge">0</code> - If the LLM‚Äôs answer is incorrect or formatted incorrectly (e.g. missing <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code> delimiters).</li> </ul> <h1 id="evaluation"><strong><em>Evaluation</em></strong></h1> <p>Our model needs to use a <strong>higher</strong> <strong>sampling temperature</strong> and <strong>a longer response length</strong> than Qwen3; all other settings are the same. For AIME24 and AIME25, we report the average performance of 32 runs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">90000</span>
    <span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">example input:</code> <code class="language-plaintext highlighter-rouge">&lt;|im_start|&gt;user\nEvery morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop. Let's think step by step and output the final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></p> <p>The evaluation scripts based on <a href="https://github.com/volcengine/verl" rel="external nofollow noopener" target="_blank">Verl</a> have been released on our GitHub. You can also use your own scripts for testing, but please note: our model‚Äôs output length has been significantly boosted. If your max response length is set too small, the performance may not even reach the level of the original <strong><code class="language-plaintext highlighter-rouge">Qwen3-4B</code></strong> due to the truncation mechanism. Therefore, please ensure the testing length is at least 64K. The graph showing performance changes with response length is available in the <strong>‚ÄúInference-Time Length Scaling‚Äù</strong> section.</p> <table> <thead> <tr> <th><strong>Models</strong></th> <th><strong>AIME24 avg@32</strong></th> <th><strong>AIME25 avg@32</strong></th> <th><strong>Minerva Math avg@4</strong></th> <th><strong>Olympiad Bench avg@4</strong></th> <th><strong>AMC23 avg@8</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code></strong></td> <td>55.0</td> <td>39.7</td> <td>36.7</td> <td>56.8</td> <td>81.9</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">AReal-boba-RL-7B</code></strong></td> <td>61.9</td> <td>48.3</td> <td>39.5</td> <td>61.9</td> <td>86.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Skywork-OR1-7B-Math</code></strong></td> <td>69.8</td> <td>52.3</td> <td><strong>40.8</strong></td> <td>63.2</td> <td>85.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-7B-Preview</code></strong></td> <td><strong>72.6</strong></td> <td><strong>52.6</strong></td> <td>40.2</td> <td><strong>65.4</strong></td> <td><strong>89.0</strong></td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-32B</code></strong></td> <td>72.6</td> <td>54.9</td> <td>42.1</td> <td>59.4</td> <td>84.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-32B</code></strong></td> <td>81.4</td> <td>72.9</td> <td>44.2</td> <td>66.7</td> <td>92.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-4B</code></strong></td> <td>73.8</td> <td>65.6</td> <td>43.6</td> <td>62.2</td> <td>87.2</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-4B-Preview</code></strong></td> <td><strong>81.2</strong></td> <td><strong>79.4</strong></td> <td><strong>44.0</strong></td> <td><strong>69.1</strong></td> <td><strong>94.8</strong></td> </tr> </tbody> </table> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Polaris2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{POLARIS: A Post-Training Recipe for Scaling Reinforcement Learning on Advanced Reasoning Models}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/Polaris}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Chenxin and Xie, Zhihui and Li, Xiaonan and Li, Lei and Zhang, Jun and Gong, Shansan and Zhong, Ming and Xu, Jingjing and Qiu, Xipeng and Wang, Mingxuan and Kong, Lingpeng}</span>
    <span class="nv">year</span> <span class="err">=</span> <span class="err">{2025</span><span class="p">}</span>
<span class="c">}</span>
</code></pre></div></div> </body></html>