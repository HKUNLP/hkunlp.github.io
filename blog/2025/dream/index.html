<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Dream 7B | HKU NLP Group </title> <meta name="author" content="HKU NLP Group "> <meta name="description" content="Introducing Dream 7B, the most powerful open diffusion large language model to date."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hkunlp.github.io/blog/2025/dream/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"]],tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Dream 7B",
      "description": "Introducing Dream 7B, the most powerful open diffusion large language model to date.",
      "published": "April 2, 2025",
      "authors": [
        {
          "author": "Jiacheng Ye",
          "authorURL": "https://jiacheng-ye.github.io/",
          "affiliations": [
            {
              "name": "University of Hong Kong",
              "url": ""
            }
          ]
        }
        
      ]
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">HKU NLP Group </span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/seminar/">HKUNLP Seminar</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Dream 7B</h1> <p>Introducing Dream 7B, the most powerful open diffusion large language model to date.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introducing-dream-7b">Introducing Dream 7B</a></div> <div><a href="#why-diffusion-for-text-generation">Why Diffusion for Text Generation?</a></div> <div><a href="#training">Training</a></div> <div><a href="#planning-ability">Planning Ability</a></div> <div><a href="#inference-flexibility">Inference Flexibility</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <p><strong>Team:</strong> Jiacheng Ye*, Zhihui Xie*, Lin Zheng*, Jiahui Gao*, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Huawei Noah’s Ark Lab</p> <h2 id="introducing-dream-7b">Introducing Dream 7B</h2> <p>In a joint effort with Huawei Noah’s Ark Lab, we release <strong>Dream 7B</strong> (<ins>D</ins>iffusion <ins>rea</ins>soning <ins>m</ins>odel), the most powerful open diffusion large language model to date.</p> <p>In short, Dream 7B:</p> <ul> <li>consistently outperforms existing diffusion language models by a large margin;</li> <li>matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;</li> <li>demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/overall_performance.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: comparison of language models on general, math, coding, and planning tasks.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/main_tab.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: comparison of language models on standard evaluation benchmarks. * indicates Dream 7B, LLaDA 8B, Qwen2.5 7B and LLaMA3 8B are evaluated under the same protocol. The best results are bolded and the second best are underlined.</figcaption> </figure> </div> </div> <p>We release the weights of the base and instruct models in:</p> <ul> <li>Base model: <a href="https://huggingface.co/Dream-org/Dream-v0-Base-7B" rel="external nofollow noopener" target="_blank"><strong>Dream-org/Dream-v0-Base-7B</strong></a> </li> <li>SFT model: <a href="https://huggingface.co/Dream-org/Dream-v0-Instruct-7B" rel="external nofollow noopener" target="_blank"><strong>Dream-org/Dream-v0-Instruct-7B</strong></a> </li> <li>Codebase: <a href="https://github.com/HKUNLP/Dream" rel="external nofollow noopener" target="_blank"><strong>GitHub</strong></a> </li> </ul> <h2 id="why-diffusion-for-text-generation">Why Diffusion for Text Generation?</h2> <p>The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, transforming numerous applications across industries. Currently, autoregressive (AR) models dominate the landscape of text generation, with virtually all leading LLMs (e.g., GPT-4, DeepSeek, Claude) relying on this same sequential left-to-right architecture. While these models have demonstrated remarkable capabilities, a fundamental question emerges: what architectural paradigms might define the next generation of LLMs? This question becomes increasingly relevant as we observe certain limitations in AR models at scale, including challenges with complex reasoning, long-term planning, and maintaining coherence across extended contexts<d-cite key="bubeck2023sparks,dziri2024faith,bachmann2024pitfalls,ye2024beyond"></d-cite>. These limitations are particularly crucial for emerging applications such as embodied AI, autonomous agents, and long-horizon decision-making systems, where sustained reasoning and contextual understanding are essential for success.</p> <p>Discrete diffusion models (DMs) have gained attention as a promising alternative for sequence generation since their introduction to the text domain <d-cite key="hoogeboom2021argmax,austin2021structured,campbell2022continuous"></d-cite>. Unlike AR models that generate tokens sequentially, discrete DMs dynamically refine the full sequence in parallel starting from a fully noised state. This fundamental architectural difference unlocks several significant advantages:</p> <ul> <li> <strong>Bidirectional contextual modeling</strong> enables richer integration of information from both directions, substantially enhancing global coherence across the generated text.</li> <li> <strong>Flexible controllable generation</strong> capabilities arise naturally through the iterative refinement process.</li> <li> <strong>Potential for fundamental sampling acceleration</strong> through novel architectures and training objectives that enable efficient direct mapping from noise to data <d-cite key="song2023consistency"></d-cite>.</li> </ul> <p>Recently, significant advancements have highlighted diffusion’s growing potential in language tasks. DiffuLLaMA<d-cite key="gong2024scaling"></d-cite> and LLaDA<d-cite key="nie2025large"></d-cite> scaled diffusion language models to 7B parameters, while <a href="https://www.inceptionlabs.ai/news" rel="external nofollow noopener" target="_blank">Mercury Coder</a>, as a commercial implementation, has demonstrated remarkable inference efficiency in code generation. This rapid progress, combined with the inherent architectural advantages of diffusion language modeling, positions these models as a promising direction for overcoming the fundamental limitations of autoregressive approaches.</p> <h2 id="training">Training</h2> <p>Dream 7B builds upon <a href="https://ikekonglp.github.io/dreams.html" rel="external nofollow noopener" target="_blank">our team’s prior effort</a><d-footnote><a href="https://ikekonglp.github.io/dreams.html" rel="external nofollow noopener" target="_blank">https://ikekonglp.github.io/dreams.html</a></d-footnote> in the diffusion language model area, drawing from RDM<d-cite key="Zheng2023ARD"></d-cite>’s theoretical foundation and DiffuLLaMA<d-cite key="gong2024scaling"></d-cite>’s adaptation strategy. We adopt a mask diffusion paradigm with the model architecture shown below. Our training data spans from text to math and code, mainly sourced from <a href="https://huggingface.co/datasets/allenai/dolma" rel="external nofollow noopener" target="_blank">Dolma v1.7</a>, <a href="https://huggingface.co/collections/OpenCoder-LLM/opencoder-datasets-672e6db6a0fed24bd69ef1c2" rel="external nofollow noopener" target="_blank">OpenCoder</a>, and <a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0" rel="external nofollow noopener" target="_blank">DCLM-Baseline</a>, with several pre-processing and curation pipelines. Following a carefully designed training process, we pretrain Dream 7B using a mixture of the aforementioned corpus, totaling 580 billion tokens. The pretraining was done on 96 NVIDIA H800 GPUs for 256 hours. The pretraining process went smoothly overall, with occasional node anomalies, and we did not experience any unrecoverable loss spikes.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/model-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/model.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: comparison of autoregressive modeling and diffusion modeling in Dream. Dream predicts all the masked tokens in a shifted manner, allowing for maximumly architectural alignment and weight initialization with AR models.</figcaption> </figure> </div> </div> <p>We extensively studied the design choices on the 1B level and identified many valuable components, such as weight initialization from AR models (e.g., Qwen2.5<d-cite key="yang2024qwen2"></d-cite> and LLaMA3<d-cite key="grattafiori2024llama"></d-cite>) and a context-adaptive token-level noise rescheduling, which enables the effective training of Dream 7B.</p> <h3 id="ar-initialization">AR initialization</h3> <p>Building on our previous work DiffuLLaMA<d-cite key="gong2024scaling"></d-cite>, we discovered that using the weights from the existing autoregressive (AR) model serves as a non-trivial initialization for the diffusion language model. We find this design is more effective than training the diffusion language model from scratch, particularly during the early stages of training, as illustrated in the figure below.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/from_scratch_adapt.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: loss comparison of the from-scratch and AR-initialization with LLaMA3.2 1B training on the Dream 1B models with 200B tokens. AR initialization also experiences a high loss at the beginning due to the transition from causal attention to full attention; however, this loss remains lower compared to training from scratch throughout the training.</figcaption> </figure> </div> </div> <p>Dream 7B is finally initialized with weights from <a href="https://huggingface.co/Qwen/Qwen2.5-7B" rel="external nofollow noopener" target="_blank">Qwen2.5 7B</a>. During the training process, we find the learning rate to be especially important. If it’s set too high, it can quickly wash away the left-to-right knowledge in the initial weights, providing little help in the diffusion training, while if it’s set too low, it can hinder diffusion training. We meticulously selected this parameter along with the other training parameters.</p> <p>Thanks to the existing left-to-right knowledge in the AR model, the diffusion model’s any-order learning can be accelerated, significantly reducing the tokens and computation required for pretraining.</p> <h3 id="context-adaptive-token-level-noise-rescheduling">Context-adaptive Token-level Noise Rescheduling</h3> <p>The selection of each token in a sequence depends on its context, yet we observed that previous diffusion training approaches fail to adequately account for this aspect. Specifically, in conventional discrete diffusion training, a timestep <em>t</em> is sampled to determine the sentence-level noise level, after which the model performs denoising. However, since the learning ultimately operates at the token level, the actual noise level for each token does not strictly align with <em>t</em> due to the application of discrete noise. This resulted in ineffective learning of tokens with varying levels of contextual information.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/reweighting.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: illustration of the context-adaptive token-level noise rescheduling mechanism. Dream re-decides a token-level timestep t for each mask token by measuring its context informationness.</figcaption> </figure> </div> </div> <p>To address this, we introduce a context-adaptive token-level noise rescheduling mechanism that dynamically reassigns the noise level for each token based on the corrupted context after noise injection. This mechanism provides more fine-grained and precise guidance for the learning process of individual tokens.</p> <h2 id="planning-ability">Planning Ability</h2> <p>In our previous work<d-cite key="ye2024beyond,ye2025implicit"></d-cite>, we demonstrated that text diffusion exhibits superior planning capabilities in the small-scale, task-specific context. However, it remains uncertain whether a general, scaled diffusion model possesses similar abilities. Now, with Dream 7B, we can better answer this question.</p> <p>We evaluated Dream on the Countdown and Sudoku tasks from <d-cite key="ye2024beyond"></d-cite>, where we can flexibly control the planning difficulty. Our comparison included Dream 7B alongside <a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base" rel="external nofollow noopener" target="_blank">LLaDA 8B</a>, <a href="https://huggingface.co/Qwen/Qwen2.5-7B" rel="external nofollow noopener" target="_blank">Qwen2.5 7B</a>, and <a href="https://huggingface.co/meta-llama/Llama-3.1-8B" rel="external nofollow noopener" target="_blank">LLaMA3 8B</a>, together with the latest <a href="https://www.deepseek.com/" rel="external nofollow noopener" target="_blank">Deepseek V3 671B (0324)</a> for reference. All models were assessed in a few-shot setting without any training on these tasks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/sudoku_cd.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: results on Countdown and Sudoku when varying planning difficulty.</figcaption> </figure> </div> </div> <p>It is evident that Dream outperforms other similar-sized baseline models. Remarkably, both diffusion models significantly surpass the two AR models and, at times, even the latest DeepSeek V3, despite its orders of magnitude more parameters. The intuition behind is that diffusion language models are more effective for solving problems with multiple constraints or for achieving specific objectives.</p> <p>Here are some examples of Qwen 2.5 7B and Dream 7B in three planning tasks:</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/cases-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/cases-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/cases-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/cases.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: generation examples from Qwen2.5 7B and Dream 7B.</figcaption> </figure> </div> </div> <h2 id="inference-flexibility">Inference Flexibility</h2> <p>Diffusion models offer more flexible inference compared to AR models in the following two main aspects.</p> <h3 id="arbitrary-order">Arbitrary Order</h3> <p>Diffusion models are not constrained to sequential (e.g., left-to-right) generation, enabling outputs to be synthesized in arbitrary orders—this allows for more diverse user queries.</p> <ul> <li><strong>Completion</strong></li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: a completion example of Dream-7B-instruct.</figcaption> </figure> </div> </div> <ul> <li><strong>Infilling</strong></li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/fig_infill_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: an infilling example of Dream-7B-instruct with an exact ending sentence.</figcaption> </figure> </div> </div> <ul> <li> <p><strong>Controlling the decoding behavior</strong></p> <p>Different queries may have preferences for the order in which the responses are generated. One can also adjust the decoding hyperparameters to control the decoding behavior, shifting it from more left-to-right like an AR model to more random-order generation.</p> </li> </ul> <div class="row mt-1"> <div class="col-sm-4 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/fig_code_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: configured to decode more in a left-to-right way like an AR model.</figcaption> </figure> </div> <div class="col-sm-4 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/fig_code_2.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: configured to add some randomness in the decoding order.</figcaption> </figure> </div> <div class="col-sm-4 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/fig_code_3.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: configured for fully randomness in the decoding order.</figcaption> </figure> </div> </div> <h3 id="quality-speed-trade-off">Quality-speed Trade-off</h3> <p>In the above cases, we show one token is generated per step. However, the number of generated tokens per step (controlled by diffusion steps) can be adjusted dynamically, providing a tunable trade-off between speed and quality: fewer steps yield faster but coarser results, while more steps produce higher-quality outputs at greater computational cost. This introduces an additional dimension for inference-time scaling <d-cite key="snell2024scaling,muennighoff2025s1,geiping2025scaling"></d-cite> that complements rather than replaces techniques like long chain-of-thought reasoning employed in large language models such as o1 and r1. This adjustable computation-quality tradeoff represents a unique advantage over traditional AR frameworks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/quality_speed.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: quality-speed comparison on the Countdown task for Dream 7B and Qwen2.5 7B. By adjusting the diffusion timesteps, the performance of Dream can be flexibly tuned for either speed or quality.</figcaption> </figure> </div> </div> <h2 id="supervised-fine-tuning">Supervised Fine-tuning</h2> <p>As a preliminary step in post-training diffusion language models, we perform supervised fine-tuning to align Dream with user instructions. Specifically, we curate a dataset with 1.8M pairs from Tulu 3<d-cite key="lambert2024t"></d-cite> and SmolLM2<d-cite key="allal2025smollm2smolgoesbig"></d-cite>, fine-tuning Dream for three epochs. The results highlight Dream’s potential to match autoregressive models in performance. Looking forward, we plan to explore more advanced post-training recipes for diffusion language models.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/sft-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/sft-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/sft-1400.webp"></source> <img src="/assets/img/2025-04-02-dream-img/sft.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: supervised fine-tuning results.</figcaption> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>We introduce Dream, a new family of efficient, scalable, and flexible diffusion language models with carefully selected training recipes. It performs comparably to the best autoregressive models of similar size in general, mathematical, and coding tasks while especially showcasing advanced planning abilities and flexible inference capabilities.</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">dream2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Dream 7B}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/dream}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 HKU NLP Group . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <d-bibliography src="/assets/bibliography/2025-04-02-dream.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>