<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hkunlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hkunlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-22T03:06:37+00:00</updated><id>https://hkunlp.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">EvaByte: Efficient Byte-level Language Models at Scale</title><link href="https://hkunlp.github.io/blog/2025/evabyte/" rel="alternate" type="text/html" title="EvaByte: Efficient Byte-level Language Models at Scale"/><published>2025-01-21T00:00:00+00:00</published><updated>2025-01-21T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/evabyte</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/evabyte/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} \newcommand{\mbpsi}{\mathbold{\psi}} \newcommand{\mcM}{\mathcal{M}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} $$ </div> <p><strong>Full team:</strong> Lin Zheng, Xueliang Zhao, Guangtao Wang, Chen Wu, David Dong, Angela Wang, Mingran Wang, Yun Du, Haige Bo, Amol Sharma, Bo Li, Kejie Zhang, Changran Hu, Urmish Thakker, and Lingpeng Kong</p> <h2 id="introducing-evabyte">Introducing EvaByte</h2> <p>In a collaborative effort between the University of Hong Kong and SambaNova Systems, we introduce <strong>EvaByte</strong>, a 6.5B state-of-the-art <strong>byte-level language model</strong> featuring an improved architecture and powered by EVA â€“ an efficient attention mechanism designed for scalability and performance.</p> <p>Trained on 1.5T bytes of natural language text, math, and code using the performant SambaNova SN30 RDU system, EvaByte demonstrates that efficient byte-level processing at scale is not just possible, but practically advantageous â€“ rivaling modern open-source tokenizer-based LMs <d-cite key="groeneveld2024olmo,li2024dclm,zhang2024mapneo"></d-cite> despite using 5x less training data, excelling in coding tasks, and decoding up to 2x faster. Its token-free design also brings added <strong>flexibility</strong>, avoiding tokenizer quirks while naturally extending to <a href="#case-study-multimodal-learning">multimodal applications</a> without any architecture tweaks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: scaling analysis between average task performance and training set size.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/main_table_v2.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of language models on standard evaluation benchmarks. â€¡ the number of tokens measured by Llama 3 tokenizer, corresponding to 1.5T training bytes. â€ Low scores are caused by failing to generate Python functions and repeat the input under EvalPlus prompt format.</figcaption> </figure> </div> </div> <p>To our knowledge, EvaByte is the first open-source byte-level model without tokenization that yet matches the performance of modern tokenizer-based LMs. Check out the model weights and code here:</p> <ul> <li>Base model before annealing: <a href="https://huggingface.co/EvaByte/EvaByte-Phase1"><strong>EvaByte/EvaByte-Phase1</strong></a></li> <li>Base model: <a href="https://huggingface.co/EvaByte/EvaByte"><strong>EvaByte/EvaByte</strong></a></li> <li>SFT model: <a href="https://huggingface.co/EvaByte/EvaByte-SFT"><strong>EvaByte/EvaByte-SFT</strong></a></li> <li>Codebase: <a href="https://github.com/OpenEvaByte/evabyte"><strong>GitHub</strong></a></li> </ul> <h2 id="byte-level-modeling-with-improved-architectures">Byte-level Modeling with Improved Architectures</h2> <p>Tokenization is a fundamental step in modern large language models, deciding how input is represented in Transformers. Although it efficiently compresses raw text into shorter sequences, tokenization comes with its own baggage â€“ it is an externally trained, detached component that can introduce complex biases and edge-case quirks, like the prompt boundary problem <d-cite key="microsoft2023guidance,lundberg2023tokenhealing,dagan2024getting,athiwaratkun2024token,vieira2024language"></d-cite>, undertrained tokens <d-cite key="rumbelow2023solidgoldmagikarp,land2024fishing,wang2024tokenizationmatters,yang2024rethinking,yang2024problematictokens"></d-cite>, and even pretraining data mixture leaks <d-cite key="hayase2024datamixture"></d-cite>.</p> <p>Byte-level modeling is an approach that inherently eliminates biases introduced by tokenization, although directly operating on bytes at scale is not easy <d-cite key="clark2022canine,xue2022byt5,tay2022charformer,yu2023megabyte,slagle2024spacebyte,wang2024mambabyte,kallini2024mrt5"></d-cite>: </p> <div class="row mt-0"> <div class="col-sm-10 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: correspondence between tokens and bytes, as measured by the GPT-4o tokenizer.</figcaption> </figure> </div> </div> <ul> <li>Byte sequences are naturally longer â€“ 3.8x longer than their tokenized counterparts in our training corpus â€“ leading to more than 3.8x computational overhead under standard Transformer architectures.</li> <li>Inference becomes more challenging due to the inherently long and sequential nature of byte-level predictions.</li> <li>Training byte-level models is less stable as we observed in our <a href="#training">experiments</a>.</li> </ul> <p>We address these hurdles with a streamlined architecture featuring two improvements: <strong>multibyte prediction</strong> and <strong>the efficient attention mechanism, EVA</strong>.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/arch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: an overview of the EvaByte architecture.</figcaption> </figure> </div> </div> <p>Although vanilla byte-level language models typically run much slower than tokenizer-based LMs, with the improved architecture, we have achieved a significant speed boost for byte models â€“ <strong>5-10x faster</strong> decoding compared to vanilla architectures and even <strong>up to 2x faster</strong> than tokenizer-based LMs, making byte-level models a practical choice for real-world applications.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: <b>bytes per second</b> (ðŸ …) measured by generating 512 bytes (or tokens) with a batch size of 1 on one H800 GPU using the HF native generate() interface.</figcaption> </figure> </div> </div> <h3 id="multibyte-prediction">Multibyte Prediction</h3> <p>We draw inspiration from recent work <d-cite key="stern2018blockwise,qi2020prophetnet,cai2024medusa,gloeckle2024multitoken"></d-cite> and equip our model with multiple prediction heads, allowing it to predict several future bytes simultaneously. During training, we average the cross-entropy losses from different output heads as the primary training objective. These heads learn very effectively â€“ their predictions are often highly accurate and sometimes even outperform the immediate next byte prediction, as shown in the figure below.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: multi-choice task performance across different prediction heads. Each head corresponds to using the likelihood from the immediate next byte prediction (Head 1), second-next byte prediction (Head 2), and so forth.</figcaption> </figure> </div> </div> <p>Multibyte prediction adds almost no training overhead, thanks to the particularly small vocabulary size. <d-footnote>Our model uses 8 prediction heads and a vocabulary size of 320, including 256 byte values and 64 special tokens.</d-footnote> However, it greatly speeds up inference with <strong>self-speculative decoding</strong>, where multiple heads are combined via Medusa-like tree attention <d-cite key="cai2024medusa"></d-cite> and enable the model to predict multiple bytes in one decoding step.</p> <h3 id="efficient-attention-with-eva">Efficient Attention with EVA</h3> <p>However, multibyte prediction alone is not enough to speed up the byte-level model: the self-attention mechanism quickly becomes the major bottleneck as the context length grows. To address this, we build our model on <strong>EVA</strong> <d-cite key="zheng2023eva"></d-cite>, an improved version of <strong>linearized attention</strong> <d-cite key="katharopoulos2020transformers_are_rnns,peng2021rfa,choromanski2021rethinking"></d-cite>. Linearized attention approximates exact self-attention by designing feature maps $\phi(\cdot)$ such that</p> <div style="font-size: 0.9em; auto; text-align: center; max-width: 100%;"> \begin{equation} \frac{\sum_{m=1}^n\exp\left(\mbq_{n}^\top \mbk_{m} \right)\mbv_{m}^\top}{\sum_{m'=1}^n \exp\left(\mbq_{n}^\top \mbk_{m'} \right)} \approx \frac{\sum_{m=1}^n \phi(\mbq_n)^\top \phi(\mbk_m)\mbv_{m}^\top}{\sum_{m'=1}^n\phi(\mbq_{n'})^\top \phi(\mbk_{m'})} = \frac{\phi(\mbq_n)^\top \sum_{m=1}^n \phi(\mbk_m)\mbv_{m}^\top}{\phi(\mbq_{n'})^\top \sum_{m'=1}^n\phi(\mbk_{m'})} \notag. \end{equation} </div> <p>By linearizing $\exp(\cdot)$, one can rearrange the order of computation and achieve linear complexity in sequence length. This approach admits the form of a linear RNN, maintaining a global hidden state. With gating mechanisms and decay coefficients <d-cite key="peng2021rfa,qin2024hgrn2,sun2023retnet,yang2024gla"></d-cite>, it also connects to recent state-space models like Mamba and Mamba-2 <d-cite key="gu2024mamba,dao2024mamba2"></d-cite>. Conventional linearized attention compresses past tokens into a single global hidden state, unlike standard attention, which explicitly caches every token.</p> <p>EVA takes a middle ground by <strong>distributing</strong> the global state into multiple local memory slots. By splitting key-value pairs into consecutive chunks and applying linearization <strong>separately</strong> on each chunk, EVA maintains a local hidden state for each chunk and aggregates them together to produce the final output. This expands the design space of linearized attention mechanisms, simplifies implementation, and directly benefits from hardware-optimized kernels for standard attention mechanisms.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/attn_sketch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: computation graphs for standard attention (<strong>left</strong>), linearized attention (<strong>middle</strong>), and EVA (<strong>right</strong>). Symbols: $\times$ denotes (multiple) matrix multiplication and $\sum$ represents sum reduction.</figcaption> </figure> </div> </div> <h2 id="training">Training</h2> <p>We pretrain EvaByte on a corpus of 1.5T bytes spanning from text to math and code, mainly sourced from <a href="https://huggingface.co/datasets/allenai/dolma">Dolma v1.7</a>, <a href="https://huggingface.co/datasets/bigcode/the-stack-v2-train-smol-ids">The Stack v2</a>, <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-Edu</a>, and <a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline</a>. We constantly refined the data mix by tweaking the proportions or swapping in new sources mid-flight. After training on 1.2T bytes, we conduct two independent annealing runs (100B and 200B bytes respectively), where the learning rate is linearly decayed from 1e-4 to 0 and the checkpoints are merged via model soup. <d-cite key="wortsman22modelsoups"></d-cite></p> <p>EvaByte is trained with a batch size of 8M bytes and 32K context length on 256 SambaNova SN30-2 RDUs. We observed non-trivial instability during pretraining:</p> <ul> <li><strong>Byte-level collapses</strong>: Occasionally, intermediate checkpoints would produce bizarre typos (e.g., <code class="language-plaintext highlighter-rouge">e</code> in generated outputs turning into an <code class="language-plaintext highlighter-rouge">i</code>) when prompted to perform generation tasks; interestingly, these glitches resolved themselves after a few thousand training steps and never appeared near the end of training.</li> </ul> <figure style="width: 92%; margin: 0 auto;"> <figcaption>A snapshot of code generation at an intermediate checkpoint with bizarre typos.</figcaption> <pre style=" font-size: 12px; font-family:monospace;color: rgb(0, 0, 0); background-color: rgb(254, 252, 252); font-weight: 100; white-space: pre-wrap; word-wrap: break-word; margin: 0; line-height: 1.25; ">
<span style="color: rgb(0, 0, 255); font-weight: 400;">from</span> typing <span style="color: rgb(0, 0, 255); font-weight: 400;">import</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>, <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>

<span style="color: rgb(0, 0, 255); font-weight: 400;">def</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">sum_product</span>(<span style="color: rgb(0, 0, 0); font-weight: 400;">numbers: <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]</span>) -&gt; <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>, <span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]:
    <span style="color: rgb(163, 21, 21); font-weight: 400;">""" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    &gt;&gt;&gt; sum_product([])
    (0, 1)
    &gt;&gt;&gt; sum_product([1, 2, 3, 4])
    (10, 24)
    """</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> = <span style="color: rgb(0, 0, 0); font-weight: 400;">0</span>
    product = <span style="color: rgb(0, 0, 0); font-weight: 400;">1</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">for</span> number <span style="color: rgb(0, 0, 255); font-weight: 400;">in</span> numb<span style="background-color: #ffb6c1;">i</span>rs:
        <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> += numb<span style="background-color: #ffb6c1;">i</span>r
        product *= numb<span style="background-color: #ffb6c1;">i</span>r
    <span style="color: rgb(0, 0, 255); font-weight: 400;">return</span> (<span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span>, product)
</pre> </figure> <ul> <li><strong>Loss spikes</strong>: The most helpful techniques for stabilizing training through our experiments include <ul> <li>Lowering Adam epsilon $\epsilon$ from 1e-8 to 1e-12.</li> <li>Skipping batches that lead to spikes to keep the model in sane state.</li> <li>Periodically resetting Adam optimizer states to zero with quickly re-warming up the learning rate to remove bad out-of-track estimates.</li> </ul> <p>Other attempts, like freezing embedding parameters or applying weighted average over different prediction heads, offered little improvement.</p> </li> </ul> <h2 id="empirical-results">Empirical Results</h2> <p>Letâ€™s dive into how EvaByte performs in practice. We compare EvaByteâ€™s intermediate checkpoints against recent language models (OLMo-1.7-7B and OLMo-2-7B), trained on the roughly same amount of data. We observe the EvaByte checkpoint at 1.22T bytes (roughly 0.4T tokens) consistently outperforms them by a large margin.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of intermediate checkpoints on standard benchmarks.</figcaption> </figure> </div> </div> <p></p> <p>We also tracked EvaByteâ€™s task performance throughout pretraining and observed a consistent <strong>upward trend with no signs of plateauing</strong>. Interestingly, EvaByte excels at coding tasks (e.g., HumanEval and MBPP), even though we intentionally reduced the proportion of code data in the later stages of training. One possible reason is that removing tokenization might eliminate domain-specific biases, enabling more efficient parallel learning across domains. A deeper investigation into this behavior is planned for future work.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>We take EvaByte a step further with <strong>supervised fine-tuning</strong>. Following DCLM <d-cite key="li2024dclm"></d-cite>, OLMo-2 <d-cite key="ai22024olmo2"></d-cite>, TULU 3 <d-cite key="lambert2024tulu3"></d-cite>, and OpenCoder <d-cite key="huang2024opencoder"></d-cite>, we curate a data mix from Tulu 3, OpenHermes 2.5, and OpenCoder, fine-tune EvaByte for 2 epochs, and achieve results on par with recent open LMs.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/sft_table.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of instruct models. â€  Evaluated by us. * Following Tulu 3, we evaluate the Pass@10 rate for HumanEval with 20 samples at temperature 0.8.</figcaption> </figure> </div> </div> <h3 id="flexibility">Flexibility</h3> <p>As mentioned at the beginning, we demonstrate below that byte-level modeling naturally avoids tokenization quirks and edge-case behaviors, such as the <strong>prompt boundary problem</strong>, where tokenizer-based LMs behave inconsistently around prompt boundaries. EvaByte resolves these cases seamlessly and delivers more predictable results.</p> <figure> <figcaption style="font-size: 100%; font-family: monospace; padding-bottom: 10px;"> <span style="background-color: #a8dcfb; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> prompt &nbsp;&nbsp;&nbsp; <span style="background-color: #B2FBA8; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> correct completion &nbsp;&nbsp;&nbsp; <span style="background-color: #ffb6c1; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> incorrect completion </figcaption> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>EvaByte</strong>: outputs from different prompt boundaries converge. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #B2FBA8;">""\n    if not strings:\n        return None\n    longest = strings...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #B2FBA8;">"\n    if not strings:\n        return None\n    longest = strings[...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #B2FBA8;">\n    if not strings:\n        return None\n    longest = strings[0...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #B2FBA8;">   if not strings:\n        return None\n    longest = strings[0]\n...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #B2FBA8;">  if not strings:\n        return None\n    longest = strings[0]\n ...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest = strings[0]\n  ...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #B2FBA8;">if not strings:\n        return None\n    longest = strings[0]\n   ...</span>
</pre> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>Qwen2.5-7B</strong>: different prompt boundaries lead to diverging and unexpected outputs. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #ffb6c1;">&gt;&gt;&gt; longest([\'a\', \'bb\', \'ccc\', \'dddd\'])\n    \'dddd\'\n    """\n    i...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #ffb6c1;">"""\n    if not strings:\n        return None\n    longest_string =...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #ffb6c1;"></span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #ffb6c1;"> if not strings:\n    return None\n  longest = strings[0]\n  for st...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #ffb6c1;"> # if not strings:\n    #    return None\n    # longest = strings[...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest_string = string...</span>

<span style="background-color: #a8dcfb;">â–¶ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #ffb6c1;"> if len(strings) == 0 None:\n        return None\n    else:\n      ...</span>
</pre> </figure> <h2 id="case-study-multimodal-learning">Case Study: Multimodal Learning</h2> <p>EvaByte is also flexible to extend to multimodal tasks, treating image data as just another byte stream according to some protocol, such as JPEG, PNG, etc. We follow prior work <d-cite key="perez2024compressed,han2024jpeglm"></d-cite> and use JPEG format, keeping image byte sequence length manageable (a 384 x 384 image takes around 10K JPEG bytes). Thanks to byte-level modeling, EvaByte can seamlessly interleave image with text bytes for vision-language training <strong>without any architectural tweaks</strong>.</p> <p>Our multimodal dataset spans around 2T bytes, primarily filtered from OBELICS <d-cite key="laurencon2023obelics"></d-cite>, LLaVA-ReCap <d-cite key="li2024llavanext-ablations"></d-cite>, and DataComp <d-cite key="gadre2023datacomp,mobileclip2024"></d-cite>. After 20K fine-tuning steps on the data mix (~3M images), starting from a pre-trained text-only checkpoint, EvaByte can zero-shot generate captions and answer basic visual questions. <d-footnote> EvaByte can also generate images though a bit fuzzily. </d-footnote> The multimodal capabilities of EvaByte are actively evolving, with current work addressing specific alignment challenges (such as spatial relationships between computer components). Scaling and refining these multimodal abilities represents one of our most promising research directions.</p> <table style="width: 100%; border-collapse: collapse;"> <caption style="font-size: 0.9em; color: gray;">Table: after fine-tuning on limited multimodal data, EvaByte demonstrates basic capabilities in image captioning and visual question answering.</caption> <thead> <tr> <th style="text-align: center; width: 30%;">Image</th> <th style="text-align: center; width: 70%;">Text</th> </tr> </thead> <tbody> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a close-up view of a laptop keyboard. The keyboard has a black color scheme with white lettering on the keys. The keys are arranged in a standard QWERTY layout, which is commonly used for English language keyboards. The keys are labeled with the standard Western letters and symbols, including the alphabet, numbers, and punctuation marks. The laptop appears to be a modern model, possibly a MacBook, given the design and the visible trackpad to the right of the keyboard. The trackpad is a flat, rectangular surface with a grid pattern, typical of MacBook trackpads. The laptop is open, and the keyboard is the main focus of the image. The background is blurred, but it seems to be an indoor setting with a wooden surface, possibly a table or desk. The lighting in the image is soft, suggesting an indoor environment with ambient lighting.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a black dog swimming in a body of water. The dog appears to be a medium to large breed, with a glossy black coat. It is captured mid-swim, with its head above the water and its body partially submerged. The water around the dog is rippling, indicating movement, and the surface of the water reflects the light, suggesting it is a sunny day. There are no visible texts or distinguishing marks that provide additional context about the location or the dog's identity. The style of the image is a candid, real-life photograph, capturing a moment of the dog's activity.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> How many throw pillows are on the bed?<br/> <strong>A:</strong> three </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> Which iconic landmark is on the picture?<br/> <strong>A:</strong> The Eiffel Tower </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> What 2 colors are the flowers?<br/> <strong>A:</strong> red and yellow </td> </tr> </tbody> </table> <h2 id="comparison-to-byte-latent-transformers-blts">Comparison to Byte Latent Transformers (BLTs)</h2> <p>A recent concurrent work, Byte Latent Transformers (BLTs) <d-cite key="pagnoni2024blt"></d-cite>, also explores tokenization-free language models and offers an in-depth analysis of BLTsâ€™ behavior at scale. BLTs introduce an elegant framework that first encodes byte sequences into patches and then processes them globally.</p> <p>The main difference between BLTs and EvaByte lies in the <strong>architecture</strong>: BLTs use patchification and propose entropy patching to dynamically group bytes. While this approach adjusts compute allocation based on data complexity and reduces context length, it still relies on external models to determine patch boundaries. The majority of compute ends up focused on patch-level modeling, detached from the byte stream, similar to tokenizer-based models.</p> <p>In contrast, <strong>EvaByte keeps things simple</strong>: it directly operates on bytes with a flat Transformer-like model without needing to invoke external modules or group inputs. Empirically, EvaByte achieves better performance than BLTs even with 3-4x fewer training bytes, as shown in the table below. Besides, EvaByte is more flexible and scales easily to multimodal data, while BLTs require retraining or swapping out the auxiliary language model used for entropy patching.</p> <div class="row mt-1"> <div class="col-sm-11 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Table: we closely follow the evaluation setup in BLTs, testing zero-shot task performance on Arc-e, Arc-c, HellaSwag, PIQA, and HumanEval; 3-shot for the original MBPP split; and 5-shot for MMLU.</figcaption> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>We introduce EvaByte, a new family of efficient, scalable, and flexible byte-level language models. The ability to rival tokenization-based LMs with 5x less data while being faster highlights the significant potential of lower-level language modeling within the EvaByte architecture. Future research directions include further refining the modelâ€™s architecture to improve both its capacity and efficiency, analyzing in depth how lower-level language models scale with increasing sizes and data volume, as well as extending the context length to seamlessly process diverse data types â€“ images, videos, and audio â€“ simultaneously.</p>]]></content><author><name>Lin Zheng</name></author><category term="language-models"/><category term="efficient-attention"/><summary type="html"><![CDATA[Introducing EvaByte, an efficient and strong byte-level language model]]></summary></entry><entry><title type="html">Randomized Attention: a Generalized Random Feature Attention Algorithm</title><link href="https://hkunlp.github.io/blog/2022/lara/" rel="alternate" type="text/html" title="Randomized Attention: a Generalized Random Feature Attention Algorithm"/><published>2022-07-12T00:00:00+00:00</published><updated>2022-07-12T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2022/lara</id><content type="html" xml:base="https://hkunlp.github.io/blog/2022/lara/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} $$ </div> <h2 id="overview">Overview</h2> <p>This blog post introduces a new perspective to understanding the <strong>Random Feature Attention (RFA)</strong> mechanism. We show that <strong>1)</strong> the conventional softmax attention can be equivalently rewritten as an <strong>expectation over RFAs</strong>, and that <strong>2)</strong> RFA is in fact a <strong>self-normalized importance sampler</strong> to estimate conventional softmax attention. This new perspective grounds the heuristic RFA approximation and also sheds light on how to generalize further and improve RFAs. More details can be found in our ICML paper <d-cite key="lara"></d-cite>.</p> <h2 id="attention">Attention</h2> <p>The attention mechanism <d-cite key="bahdanau2014neural,vaswani2017attention"></d-cite> has become a ubiquitous building block in modern deep learning models and brought great success across various domains, including natural language processing (NLP), computer vision (CV), bioinformatics, reinforcement learning, etc. Attention mechanisms take three different kinds of inputs: a set of $N$ query vectors $\mbQ \in \R^{N \times D}$, $M$ key vectors $\mbK \in \R^{M \times D}$ and value vectors $\mbV \in \R^{M \times D}$.<d-footnote> In this work we focus on self-attention, where $M=N$ and all of queries, keys and values are obtained by projecting tokens of the same input sequence. </d-footnote></p> <div class="row mt-3"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/attn-sketch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For each query $\mbq_n$, conventional softmax attention computes the following quantity,<d-footnote> We omit the commonly used scaling factor $1 / \sqrt{d}$ for simplicity as it can be merged into the computation of queries or keys.</d-footnote> \begin{equation} \mathsf{SoftmaxAttn}\left(\mbq_{n},\mbK,\mbV\right)\coloneqq\sum_{m=1}^M\frac{\exp\left(\mbq_{n}^\top \mbk_{m} \right)}{\sum_{mâ€™=1}^M \exp\left(\mbq_{n}^\top \mbk_{mâ€™} \right)} \mbv_{m}^{\top}. \end{equation} Intuitively, softmax attention first compares the query against each key and then computes the average over value vectors weighted by the normalized query-key similarities. It is effective in capturing long-term dependencies across sequence elements and producing contextualized representations; however, it suffers from <strong>quadratic</strong> time and memory complexity due to the explicit computation of all $NM$ query-key pairs.</p> <div class="row mt-3"> <div class="col-sm-8 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="random-feature-attention">Random Feature Attention</h2> <p>To reduce the computational complexity of softmax attention, researchers proposed to use <strong>random features (RF)</strong> <d-cite key="random-features"></d-cite> to linearize softmax attention. In particular, they make use of the following identity to rewrite the <strong>exponential kernel</strong> as an expectation, \begin{equation} \exp(\mbx^\top \mby) = \mathbb{E}_{\omega \sim \mathcal{N}(\omega;0,\mathbf{I})}\left[\xi(\mbx,\omega)^\top\xi(\mby, \omega)\right], \label{eqn:identity} \end{equation} where $\xi(\cdot, \cdot): \R^D \times \R^D \rightarrow \R^l$, $l\geq 1$, is a non-linear <strong>randomized mapping</strong> projecting the input vector to a $l$-dimensional vector via a randomly drawn $\omega \sim \mathcal{N}(\omega;0,\mathbf{I})$. There are several parameterization choices of $\xi(\mbx,\omega)$ for the identity to hold; in this work we focus on the positive type $\xi(\mbx,\omega) = \exp{\left(\omega^\top \mbx - \frac{1}{2}\norm{\mbx}^2\right)}$ as proposed in <d-cite key="choromanski2021rethinking"></d-cite>.<d-footnote> A classical choice of randomized mappings is to let $\xi(\mbx,\omega) = \exp{\left( \frac{1}{2}\norm{\mbx}^2\right)}\left[\sin{\left(\omega^\top \mbx\right)},\cos{\left(\omega^\top \mbx\right)}\right]^\top$ <d-cite key="random-features,peng2021rfa"></d-cite>. Besides, there are other advanced randomized mappings enjoying more appealing properties; see <d-cite key="choromanski2022hybrid,likhosherstov2022chefs"></d-cite> for a more in-depth study. </d-footnote></p> <p> To estimate the expectation in \eqref{eqn:identity}, one can draw multiple Monte Carlo samples from $\mathcal{N}(\omega;0,\mathbf{I})$ such that $\exp(\mbx^{\top} \mby) \approx \frac{1}{S}\sum_{s=1}^S \xi(\mbx,\omega_s)^{\top}\xi(\mby, \omega_s)$. By substituting such approximation into the softmax attention, we obtain <b>random feature attention (RFA)</b> <d-cite key="peng2021rfa"></d-cite> (also called <b>Performer</b> <d-cite key="choromanski2021rethinking"></d-cite>), $$ \begin{align} \frac{\sum_{m=1}^M\exp\left(\mbq_{n}^\top \mbk_{m} \right)\mbv_{m}^{\top}}{\sum_{m'=1}^M \exp\left(\mbq_{n}^\top \mbk_{m'} \right)} &amp;\approx \frac{\sum_{m=1}^M \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{m'=1}^M\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\xi(\mbk_{m'}, \omega_s)} \notag \\ &amp;=\frac{ \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)} \label{eqn:rfa}\\ &amp;\coloneqq \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) \notag. \end{align} $$ Thanks to the linearized formulation, one can first pre-compute the corresponding key-value statistics $\sum_{m=1}^M\xi(\mbk_{m},\omega_s)\mbv_{m}^{\top}$ and $\sum_{m=1}^M\xi(\mbk_{m},\omega_s)$ once, and then reuse them for each query. Consequently, it achieves <b>linear</b> complexity in both time and memory with respect to the sequence length. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </p> <div class="row mt-3" style="border: 1px solid #000;background-color: #ebebeb; border-radius: 10px; padding-top: 30px"> <p> <b> <font size="+2"> Why it is called random feature attention? </font> </b> <br/> This is due to the fact that the sample average can be written as $\mbphi(\mbx,\mbw)^\top \mbphi(\mby,\mbw)$, where $\mbphi(\mbx,\mbw) \coloneqq 1/\sqrt{S}[\xi(\mbx,\omega_1), \dots, \xi(\mbx, \omega_S)]^\top \in \R^{lS}$. The $\mbphi(\cdot,\cdot)$ can be considered as a feature map transforming the input vector to a new vector representation; as a result, $\mbphi(\cdot,\cdot)$ are conveniently referred to as <b>random features</b> <d-cite key="random-features"></d-cite>.</p> <div class="col-sm-3 col-lg-3 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-biasedness-of-rfa">The Biasedness of RFA</h2> <p>RFA suffers from significant performance degradation across various tasks, as observed in many previous studies (e.g., see <a href="#comp-tb">here</a>). Although computationally efficient, the reduced modeling capacity of RFA greatly limits its practical usage. Researchers try to improve the performance from different perspectives, such as</p> <ul> <li>developing better random feature types <d-cite key="likhosherstov2022chefs, choromanski2022hybrid, chowdhury2021learning"></d-cite>,</li> <li>connecting it with fast weight programmers <d-cite key="pmlr-v139-schlag21a, irie2021going"></d-cite>,</li> <li>incorporating relative information <d-cite key="spe, luo2021stable, ripple, zhen2022cosformer"></d-cite>, and so on.</li> </ul> <p>In this work, we explore an orthogonal axis by re-examining the estimation bias of RFA. Our key observation is that RFA is a heuristic approximation to the <strong>whole</strong> softmax attention. Although the estimation of individual <strong>exponential kernels</strong> is unbiased, such estimation of a ratio of exponential kernels is not unbiased anymore. This is due to the non-linearity of ratios,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 mb-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Such bias not only incurs a potentially large approximation gap between RFA and softmax attention but also bottlenecks the effectiveness of unbiased kernel estimation.</p> <div class="row mt-3"> <p> Our work aims to address the following research question:<br/> <b>Given that we already know how to unbiasedly estimate exponential kernels, how do we construct an unbiased estimator for the whole softmax attention?</b> </p> <div class="col-sm mt-3 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/lara-motivation.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="randomized-attention-an-unbiased-estimator-for-softmax-attention">Randomized Attention: An Unbiased Estimator for Softmax Attention</h2> <p> We answer this question in the affirmative and prove that softmax attention can be rewritten as an expectation of simple RFAs, $$ \begin{equation} \mathsf{SoftmaxAttn}(\mbq_n, \mbK,\mbV) = \sum_{m} \frac{\exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m} \right)}{\sum_{m'} \exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m'} \right)} \mathbf{v}_{m}^{\top} = \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right],\label{eqn:softmax_as_expectation} \end{equation} $$ where $$ \begin{align} p_n(\omega) &amp;= \sum_{m=1}^M \frac{\exp\left( \mbq_n^\top\mbk_m \right)}{\sum_{m'=1}^M\exp\left( \mbq_n^\top\mbk_{m'} \right)} \mathcal{N}(\omega; \mbq_n + \mbk_m, \mathbf{I}), \label{eqn:ra-density} \\ f_n(\omega) &amp;= \frac{\xi(\mbq_n,\omega)^\top \sum_{m=1}^M \xi(\mbk_m, \omega) \mbv_{m}^{\top}}{\xi(\mbq_n,\omega)^\top \sum_{m'=1}^M \xi(\mbk_{m'}, \omega)} \label{eqn:ra-function}. \end{align} $$ Intuitively, <ul> <li>$p_n(\omega)$ is a Gaussian mixture distribution whose component weights are exactly attention scores, while </li> <li>$f_n(\omega)$ is an <b>RFA</b>, except that the linearized similarity between queries and keys is computed via individual <b>randomized mappings</b> (or equivalently, <b>one-dimensional</b> random features).</li> </ul></p> <p> Notably, our result can be viewed as a neat generalization of random feature approximation, which exhibits a high degree of symmetry: $$ \begin{align} \exp(\mbq_n^\top \mbk_m)\mbv_m^\top &amp;= \mathbb{E}_{q(\omega)}\left[\xi(\mbq_n,\omega)^\top\xi(\mbk_m, \omega)\mbv_m^\top\right] \notag \\ \sum_{m} \frac{\exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m} \right)}{\sum_{m'} \exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m'} \right)} \mathbf{v}_{m}^{\top} &amp;= \mathbb{E}_{p_n(\omega)}\left[\frac{\xi(\mbq_n,\omega)^\top \sum_{m=1}^M \xi(\mbk_m, \omega) \mbv_{m}^{\top}}{\xi(\mbq_n,\omega)^\top \sum_{m'=1}^M \xi(\mbk_{m'}, \omega)}\right].\notag \end{align} $$ Another implication is that we can construct a Monte Carlo estimator to approximate softmax attention in an <b>unbiased</b> way. By drawing $\omega_n \sim p_n(\omega)$, we obtain $$ \begin{align} \mathsf{SoftmaxAttn}(\mbq_n, \mbK,\mbV) &amp;\approx \frac{\xi(\mbq_n,\omega_n)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_n) \mbv_{m}^{\top}}{ \xi(\mbq_n,\omega_n)^\top \sum_{m'=1}^M\xi(\mbk_{m'}, \omega_n)} \label{eqn:ra}\\ &amp;\coloneqq \mathsf{RA}\left(\mbq_{n},\mbK,\mbV\right) \notag \end{align} $$ We name such estimator <b>Randomized Attention (RA)</b> since it computes similarity scores with individual <b>randomized mappings (instead of concatenated features)</b>. To the best of our knowledge, this is the first result that generalizes the unbiased <b>kernel</b> estimation to unbiased <b>attention</b> estimation. </p> <div class="row mt-3" style="border: 1px solid #000; background-color: #ebebeb; border-radius: 10px; padding-top: 20px"> <p> <b> Remark: </b> The proof of \eqref{eqn:softmax_as_expectation} is done by first reverse-engineering the formulation of RFA, equating it with self-normalized importance sampling (see below) and then completing the square of Gaussians to derive the density $p_n(\omega)$. The function $f_n(\omega)$ can be solved by substituting the density $p_n(\omega)$ into the equation. See the paper for a detailed proof.</p> </div> <h2 id="rfa-as-a-self-normalized-importance-sampler">RFA as a Self-normalized Importance Sampler</h2> <p>The analysis above further reveals that RFA is a specific <strong>self-normalized importance sampling</strong> estimator to softmax attention.</p> <h3 id="self-normalized-importance-sampling-snis">Self-normalized Importance Sampling (SNIS)</h3> <p> <b>Importance sampling (IS)</b> is a general approach to approximating expectation $\mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right]$ when it is difficult to directly draw samples from $p_n(\omega)$. In importance sampling, we use a <b>proposal</b> distribution $q(\omega)$ to draw samples and estimate the quantity as $$ \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right] = \mathbb{E}_{\omega \sim q(\omega)}\left[\frac{p_n(\omega)}{q(\omega)}f_n(\omega)\right] \approx \frac{1}{S} \sum_{s=1}^S \frac{p_n(\omega)}{q(\omega)} f_n(\omega_s), $$ where $\omega_1, \dots, \omega_S \sim q(\omega)$. The <b>self-normalized importance sampling (SNIS)</b> is defined as $$ \begin{equation} \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right] \approx \frac{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}f(\omega_s)}{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}} = \sum_{s=1}^S\frac{\frac{p_n(\omega_s)}{q(\omega_s)}}{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}}f(\omega_s). \label{eqn:snis} \end{equation} $$ The name <b>self-normalized</b> comes from the fact that the importance weights $p_n(\omega)/q(\omega)$ are explicitly normalized and sum to 1. </p> <h3 id="rfa-as-snis">RFA as SNIS</h3> <p> Our key finding here is that the formulation of RFA can be exactly derived from SNIS. Supposing $p_n(\omega)$ and $f_n(\omega)$ are given in \eqref{eqn:ra-density} and \eqref{eqn:ra-function} respectively, and $q(\omega) = \mathcal{N}(\omega;0,\mathbf{I})$, we have $$ \begin{align} \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{s=1}^S\textcolor{strings}{\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}}{\sum_{s=1}^S \textcolor{keywords}{\xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)}} = \frac{ \sum_{s=1}^S\textcolor{strings}{\frac{p_n(\omega_s)}{q(\omega_s)} f(\omega_s)}}{ \sum_{s=1}^S\textcolor{keywords}{\frac{p_n(\omega_s)}{q(\omega_s)}}}. \label{eqn:rfa-as-snis} \end{align} $$ This formulation provides a new understanding of RFA: it is just a specific instantiation of <b>SNIS</b> estimators for softmax attention, whose proposal distribution $q(\omega)$ is chosen to be standard Gaussian. This reveals one of the possible reasons why RFA does not work well in practice: <b>The plain standard Gaussian proposal in RFA is far away from the true Gaussian mixture (as in RA), which might lead to a large approximation gap.</b> More importantly, this view implies that we can generalize and extend RFA by using other proposal distributions or adopting other estimating schemes! </p> <h2 id="lara-generalizing-both-ra-and-rfa">LARA: Generalizing Both RA and RFA</h2> <p>So far, we have two types of estimators available for approximating softmax attention: unbiased RA and biased RFA. Besides the theoretical biasedness, how do they differ in terms of practical modeling behavior? We list a comprehensive comparison to better illustrate their main differences.</p> <ul> <li>In terms of <strong>expressiveness</strong>: <ul> <li>âœ”ï¸ RA directly draws from the true distribution $p_n(\omega)$ \eqref{eqn:ra-density}. This makes the mechanism <strong>adaptive</strong> and <strong>query-specific</strong>, since the sampling distribution depends on the query vector. As a result, RA can <strong>specialize</strong> to each query and process the whole sequence at a finer-grained level.</li> <li>âŒ RFA suffers from limited capacity, since there is a large discrepancy between the used proposal $q(\omega) = \mathcal{N}(\omega;0,\mathbf{I})$ and the true distribution $p_n(\omega)$; furthermore, $q(\omega)$ captures no contextual information, leading to low sample efficiency.</li> </ul> </li> <li>In terms of <strong>efficiency</strong>: <ul> <li>âŒ RA suffers from <strong>quadratic</strong> computational costs. Since the sampling distribution is distinct for each query, we have to draw at least $\mathcal{O}(N)$ samples for all queries; at the same time, we needs to sum over all $M$ key-value pairs at each sampled $\omega_n$ (see \eqref{eqn:ra}), which leads to $\mathcal{O}(NM)$ complexity overall, as is the case for softmax attention;</li> <li>âœ”ï¸ RFA uses SNIS to avoid computing or sampling from $p_n(\omega)$, which require quadratic-time computation. Drawing from a simpler proposal distribution allows samples to be shared across queries and still resulting in a $\mathcal{O}(M)$ complexity.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">A diagram of LARA that combines the strengths of both approaches.</figcaption> </figure> </div> </div> <p>Motivated by the comparison, we propose <b>LineAr-time Randomized Attention (LARA)</b> that attempts to get the best of both worlds, combining both the efficiency of RFA and the expressiveness of RA.</p> <ul> <li>On the one hand, LARA inherits the <strong>SNIS</strong> formulation as in RFA to achieve <strong>linear complexity</strong>;</li> <li>On the other hand, its expressiveness is significantly improved towards RA via the following modifications: <ul> <li>The proposal distribution is data-dependent so that LARA can be <strong>adaptive</strong> with respect to the contextual information;</li> <li>LARA adopts <strong>multiple</strong> proposal distributions, which can be combined in a <strong>query-specific</strong> manner.</li> </ul> </li> </ul> <p> LARA takes the following form $$ \begin{align} \mathsf{LARA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{c=1}^C\alpha_{nc}(\omega_c)\frac{p_n(\omega_c)}{q_c(\omega_c)} f_n(\omega_c)}{\sum_{c=1}^C\alpha_{nc}(\omega_c)\frac{p_n(\omega_c)}{q_c(\omega_c)}}.\label{eqn:lara} \end{align} $$ Here, <ul> <li>$q_1(\omega), \dots, q_C(\omega)$ are $C$ different proposals that depend on different query and key information;</li> <li>$\{\alpha_{nc}(\cdot)\}_{c=1}^C$ is a set of weights that controls the contribution of different proposals, which is <b>exclusive</b> to each query and satisfies $\sum_{c=1}^C \alpha_{nc}(\omega) = 1$;</li> <li>as long as $C$ is smaller than $N$, the computational complexity of LARA is linear, that is, $\mathcal{O}(CN+CM)$.</li> </ul> </p> <div class="row mt-3" style="border: 1px solid #000; background-color: #ebebeb; border-radius: 10px; padding-top: 20px"> <p> <b> Remark: </b> We provide a detailed discussion about the parameterization of our proposal distributions in Appendix G.3 of our paper. To summarize, we find the key is to let different proposals depend on different sets of query information so that they could be as query-specific as possible. A good default is to divide the whole sequence into $C$ chunks, compute the mean of queries $\{\widetilde{\mbq}_c\}_{c=1}^C$ and keys $\{\widetilde{\mbk}_c\}_{c=1}^C$ within the same chunk, and set $q_c(\omega) = \mcN(\omega;\widetilde{\mbq}_c + \widetilde{\mbk}_c, \mathbf{I})$. We find this choice works well across various benchmarks. </p> </div> <h3 id="a-unified-view-of-lara-ra-and-rfa">A Unified View of LARA, RA, and RFA</h3> <p> LARA can be equivalently written in a similar way to RA and RFA. We spell it out here to see a systematic comparison among RA, LARA, and RFA: $$ \begin{align} \mathsf{RA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\xi(\mbq_n,\omega_n)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_n) \mbv_{m}^{\top}}{ \xi(\mbq_n,\omega_n)^\top \sum_{m'=1}^M\xi(\mbk_{m'}, \omega_n)}, &amp;&amp;\textcolor{keywords}{\omega_n \sim p_n(\omega)}\notag\\ \mathsf{LARA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{c=1}^C \textcolor{strings}{\alpha'_{nc}(\omega_c)} \xi(\mbq_n,\omega_c)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_c) \mbv_{m}^{\top}}{\sum_{c=1}^C \textcolor{strings}{\alpha'_{nc}(\omega_c)} \xi(\mbq_n,\omega_c)^\top \sum_{m=1}^M \xi(\mbk_{m}, \omega_c)}, &amp;&amp;\textcolor{keywords}{\omega_c \sim q_c(\omega)}\notag\\ \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{ \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)}, &amp;&amp;\textcolor{keywords}{\omega_1,\dots,\omega_S \sim \mcN(\omega;0, \mathbf{I})} \notag \end{align} $$ where we denote $\alpha'_{nc}(\omega_c) \coloneqq \alpha_{nc}(\omega_c)\mcN(\omega_c;0, \mathbf{I})/q_c(\omega_c)$ to simplify the notation. Note that their major difference lies in the choice of sampling distributions.<br/> LARA is not designed to be a simple interpolation between RA and RFA; instead, it is a generalized estimation framework that includes both RA and RFA as its special cases. To see this, <ul> <li>LARA is equivalent to RA if $C = N$, $q_n(\omega) = p_n(\omega)$ and $\alpha_{nc}(\omega) = \delta_{nc}$ (that is, $\alpha_{nc}(\omega) = 1$ if $n = c$ and $0$ otherwise);</li> <li>LARA is equivalent to RFA if $q_c(\omega) = \mcN(\omega;0,\mathbf{I})$ and $\alpha_{nc}(\cdot)$ is constant for all $c = 1,\dots,C$ and $n = 1,\dots,N$.</li> </ul> With general proposals and weighting functions, LARA approximates softmax attention in a query-specific manner as in RA while achieving linear complexity as in RFA, effectively combining the advantages of both estimators. It is also easy to implement with a couple of lines of code. </p> <h2 id="experimental-results">Experimental Results</h2> <p>To demonstrate the effectiveness of our approach, we first visualize the approximation error of LARA to the true softmax attention outputs. Our unbiased estimate, RA, achieves the lowest MSE among these three methods and gets very close to the true softmax attention; RFA (Performer) soon plateaus at large approximation error and does not improve even with more samples, possibly due to low sample efficiency. On the other hand, LARA exhibits much lower MSE than RFA and the approximation error continually decreases as the number of proposals increases.</p> <div class="row mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Mean Squared Error (MSE) between the true softmax attention and different RF approximations under different numbers of samples (lower is better), which are evaluated on DeiT.</figcaption> </figure> </div> </div> <p>We further verify the improved performance of LARA by conducting experiments across a wide range of data modalities, including images, videos, natural language texts, and a long-sequence benchmark. From the <a name="comp-tb"> Table </a> below, we observe that</p> <ul> <li>RA performs competitively with conventional softmax attention, and</li> <li>LARA greatly improves the performance of RFA and significantly reduces the performance gap between RFA and softmax attention.</li> </ul> <table> <thead> <tr> <th style="text-align: left"><font size="2"> Model </font></th> <th style="text-align: center"><font size="2"> Complexity </font></th> <th style="text-align: center"><font size="2"> Image Classification on ImageNet (ðŸ …) </font></th> <th style="text-align: center"><font size="2"> Video Recognition on SSv2 (ðŸ …) </font></th> <th style="text-align: center"><font size="2"> Machine Translation on WMT (ðŸ …) </font></th> <th style="text-align: center"><font size="2"> Long Range Arena suite (ðŸ …) </font></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Softmax</td> <td style="text-align: center">$\mathcal{O}(N^2)$</td> <td style="text-align: center">79.9</td> <td style="text-align: center">66.5</td> <td style="text-align: center">27.5</td> <td style="text-align: center">59.08</td> </tr> <tr> <td style="text-align: left">RA</td> <td style="text-align: center">$\mathcal{O}(N^2)$</td> <td style="text-align: center">80.0</td> <td style="text-align: center">64.9</td> <td style="text-align: center">27.8</td> <td style="text-align: center">59.30</td> </tr> <tr> <td style="text-align: left">RFA</td> <td style="text-align: center">$\mathcal{O}(N)$</td> <td style="text-align: center">74.3</td> <td style="text-align: center">53.1</td> <td style="text-align: center">23.7</td> <td style="text-align: center">57.63</td> </tr> <tr> <td style="text-align: left">LARA</td> <td style="text-align: center">$\mathcal{O}(N)$</td> <td style="text-align: center"><strong>79.5</strong></td> <td style="text-align: center"><strong>63.7</strong></td> <td style="text-align: center"><strong>27.0</strong></td> <td style="text-align: center"><strong>59.12</strong></td> </tr> </tbody> </table> <p>LARA also enjoys much better scalability and could achieve SOTA results for image classification when applying it to advanced transformer architectures.</p> <div class="row mt-3"> <div class="col-sm-7 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">SOTA results on ImageNet-1k across various model architectures.</figcaption> </figure> </div> </div> <p>Finally, we evaluate the empirical efficiency of different attention methods. We note that RA runs almost twice slower than softmax attention, while its linear variant LARA runs much faster and brings marginal computational overheads compared to RFA.</p> <div class="row mt-3"> <div class="col-sm mb-0 mt-0 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/mem-time-comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Empirical memory consumption (left) and running time (right) of different attention mechanisms under different sequence lengths. Metrics are measured relative to the true softmax attention.</figcaption> </figure> </div> </div> <h2 id="conclusion-and-limitation">Conclusion and Limitation</h2> <p>In this work, we generalize the random feature attention (RFA) algorithm in two aspects:</p> <ul> <li>On the one hand, we generalize the unbiased <strong>kernel</strong> estimation (RFA) to unbiased <strong>attention</strong> estimation (RA);</li> <li>On the other hand, we uncover the self-normalized importance sampling nature of RFA and propose a general framework <strong>LARA</strong> that includes both RA and RFA as its special cases.</li> </ul> <p>LARA greatly improves the performance of RFA while also maintaining its efficiency of RFA. Our framework provides a novel perspective for understanding and improving RF-based attention approximation, which is also orthogonal to most previous work. At the same time, there are several limitations of our approach:</p> <ul> <li>The choice of proposal distributions is sub-optimal. Although our modeling strategy enables a query-specific treatment as desired in RA, it needs to evaluate all the $C$ samples for each key-value pair, incurring a potential waste of computing. Future work might include more sophisticated implementations that use variable sample sets for different key-value pairs.</li> <li>Currently, LARA is still a biased (but consistent) estimator for softmax attention. It would be interesting to adopt advanced sampling strategies (e.g., Markov chain Monte Carlo (MCMC), etc.) to derive lower-bias lower-variance estimators.</li> <li>The proposal distribution should be carefully designed when dealing with auto-regressive modeling since it is non-trivial to apply the causal masking mechanism to LARA as in conventional softmax attention. Adapting the importance sampling formulation of LARA to support arbitrary auto-regressive modeling is also an interesting research direction.</li> </ul> ]]></content><author><name>Lin Zheng</name></author><category term="attention"/><category term="transformers"/><summary type="html"><![CDATA[A blog post on novel perspectives to understand random feature attention]]></summary></entry></feed>