<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://hkunlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hkunlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-25T09:24:44+00:00</updated><id>https://hkunlp.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Dream-Coder 7B</title><link href="https://hkunlp.github.io/blog/2025/dream-coder/" rel="alternate" type="text/html" title="Dream-Coder 7B"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/dream-coder</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/dream-coder/"><![CDATA[<p><strong>Team</strong>: Zhihui Xie*, Jiacheng Ye*, Lin Zheng*, Jiahui Gao*, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li‚Ä†, and Lingpeng Kong‚Ä†</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Huawei Noah‚Äôs Ark Lab</p> <p>(*Equal contribution. ‚Ä†Core advising. )</p> <h2 id="introducing-dream-coder-7b">Introducing Dream-Coder 7B</h2> <p>In a joint effort with Huawei Noah‚Äôs Ark Lab, we release <strong>Dream-Coder 7B</strong>, the first fully open-source <strong>diffusion LLM for code</strong> that provides complete transparency throughout its development pipeline, with all components publicly available ‚Äì data processing scripts, implementation code, and model weights.</p> <p>Text diffusion models represent a fundamental shift away from autoregressive LLMs. This emerging direction has attracted significant attention across academia and industry<d-cite key="dream2025,nie2025large,khanna2025mercury,geminidiffusion"></d-cite>, with startups like <a href="https://www.inceptionlabs.ai/introducing-mercury">Mercury</a> pioneering diffusion LLMs for code generation. Compared to autoregressive models, diffusion-based approaches offer greater generation diversity, improved robustness, and better capture of complex, multi-modal code structures. As a diffusion-based language model demonstrating competitive performance with autoregressive code LLMs at the same scale, Dream-Coder 7B Instruct achieves <strong>21.4% pass@1 on LiveCodeBench</strong> (2410-2505), a remarkable result given that it was <strong>trained exclusively on publicly available datasets</strong>.</p> <p><a href="https://github.com/DreamLM/Dream-Coder"><strong>üë®‚Äçüíª¬†Github</strong></a> <a href="https://huggingface.co/Dream-org/Dream-Coder-v0-Instruct-7B"><strong>ü§ó¬†HuggingFace</strong></a></p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/image.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Instruct model performance comparison on coding benchmarks. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-6"> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure1"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_lcb.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1. Sketch-First Generation (from LiveCodeBench)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure4"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_short.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 4. Variable-Length Code Infilling I</figcaption> </figure> </div> </div> <div class="col-sm-6 "> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure2"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_bcb.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2. Left-to-Right Generation (from BigCodeBench)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure3"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/fig_history_cruxeval.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 3. Interleaved Reasoning Generation (from CRUXEval)</figcaption> </figure> </div> <div class="mt-1 mt-md-0" style="float:none;margin:auto;" id="figure5"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/code_infilling_dreaon_from_long.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 5. Variable-Length Code Infilling II</figcaption> </figure> </div> </div> </div> <h2 id="features">Features</h2> <h3 id="flexible-code-generation"><strong>Flexible Code Generation</strong></h3> <p>We observe Dream-Coder 7B exhibits emergent any-order generation that adaptively determines its decoding style based on the coding task. For example, Dream-Coder 7B Instruct displays patterns such as:</p> <ul> <li><strong>Sketch-first generation</strong>: For problems that read inputs from standard input and write outputs to standard output (<a href="#figure1">Figure 1</a>), Dream-Coder 7B Instruct begins by sketching the entire entry-point scaffold, then works backward to implement and refine helper functions and core logic.</li> <li><strong>Left-to-right generation</strong>: For single-function completions (<a href="#figure2">Figure 2</a>), Dream-Coder 7B Instruct writes almost linearly‚Äîstarting at the def header and moving left-to-right.</li> <li><strong>Interleaved reasoning generation</strong>: For code reasoning tasks that require predicting output from code and input (<a href="#figure3">Figure 3</a>), Dream-Coder 7B Instruct first echoes the given input, then walks through the program step by step, jotting down each calculation and filling in output lines as soon as it figures them out.</li> </ul> <p>These demos were collected using consistent sampling parameters: <code class="language-plaintext highlighter-rouge">temperature=0.1, diffusion_steps=512, max_new_tokens=512, alg="entropy", top_p=1.0, alg_temp=0.0, and pad_penalty=3.0</code>.</p> <h3 id="variable-length-code-infilling"><strong>Variable-Length Code Infilling</strong></h3> <p>One of the biggest challenges for diffusion LLMs is their lack of natural capability to generate variable-length sequences. This limitation is particularly problematic for infilling‚Äîgenerating code that seamlessly fits between existing snippets. We introduce an infilling variant, <strong>DreamOn-7B</strong> , that naturally adjusts the length of masked spans during generation by introducing two special tokens, <code class="language-plaintext highlighter-rouge">&lt;|expand|&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;|delete|&gt;</code>, which dynamically expand or contract the mask region to match the required infill length (<a href="#figure4">Figure 4</a> and <a href="#figure5">Figure 5</a>). This capability allows the model to handle variable-length code infilling tasks more effectively, without prior knowledge of the target sequence length.</p> <p>For more details, please refer to our accompanying blog post for our variable-length generation method <a href="https://hkunlp.github.io/blog/2025/dreamon/">DreamOn</a>.</p> <h2 id="adaptation">Adaptation</h2> <p>Dream-Coder 7B belongs to the family of discrete diffusion models <d-cite key="zheng2023reparameterized"></d-cite> that generate tokens through denoising from mask tokens. Building on our previous work <d-cite key="dream2025,gong2024scaling"></d-cite>, we adapt from Qwen2.5-Coder 7B base <d-cite key="hui2024qwen2"></d-cite> using 322B training tokens. Our training data comprises a carefully curated mixture of code, math, and general datasets, including <a href="https://huggingface.co/collections/OpenCoder-LLM/opencoder-datasets-672e6db6a0fed24bd69ef1c2">OpenCoder</a>, <a href="https://huggingface.co/datasets/HuggingFaceTB/stack-edu">Stack-Edu</a>, <a href="https://huggingface.co/datasets/allenai/dolmino-mix-1124">Dolmino</a>, and¬†<a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline</a>. We apply Context-adaptive Token-level Noise Rescheduling introduced in Dream <d-cite key="dream2025"></d-cite> to dynamically adjust noise levels based on context complexity. Dream-Coder 7B is able to achieve top-tier coding performance among open autoregressive and diffusion language models, while possessing general language understanding, math, and science reasoning abilities.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image1-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/image1.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Base model performance comparison on coding benchmarks. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image2-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/image2.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Performance comparison of code base models regarding general, math and science reasoning abilities. We mark models trained on open-source data with ‚úì, and those trained on in-house data with ‚úó. The best results among open-weight diffusion language models are bolded.</figcaption> </figure> </div> </div> <h2 id="post-training">Post-training</h2> <p>Our post-training recipe consists of supervised fine-tuning and reinforcement learning from verifiable rewards.</p> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>We use 5 million high-quality training samples from <a href="https://huggingface.co/datasets/inclusionAI/Ling-Coder-SFT">Ling-Coder-SFT</a>, which includes a diverse range of programming tasks across multiple languages and corpora.</p> <p>In our early experiments, we observed low sample efficiency and generation instability issues due to losses on [PAD] tokens (used as end-of-sequence markers). Specifically, when applying a simple max-length padding strategy, we observed:</p> <ul> <li><strong>Low sample efficiency</strong>: A large portion of compute is wasted on [PAD] tokens, dominating the loss and causing overfitting while slowing effective token learning.</li> <li><strong>Generation instability</strong>: Since responses are all padded with [PAD], the model tended to produce short outputs during inference.</li> </ul> <p>To address these issues, we implement <strong>Random Truncation</strong> and <strong>[PAD] penalty</strong>. As illustrated below, we randomly select a sample from the batch and truncate responses based on its length during training. This improves sample efficiency and avoids over-padded outputs. During inference, we apply a penalty to the logits of the [PAD] token to prevent its premature generation. This penalty term is gradually annealed as decoding progresses. Through this mechanism, the model initially prioritizes generating meaningful tokens and considers termination in the later decoding stage. Additionally, as in adaptation training, we apply Context-adaptive Token-level Noise Rescheduling to dynamically adjust noise based on context complexity to improve training efficacy.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image3-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/image3.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Illustration of the random truncation technique. P: prompt; R: response; white areas: [PAD] tokens.</figcaption> </figure> </div> </div> <h3 id="reinforcement-learning-with-verifiable-rewards">Reinforcement Learning with Verifiable Rewards</h3> <p>Inspired by the success of RL with verifiable rewards <d-cite key="jaech2024openai,guo2025deepseek"></d-cite>, we further conduct RL training with open-source code datasets. We curate a high-quality training set from <a href="https://huggingface.co/datasets/KodCode/KodCode-V1">KodCode-V1</a>, <a href="https://huggingface.co/datasets/agentica-org/DeepCoder-Preview-Dataset">DeepCoder-Preview-Dataset</a>, and <a href="https://huggingface.co/datasets/LLM360/guru-RL-92k">guru-RL-92k</a>. We use Qwen2.5-Coder 7B Instruct to exclude prompts with entirely correct responses (out of 8 per prompt). To prevent reward hacking and ensure prompt diversity, we filter out samples with fewer than 5 unit tests and deduplicate similar prompts. The final training set contains 17k balanced prompts across function calling, standard I/O, and simulation tasks.</p> <p>We use the GRPO algorithm <d-cite key="shao2024deepseekmath"></d-cite> with several notable improvements inspired by prior work <d-cite key="yu2025dapo,gong2025diffucoder,Polaris2025"></d-cite>:</p> <ul> <li><strong>No entropy &amp; KL loss</strong>: We eliminate entropy loss as it often results in training instability. Likewise, KL loss prevents exploration and requires additional computation for the reference policy.</li> <li><strong>Clip-higher</strong>: We use an asymmetric clipping range for importance sampling ratios in policy updates, raising the upper bound to encourage exploration of low-probability tokens.</li> <li><strong>Coupled sampling</strong>: For each batch, we sample complementary masks to estimate token-level log-likelihoods, which increases sample efficiency and reduces variance.</li> <li><strong>Intra-batch informative substitution</strong>: For each batch, we randomly duplicate samples with nonzero advantages to replace those yielding zero advantage, ensuring every batch provides informative learning signals.</li> </ul> <p>To speed up training, we adopt Fast-dLLM <d-cite key="wu2025fastdllmtrainingfreeaccelerationdiffusion"></d-cite> for rollout generation. We use binary rewards of whether the generated code is formatted correctly and passes all unit tests, leveraging deployed sandboxes <d-cite key="liu2024fullstack"></d-cite> that execute code snippets in parallel for verification.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-07-15-dream-coder-imgs/image4-1400.webp"/> <img src="/assets/img/2025-07-15-dream-coder-imgs/image4.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"></figcaption> </figure> </div> </div> <p>Our final model, Dream-Coder 7B Instruct, delivers strong performance across standard benchmarks, including HumanEval, MBPP, BigCodeBench, LiveCodeBench, and CRUXEval. Notably, trained solely on publicly available data, Dream-Coder 7B Instruct outperforms OpenCoder 8B Instruct <d-cite key="Huang2024OpenCoderTO"></d-cite>, highlighting the competitiveness of diffusion LLMs over the autoregressive approach in coding tasks. On LiveCodeBench (2410-2505), our model achieves 21.4% pass@1, approaching the performance of proprietary models like Mercury Coder Small (22.9%).</p> <h2 id="conclusion">Conclusion</h2> <p>Dream-Coder 7B represents a continuation of our efforts to enhance open-source diffusion LLMs, with particular focus on post-training improvements. Trained entirely on open-source data, it delivers competitive performance in code generation. Future efforts will explore context extension and improved data curation to further boost Dream models‚Äô capabilities.</p> <h2 id="citation"><strong>Citation</strong></h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">dreamcoder2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Dream-Coder 7B}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/dream-coder}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Zhihui and Ye, Jiacheng and Zheng, Lin and Gao, Jiahui and Dong, Jingwei and Wu, Zirui and Zhao, Xueliang and Gong, Shansan and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Zhihui Xie</name></author><category term="language-models"/><category term="diffusion-models"/><summary type="html"><![CDATA[Introducing Dream-Coder 7B, the most powerful open diffusion large language model for code to date.]]></summary></entry><entry><title type="html">DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-Size Canvas</title><link href="https://hkunlp.github.io/blog/2025/dreamon/" rel="alternate" type="text/html" title="DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-Size Canvas"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/dreamon</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/dreamon/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} \newcommand{\mbtheta}{\mathbold{\theta}} \newcommand{\expandtoken}{[\texttt{EXPAND}]} \newcommand{\deletetoken}{[\texttt{DELETE}]} \newcommand{\masktoken}{[\texttt{MASK}]} \newcommand{\padtoken}{[\texttt{PAD}]} $$ </div> <p><strong>Team:</strong> Zirui Wu*, Lin Zheng*, Zhihui Xie, Jiacheng Ye, Jiahui Gao, Yansong Feng, Zhenguo Li, Victoria W., Guorui Zhou , Lingpeng Kong</p> <p>*: Equal Contribution</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Kuaishou Technology, Huawei Noah‚Äôs Ark Lab, Peking University</p> <h2 id="introducing-dreamon">Introducing DreamOn</h2> <p>In this post, we introduce a simple yet effective training method to unleash the full potential of diffusion language models for variable-length generation. Built upon existing masked diffusion models, our approach features</p> <ul> <li>Flexible generation from any-length sequences</li> <li>Simple and practical implementation with two special sentinel tokens for length control</li> <li>Easy finetuning on existing masked diffusion models</li> <li>Catch up with oracle-length performance on infilling tasks</li> </ul> <p><a href="https://github.com/DreamLM/DreamOn"><strong>üë®‚Äçüíª¬†Github</strong></a> <a href="https://huggingface.co/Dream-org/DreamOn-v0-7B/"><strong>ü§ó¬†HuggingFace</strong></a></p> <h3 id="effective-variable-length-generation-on-infilling">Effective Variable-length Generation on Infilling</h3> <p>Although Diffusion Language Models (DLMs) have recently gained significant attention <d-cite key="austin2021d3pm,hoogeboom2021multinomialdiffusion,zheng2023rdm,lou2024sedd,sahoo2024simplemdm,shi2024md4,nie2025llada,ye2025dream,labs2025mercury"></d-cite>, they face a critical limitation: they require a fixed-size canvas to be specified in advance, making variable-length generation a long-standing and difficult problem. This restriction arises from standard discrete diffusion formulations that merely transmit tokens between different states in-place over a predetermined canvas size.</p> <p>This limitation makes it challenging for DLMs to tackle flexible generation in real-world applications, such as infilling, where the content length must be specified a priori. To illustrate, we evaluate the performance of our <a href="https://hkunlp.github.io/blog/2025/dream-coder/">Dream-Coder-7B</a> on code infilling tasks, where the model is asked to fill the missing span given a prefix and suffix context. When the given mask length does not align with the length of the canonical solution, it struggles to infill the code and pass@1 drops by 35.5% compared with oracle-length performance.</p> <p>In this work, we present <strong>DreamOn</strong> (<u>D</u>iffusion <u>Rea</u>soning <u>M</u>odel with Length C<u>on</u>trol), a novel discrete diffusion algorithm designed to address the variable-length generation challenge in code infilling. Our approach enables <strong>dynamic expansion and contraction of mask tokens</strong> during inference, providing flexible length control without requiring predetermined canvas sizes.</p> <div class="row mt-1"> <div class="col-sm-6 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_short.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_short.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_short.gif-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_short.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">With too few masked tokens, diffusion models lack sufficient room for meaningful code infilling.</figcaption> </figure> </div> <div class="col-sm-6 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_long.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_long.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_long.gif-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/code_infilling_dream_from_long.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Too many masked tokens cause overgeneration of unnecessary code snippet depth &gt; 0 that is incorrect.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-6 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_short.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_short.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_short.gif-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_short.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">DreamOn adds mask tokens as needed.</figcaption> </figure> </div> <div class="col-sm-6 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_long.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_long.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_long.gif-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/code_infilling_dreaon_from_long.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">DreamOn deletes excess mask tokens.</figcaption> </figure> </div> </div> <p>We believe that enabling variable-length sequence generation opens new avenues for DLMs, unlocking their potential for more sophisticated applications including adaptive prompting, flexible infilling, and seamless editing workflows, particularly in programming contexts where content length is inherently unpredictable.</p> <h3 id="dreamon-masked-diffusion-with-augmented-states">DreamOn: Masked Diffusion with Augmented States</h3> <p>DreamOn extends standard masked diffusion models <d-cite key="austin2021d3pm, hoogeboom2021multinomialdiffusion,zheng2023rdm,lou2024sedd,sahoo2024simplemdm,shi2024md4,ou2025radd,zheng2025masked"></d-cite> by introducing two special states <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†and¬†<code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> to enable precise length control. We define them in such a way that in the forward diffusion process, ¬†tokens in both <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†and <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code>¬†are always transmitted to¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code> ; and during the backward process, <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†is deterministically expanded into two¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†tokens at the same position, while <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> is removed from the sequence. This design allows the model to dynamically adjust sequence length.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/augmented-diffusion-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/augmented-diffusion-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/augmented-diffusion-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/augmented-diffusion.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"></figcaption> </figure> </div> </div> <p>To train the model with these special states, we construct an auxiliary sequence¬†$\mbz_0$ from each original sequence $\mbx_0$¬†by 1) randomly merging token spans in $\mbx_0$¬†into <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code> , and 2) inserting a random number of¬†tokens with state <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> . As illustrated in the diagram below, $\mbz_0$ typically differs in length from $\mbx_0$. We then train the masked diffusion model on $\mbz_0$ instead of diffusing over $\mbx_0$, and by doing so, the model learns to denoise not only regular tokens but also special states from <code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>, thus achieving effective variable-length generation.</p> <h3 id="implementation">Implementation</h3> <p>Similar to <code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code> in masked diffusion models, we define the introduced states <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†and¬†<code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> as special sentinel tokens in the tokenizer vocabulary, and train the model to denoise them just as if they were regular tokens. This formulation is appealing due to its ease of implementation ‚Äî requiring no changes to the model architecture and supporting straightforward fine-tuning from pretrained masked diffusion models.</p> <h4 id="training">Training</h4> <p>To construct $\mbz_0$, instead of merging and inserting tokens before masking (as demonstrated in the diagram above), we first apply a mask noising process over¬†$\mbx_0$, followed by random merges of consecutive¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†tokens to form¬†<code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†and random insertion of¬†<code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code>¬†tokens. This sampling scheme provides greater control over the noise level and the balance between length control and generation quality (see Section Analysis).</p> <p>We design two heuristic schedulers for constructing¬†<code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†tokens. (1) <strong>Static scheduler</strong> merges adjacency¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†tokens with a fixed merging probability $p_{merge}=0.5$. (2) <strong>Dynamic inverse scheduler</strong> merges adjacent¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†with a probability that is inversely proportional to the number of¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†tokens in the sequence. We find that mixing the two schedulers leads to the best performance in practice.</p> <p>We fine-tune <a href="https://hkunlp.github.io/blog/2025/dream-coder/"><code class="language-plaintext highlighter-rouge">DreamCoder-7B</code></a> on the education instruct subset of <a href="https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2"><code class="language-plaintext highlighter-rouge">opc-sft-stage2</code></a> from <a href="https://arxiv.org/pdf/2411.04905">OpenCoder</a>. This subset contains 110k instruction-solution pairs synthesized from seed data of high educational value. For infilling tasks, we randomly split the solution into prefix, middle, and suffix. We treat the instruction, prefix, and suffix of the solution as fixed, and only diffuse over tokens in the middle. In this case, we found it suffices to learn effective sequence contraction by appending a random number of <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> tokens at the end of the middle section. We downweight the loss of predicting¬†<code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code>¬†tokens to avoid overfitting.</p> <h4 id="inference">Inference</h4> <p>During inference, our model shows little difference from the original masked diffusion denoising, except that at each iteration, when a¬†<code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code>¬†token is predicted from¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code> , we expand it into two¬†<code style="background-color:rgb(237, 235, 235); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|mask|&gt;</code>¬†tokens in the same position; and when a¬†<code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code>¬†token is predicted, we simply remove it from the sequence. This is a crude heuristic that greedily expands or contracts sequence length at each step; however, we found it performed effectively and robustly in infilling tasks.</p> <h3 id="evaluation">Evaluation</h3> <p>We evaluate our model on HumanEval-Infilling and the Python subset of Santacoder-FIM. We report pass@1 for HumanEval-Infilling and exact match for Santacoder-FIM following official evaluation scripts. We evaluate our model with different initial mask lengths to assess its generalizability in length control. We also evaluate the performance under oracle length, with expansion and deletion disabled for this setting, to monitor the infilling capabilities for fixed-size canvas.</p> <style>figure+.caption,figure .caption{text-align:left!important}</style> <div class="row mt-1"> <div class="col-sm-12 col-md-8 col-lg-6" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/main_result-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/main_result-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/main_result-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/main_result.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Pass@1 on HumanEval-Infilling and exact match on Santacoder-FIM. We evaluate the infilling performance of diffusion language models by averaging results across given mask lengths of 4, 8, 16, 32, and 64.</figcaption> </figure> </div> </div> <p>Our results demonstrate that DreamOn achieves significant improvements over other diffusion models that lack variable-length generation capabilities in code infilling, approaching the performance of leading open-source autoregressive language models that are trained with infilling objectives.</p> <h3 id="analysis">Analysis</h3> <h4 id="performance-breakdown">Performance Breakdown</h4> <p>We perform ablation studies on our expansion and contraction design to show the effectiveness of our approach. Performance with different masked token lengths shows that DLMs trained on infilling without sentinel tokens exhibit poor performance, particularly on short sequences, while those incorporating both expansion and deletion achieve the highest pass@1 across all lengths. The combination of both mechanisms leads to a substantial improvement in pass rate (90.8% average) and exact match accuracy (73.9% average), approaching oracle performance.</p> <div class="row mt-1"> <div class="col-sm-10 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/breakdown-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/breakdown-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/breakdown-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/breakdown.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Pass@1 and exact for diffusion language model with different mask length. w/o Delete or Expand : deletion or expansion excluded from training. Oracle: performance under oracle length without expansion or deletion as reference. ‚Ä†: We use an AST parser to compute exact match to normalize huge syntactic differences between the model output and the ground truth.</figcaption> </figure> </div> </div> <p>Exact match scores highlight that our approach leads to perfect token alignment with the ground truth regardless of the length of the infilling span. Our approach offers a text diffusion process that is no longer limited to fixed-length sequences and can adapt to variable-length generation tasks at scale, such as infilling.</p> <h4 id="padding-vs-deletion">Padding vs. Deletion</h4> <p>Current DLMs often use <code class="language-plaintext highlighter-rouge">&lt;|pad|&gt;</code> tokens to fill at the end of the sequence, which can be viewed as a special form of length cutoff control. The predicted <code class="language-plaintext highlighter-rouge">&lt;|pad|&gt;</code> tokens are kept in the sequence as input for the next step of denoising. We experiment with training our model while keeping <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> tokens in the prompt and do not remove them from the sequence. The presence of <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> tokens introduces potential distraction into the denoising process, which could disrupt attention patterns or token-position alignments, especially when deletions are frequent or irregular. Our design of <code style="background-color:rgb(249, 215, 218); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|delete|&gt;</code> tokens offers greater flexibility and effectiveness for dynamic length control</p> <div class="row mt-1"> <div class="col-sm-10 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dreamcoder-infilling-imgs/ablation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dreamcoder-infilling-imgs/ablation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dreamcoder-infilling-imgs/ablation-1400.webp"/> <img src="/assets/img/dreamcoder-infilling-imgs/ablation.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Ablation study for length shortening strategy and merging scheduler for mask expansion. Oracle: performance under oracle length without expansion or deletion as reference.</figcaption> </figure> </div> </div> <h4 id="choice-of-merging-scheduler">Choice of Merging Scheduler</h4> <p>Balancing between the degree of length control and the quality of generation is crucial for robust variable-length generation. We train our model with static or dynamic inverse scheduler separately to study their effect on generalizability.</p> <p>The static scheduler offers a higher degree of length control, achieving the highest pass rate when the mask length is extremely short. However, it leads to lower quality generation compared with other schedulers. Large number of regular tokens that are replaced with <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code> tokens during training and have negative effects on language modeling abilities. The dynamic inverse scheduler addresses this problem by downweighting merged <code style="background-color:rgb(195, 252, 233); color: #2d3436; padding: 2px 2px; border-radius: 3px; font-family: monospace;">&lt;|expand|&gt;</code> tokens. But it has poor expanding performance when the mask length is too short. Therefore, we mix the two schedulers to balance length control and quality of generation.</p> <h3 id="conclusion">Conclusion</h3> <p>It remains challenging for non-autoregressive generative models to generate variable-length sequences. Prior research has explored several strategies to address this, such as learning a separate length prediction module <d-cite key="gu2018narmt, lee2018deterministic,ghazvininejad2019cmlm,zheng2023rdm"></d-cite>, contracting length with latent alignment marginalization <d-cite key="chan2020imputer"></d-cite>, incorporating edit operations <d-cite key="gu2019insertion,gu2019levenshtein,stern2019insertion,johnson2021beyond,reid2023diffuser,campbell2023transdimensional,patel2025insertionlm,havasi2025editflow"></d-cite>, and performing diffusion over sequence positions <d-cite key="zhang2025flexible"></d-cite>. Most of these methods require modifying the model architecture and have been evaluated at limited scale.</p> <p>In contrast, our approach introduces only two special tokens into the tokenizer vocabulary‚Äîrequiring no changes to the model architecture or the loss objective. This leads to a simple and scalable implementation that remains effective on infilling tasks. Notably, DreamOn even achieves code infilling performance comparable to that with oracle length, highlighting its capability to handle variable-length generation.</p> <p>This blog post presents our preliminary results on variable-length generation with DLMs. Future work will explore extensions beyond fill-in-the-middle (FIM) tasks and further improve training and inference strategies.</p> <h3 id="citation">Citation</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Dreamon2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/dreamon}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zirui and Zheng, Lin and Xie, Zhihui and Ye, Jiacheng and Gao, Jiahui and Feng, Yansong and Li, Zhenguo and W., Victoria and Zhou, Guorui  and Kong, Lingpeng}</span>
    <span class="nv">year</span> <span class="err">=</span> <span class="err">{2025</span><span class="p">}</span>
<span class="c">}</span>
</code></pre></div></div>]]></content><author><name>Zirui Wu</name></author><category term="language-models"/><category term="diffusion-models"/><category term="variable-length-generation"/><summary type="html"><![CDATA[A simple yet effective approach to unlock variable-length generation for diffusion language models.]]></summary></entry><entry><title type="html">Polaris</title><link href="https://hkunlp.github.io/blog/2025/Polaris/" rel="alternate" type="text/html" title="Polaris"/><published>2025-06-20T00:00:00+00:00</published><updated>2025-06-20T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/Polaris</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/Polaris/"><![CDATA[<h1 id="polaris-a-post-training-recipe-for-scaling-reinforcement-learning-on-advanced-reasoning-models">POLARIS: A POst-training recipe for scaling reinforcement Learning on Advanced ReasonIng modelS</h1> <p><strong>Team</strong>: Chenxin An *, Zhihui Xie‚Ä†, Xiaonan Li‚Ä†, Lei Li‚Ä†, Jun Zhang, Shansan Gong, Ming Zhong</p> <p>Jingjing Xu *, Xipeng Qiu, Mingxuan Wang, Lingpeng Kong</p> <p>*: Project Leads; ‚Ä†: Significant Contributor</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Bytedance Seed, Fudan University</p> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px;"> <p> We are thrilled to unveil our latest breakthroughs, <code>POLARIS-7B-Preview</code> and <code>POLARIS-4B-Preview</code>, which mark a new frontier in open‚Äêrecipe reasoning models developed using academic‚Äêlevel resources. <code>POLARIS-4B-Preview</code> is fine-tuned from <code>Qwen3-4B</code> and <code>POLARIS-7B-Preview</code> is fine-tuned from <code>Deepseek-R1-Distill-Qwen-7B</code>. Our 4B model achieves an impressive <strong>81.2% Pass@1 accuracy on AIME24</strong> and <strong>79.4% Pass@1 accuracy on AIME25</strong>, outperforming state-of-the-art commercial models like <code>Claude-4-Opus</code>, <code>Grok-3-Beta</code>, and <code>o3-mini-high(2025/01/31)</code> via scaling reinforcement learning on open-source data. On AIME25, POLARIS astonishingly achieves comparable performance to <code>Qwen3-235B-A22B</code> while using less than <strong>2%</strong> of its parameters and can be deployed on consumer-grade GPUs. </p> <p> To foster progress in scaling RL on advanced reasoning models, we are open-sourcing our dataset, code, and training details for the research community. </p> <p> üë®‚Äçüíª¬†<a href="https://github.com/ChenxinAn-fdu/POLARIS">Github</a> | ü§ó¬†<a href="https://huggingface.co/POLARIS-Project/Polaris-4B-Preview">HF Model</a> | ü§ó¬†<a href="https://huggingface.co/datasets/POLARIS-Project/Polaris-Dataset-53K">HF Dataset</a> | üìñ <a href="comming soon">paper</a> | üîé¬†<a href="https://github.com/ChenxinAn-fdu/POLARIS/tree/main/evaluation">Evaluation results</a> </p> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image-1400.webp"/> <img src="/assets/img/polaris-imgs/image.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div> <h4>‚úÖ Takeaways for post-training of advanced reasoning models</h4> <div style="background-color: #ffffff; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Data Difficulty:</strong> Before training, Polaris analyzes and maps the distribution of data difficulty. The dataset should not be overwhelmed by either overly difficult or trivially easy problems. We recommend using a data distribution with a slight bias toward challenging problems, which typically exhibits a mirrored J-shaped distribution. </p> </div> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Diversity-Based Rollout:</strong> We leverage the <em>diversity among rollouts</em> to initialize the sampling temperature, which is then progressively increased throughout the RL training stages. </p> </div> <div style="background-color: #ffffff; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Inference-Time Length:</strong> Polaris incorporates length extrapolation techniques for generating longer CoT at inference stage. This enables a <em>"train-short, generate-long"</em> paradigm for CoT reasoning, mitigating the computational burden of training with excessively long rollouts. </p> </div> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px;"> <p style="margin: 0;"> <strong>Exploration Efficiency:</strong> Exploration efficiency in Polaris is enhanced through multi-stage training. However, reducing the model's response length in the first stage poses potential risks. A more conservative approach would be to directly allow the model to "think longer" from the beginning. </p> </div> </div> <hr/> <h1 id="polariss-recipe">POLARIS‚Äôs Recipe</h1> <p>Current work (e.g., <a href="https://www.notion.so/19681902c1468005bed8ca303013a4e2?pvs=21">DeepscaleR</a>) demonstrates that a small model (e.g., 1.5B parameters) can achieve surprising improvements in reasoning tasks through scaling RL training. However, when we apply their recipe to train more advanced reasoning models, we observe marginal improvements even decline during the RL training of <code class="language-plaintext highlighter-rouge">Qwen3</code>. This suggests a critical gap in the open-source community‚Äôs understanding of how to further scale RL on advanced reasoning models. To address this, we introduce <strong>POLARIS</strong>‚Äîa post-training recipe centered on calibrated data difficulty, enhanced data diversity, inference-time length scaling, and efficient training.</p> <p>We are committed to transparency and will be open-sourcing our trained models, training code, and data to foster community progress.</p> <h2 id="1-data-difficulty"><strong><em>1. Data Difficulty</em></strong></h2> <p>Our POLARIS recipe builds upon a deep investigation on the training data difficulty. Specifically, we conduct controlled experiments regarding data difficulty measured by model pass rate, and choose public available training datasets to enable better reproducibility.</p> <h3 id="balanced-data-difficulty-matters">Balanced Data Difficulty Matters</h3> <p>Our initial experiments involve training models of different scales on the public <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">DeepScaleR dataset</a>. While a 1.5B model shows significant performance gains as expected, a 7B model trained on the same data exhibits only marginal improvements. We observe that the 7B model‚Äôs average reward quickly surpasses 0.7, indicating that the training set is too simple to drive further improvements.</p> <p>This leads us to a core hypothesis: <strong>For effective RL training, the difficulty of the data must be carefully calibrated to the model‚Äôs scale and capability.</strong></p> <p>To validate this, we analyze the difficulty distribution of the 40,000 samples in the DeepScaleR training set. We use <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> and its <code class="language-plaintext highlighter-rouge">1.5B</code> version to perform an offline evaluation, generating 8 solutions for each problem with a sampling temperature of 0.6. The percentage of correct solutions serves as a proxy for the difficulty of each sample.</p> <p>The results, shown in the figure below, are revealing.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%200-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%200-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%200-1400.webp"/> <img src="/assets/img/polaris-imgs/image%200.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Across both model scales, we observe that most problems are either very easy (8/8 correct solutions) or very hard (0/8 correct solutions). Crucially, we note a ‚Äúmirror effect‚Äù between the two models:</p> <ul> <li>The <strong>1.5B model</strong> shows a mirrored J-shaped (·Ç±) distribution, with most problems being extremely difficult (0/8 correct).</li> <li>The <strong>7B model</strong> shows a standard J-shaped distribution, with the vast majority of problems being far too easy (8/8 correct).</li> </ul> <p>This stark contrast confirms that the dataset, while challenging for a 1.5B model, is not sufficiently difficult to train a 7B model effectively. This insight motivates our work in curating a new, more challenging dataset tailored for advanced models.</p> <p>To confirm our hypothesis, we conduct an ablation study on the <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">DeepScaleR 40K dataset</a> using the <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> model. We create distinct training sets by systematically altering the difficulty distribution:</p> <ol> <li><strong>Full Dataset (40K samples):</strong> The dataset exhibits the original J-shaped distribution, dominated by easy samples (8/8 correct solutions).</li> <li><strong>Removal of Perfect Scores (26K samples):</strong> We remove all samples with 8/8 correct solutions, creating a mirrored J-shaped distribution.</li> <li><strong>Aggressive Filtering (19K samples):</strong> We filter out all samples with a pass rate greater than 4/8, resulting in an distribution that focuses only on the hardest problems.</li> </ol> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%201-1400.webp"/> <img src="/assets/img/polaris-imgs/image%201.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1: Model performance across the three above mentioned conditions</figcaption> </figure> </div> </div> <p>The results, as shown in Figure¬†1, clearly demonstrate that removing the easiest samples leads to consistent performance improvements. In contrast, both the unfiltered dataset (which lacks sufficient challenge) and the aggressively filtered dataset (which is overly saturated with difficult problems) hinder training progress.</p> <p>These findings confirm that optimal RL training requires <strong>a balanced difficulty distribution</strong>‚Äîone that provides enough challenging samples to drive learning while avoiding both trivial problems and overwhelming difficulty.</p> <h3 id="polariss-data-curation-strategy"><strong>POLARIS‚Äôs Data Curation Strategy</strong></h3> <p>Motivated by these findings, the POLARIS recipe curates a <strong>mirrored J-shape difficulty distribution</strong> by filtering high-quality public datasets, including DeepScaleR-40K, and <a href="https://huggingface.co/datasets/inclusionAI/AReaL-boba-Data">AReaL-boba-106k</a>.</p> <p>Our data engineering process is as follows:</p> <ol> <li><strong>Offline Difficulty Estimation:</strong> We use the specific model being trained to generate <code class="language-plaintext highlighter-rouge">8 rollouts</code> for each potential training problem. The pass rate determines the problem‚Äôs difficulty relative to that model.</li> <li><strong>Targeted Filtering:</strong> To create the desired mirrored J-shape distribution, we remove all samples that the model solves perfectly (8/8 correct).</li> <li><strong>Dataset Assembly and Calibration:</strong> <ul> <li>For training <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we applied this filtering to DeepScaleR and AReaL, creating a final training set of <strong>53K samples</strong> (26K from DeepScaleR and 27K from AReaL).</li> <li>To train the the <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> model, we performed an additional filtering pass on this 53K set, resulting in a <strong>30K sample</strong> dataset specifically calibrated to its difficulty level.</li> </ul> </li> </ol> <p>This model-specific calibration of data difficulty is a cornerstone of the POLARIS recipe, ensuring that the training process remains challenging and effective for any given model.</p> <h3 id="dynamically-drop-easy-data-during-training"><strong>Dynamically drop easy data during training</strong></h3> <p>As the RL training process progresses, the model‚Äôs capabilities will grow, and the proportion of difficult questions will decrease. Therefore, in addition to initially adjusting the difficulty distribution, we also adjust the training data distribution during the training process. Here is the figure showing the distribution shift of data difficulty during training.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%202-1400.webp"/> <img src="/assets/img/polaris-imgs/image%202.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2: Data Difficulty Distribution Shifts (Left: Before Training, Right: After-Training; Top: Qwen3-4B, Bottom: Deepseek-R1-distill-Qwen-7B)</figcaption> </figure> </div> </div> <p>We observe that the sample difficulty distribution consistently shifts from a mirrored J-shape (·Ç±) to a J-shape. This evolution reinforces our motivation to start with a ·Ç±-shaped distribution, allowing for a smooth transition to the J-shape.</p> <p>Additionally, since data difficulty changes dynamically during training, using the initial dataset throughout is suboptimal. To maintain an ideal difficulty distribution, we introduce <strong>dynamic difficulty distribution updates</strong>: during training with a rollout size of n=8, we update each sample‚Äôs accuracy after reward computation (initial difficulty determined by offline filtering accuracy). <strong>At the end of each training phase, we remove samples with <code class="language-plaintext highlighter-rouge">Accuracy &gt; 0.9</code> to preserve the initial distribution shape and prevent skewing towards a J-shape<em>.</em></strong></p> <p>This dynamic filtering ensures that the model continues to face appropriately challenging samples, preventing the learning signal from degrading due to an overabundance of mastered samples.</p> <h2 id="2-diversity-based-rollout-sampling"><em>2. Diversity-based Rollout Sampling</em></h2> <p><a href="https://arxiv.org/abs/2501.12948">In GRPO training</a>, diversity is an essential factor. GRPO‚Äôs key is to contrast positive and negative trajectories, increasing the probability of positive ones. The diversity of sampled trajectories is crucial, encompassing two aspects:</p> <ol> <li>High diversity encourages the model to generate both positive and negative trajectories in a single rollout, enhancing trajectory contrast.</li> <li>High diversity allows the model to explore a wider range of potential reasoning paths, which helps prevent the model from quickly becoming overconfident in a narrow set of patterns. We also explore the method to increase the sampling diversity. Our approach aims to achieve the best diversity while ensuring performance.</li> </ol> <p>During the rollout phase, the primary hyperparameters affecting diversity are top‚Äëp,top‚Äëk, and the sampling temperature. In previous open‚Äësource projects, the default settings are typically a top‚Äëp value of 1.0 and a top‚Äëkvalue of ‚Äì1, which together yield maximum diversity. Only the sampling temperature remains adjustable. Temperature is usually established either by following the decoding temperature recommended on the official website (e.g., 0.6) or by setting it as a hyperparameter at 1.0. Therefore, in this section, we focus on temperature to analyze how the temperature affects the RL training performance and propose adjusting the temperature during training to match the base model‚Äôs diversity.</p> <p>We start from a probing experiment to explore the relationship between sampling temperature $t$, performance (mean@32), and diversity among rollouts. To quantify the diversity of the sampled trajectories, we use the distinct N-gram metric. This metric evaluates lexical diversity by measuring the ratio of unique N-grams (contiguous sequences of n words) to the total number of N-grams across all generated outputs. In our experiments we set N=4. A score closer to 1.0 indicates higher diversity, meaning the all trajectories contain a wide variety of phrases with little repetition. Conversely, a score closer to 0 indicates low diversity, suggesting the generated outputs are highly similar or repetitive.</p> <p><strong>Diversity vs. Sampling Temperature:</strong> Higher temperatures bring better diversity. To use more diverse trajectories for training, it is recommended to increase the sampling temperature. At the same temperature, there are significant differences in diversity performance across models. For instance, <code class="language-plaintext highlighter-rouge">Qwen3</code> has fewer unique n-grams and a more concentrated output distribution.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%203-1400.webp"/> <img src="/assets/img/polaris-imgs/image%203.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 3: Rollout diversity with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes</figcaption> </figure> </div> </div> <p><strong>Performance vs. Sampling Temperature:</strong> While pursuing diverse trajectories, it is also necessary to ensure the model‚Äôs performance. When we increase the temperature from 0 to higher, all the tested models‚Äô average accuracy exhibits a low-high-low trend. We also notice that each model has significant differences in their zone spans, highlighting that there is no one-size-fits-all temperature setting . The optimal temperature for achieving a desired level of diversity is highly dependent on the specific model being used.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%204-1400.webp"/> <img src="/assets/img/polaris-imgs/image%204.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 4: Model performance with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes</figcaption> </figure> </div> </div> <h3 id="definition-temperature-zone"><strong>Definition</strong>: Temperature Zone</h3> <p>According to the trends, we can get the following sampling temperature zone empirically:</p> <ol> <li><strong>Robust Generation Zone (RGZ)</strong>: RGZ defines the zone where the model‚Äôs performance is both <code class="language-plaintext highlighter-rouge">optimal</code>and <code class="language-plaintext highlighter-rouge">stable</code><strong>,</strong> without significant increases or decreases. The suggested decoding temperature is typically from RGZ.</li> <li><strong>Controlled Exploration Zone(CEZ):</strong> Temperature in CEZ leads to slight performance degradation compared with RGZone, but the degradation level is acceptable because it leads to the increased rollout diversity.</li> <li><strong>Performance Collapse Zone (PCZ)</strong>: In PCZ, the model tends to output noisy tokens and thus the performance will be extremely low.</li> </ol> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%205-1400.webp"/> <img src="/assets/img/polaris-imgs/image%205.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For illustration, We show <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>‚Äôs accuracy on AIME24 to demonstrate the differences among these three areas in the following figure. When temperature is 0.6~1.4, the model achieves the optimal and stable performance curve. When temperature is 1.4-1.55, the performance slightly degrades but has better rollout diversity. Then increasing temperature from 1.55 to 1.7 causes significant model performance collapse, which indicates the PCZone starts from temperature=1.55. Noise begins to appear, making it unsuitable for both training and decoding.</p> <h3 id="temperature-initialization-on-controlled-exploration-zone"><strong>Temperature initialization on Controlled Exploration Zone</strong></h3> <p>Our probing experiments reveal that sampling temperature significantly impacts rollout diversity, and its optimal setting varies across base models. The recommended test temperatures are usually from <strong><code class="language-plaintext highlighter-rouge">Robust Generation Zone</code></strong> , which usually result in low diversity. An overly deterministic sampling temperature restricts the model‚Äôs ability to explore better pattern spaces. In Polaris, we propose initializing the sampling temperature based on the model‚Äôs <strong><code class="language-plaintext highlighter-rouge">Controlled Exploration Zone</code></strong> to achieve comparable performance with improved diversity.</p> <p>We recommend using the sampling temperature at the point where model performance begins to decline while maximizing diversity. For Qwen3-4B and <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we set the initial sampling temperatures to 1.4 and 0.7, respectively.</p> <p>The comparison of different temperature initialization settings shows that the most common setting, <code class="language-plaintext highlighter-rouge">t= 0.6 / 1.0</code>, causes a decline in model performance, as it is too low to allow the model to explore better trajectories. In contrast, the temperature within the Controlled Exploration Zone demonstrates the best RL performance.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%206-1400.webp"/> <img src="/assets/img/polaris-imgs/image%206.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 5: The performance trend of different sampling temperature initialization</figcaption> </figure> </div> </div> <h3 id="dynamic-temperature-reset-across-training-stages"><strong>Dynamic Temperature Reset Across Training Stages</strong></h3> <p>We also find that the model‚Äôs Robust Generation Zone and Controlled Exploration Zone shift during RL training (as shown in the following figure). Since reinforcement learning increases the probability of positive expression patterns, the model‚Äôs entropy tends to decrease and its exploration space becomes narrower, which is manifested by the convergence of N-grams in different trajectories.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:80%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%207-1400.webp"/> <img src="/assets/img/polaris-imgs/image%207.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 6: The RGZ and CEZ shift towards the high-temperature region after 800 steps of RL training</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%208-1400.webp"/> <img src="/assets/img/polaris-imgs/image%208.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 7: Model performance and rollout diversity as a function of RL training steps. The experiment uses a training/testing temperature of 0.7 and a sampling size of 32 for testing.</figcaption> </figure> </div> </div> <p>üìå Since the diversity of sample trajectories is critical for RL training, using the same sampling temperature throughout the training process can result in insufficient diversity at the later training stages and limit the potential for performance gains.</p> <p>Therefore, we propose dynamically updating the temperature during RL training. As the model converges on high-quality patterns, we will increase the sampling temperature to encourage further exploration.</p> <p><strong>After each training stage, we will use a higher sampling temperature to maintain the model‚Äôs previous diversity score.</strong></p> <p>Specifically, we test various sampling temperatures and select the one that achieves the desired diversity score. Our experiments suggest setting the temperature interval based on the previous stage‚Äôs entropy decrease. If entropy decreases slightly, we recommend a 0.05 interval for the sampling temperatures. If entropy decreases significantly, we will use a larger interval. We show the sampling temperatures of each stage of Polaris in this Table:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Stage-1</strong></th> <th><strong>Stage-2</strong></th> <th><strong>Stage-3</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-7B-Preview</code></strong></td> <td>0.7</td> <td>1.0</td> <td>1.1</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong></td> <td>1.4</td> <td>1.45</td> <td>1.5</td> </tr> </tbody> </table> <p>To verify the effectiveness of temperature increase, we conduct a baseline with the same temperature across the whole training. As we can see, the multi-stage with increased temperature leads to better RL training and further expanding the model‚Äôs thought depth by expanding the response length.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%209-1400.webp"/> <img src="/assets/img/polaris-imgs/image%209.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="3-inference-time-length-scaling"><em>**3. Inference-Time Length Scaling</em></h2> <h3 id="insufficient-long-context-training-in-rl-stage"><strong>insufficient</strong> long-context training in RL stage</h3> <p>A significant challenge in developing advanced reasoning models is the cost of long-context training. For instance, our model, based on <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, has a pre-training context length of <strong>32K</strong>. While we increase the maximum training length to 52K during RL, our experiments reveal a critical limitation. The <code class="language-plaintext highlighter-rouge">clip_ratio</code>, which measures the proportion of training samples that reach the maximum sequence length, remained below 10%. This indicates that very few samples are actually trained at the 52K length. During the RL training process, the inference time for rollouts often consumes a significant amount of resources. Within a single batch, if no additional optimizations are made, shorter samples must wait for longer samples to finish decoding, leading to wasted training time and resources. Therefore, it is not efficient to directly use a large training length. We also try some train-short, test-long methods to increase the inference length given limited training budgets.</p> <h3 id="performance-degradation-beyond-pre-training-length">Performance degradation beyond pre-training length</h3> <p>To quantify the effective Chain-of-Thought (CoT) length of <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> , we conduct an analysis using 60 problems from the AIME 2024/25 datasets. There are 32 rollouts for each problem (for a total of 1,920 rollouts) and grouped them by the length of the response:</p> <ul> <li><strong>Short Rollouts Group</strong>: Responses with a length of less than <strong>16K</strong>.</li> <li><strong>Mid-Length Rollouts Group</strong>: Responses with a length between 16K and 32K.</li> <li><strong>Long Rollouts Group:</strong> Responses with a length exceeding the <strong>32K</strong> pre-training limit.</li> </ul> <p>The <em>Accuracy</em> for each group is calculated using the following formula:</p> <p>$\text{Accuracy} = \frac{\text{Number of Correct Rollouts}}{\text{Total Rollouts in Group}}$</p> <p>The results were striking (blue bars). We observe a dramatic performance drop for responses in the <strong>Long Rollouts Group</strong>, which achieved an accuracy of only 26%.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2010-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2010-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2010-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2010.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This finding supports our hypothesis: due to the inefficiencies of long-context RL training, the model struggles to generate effective and accurate long CoTs beyond its original pre-training length, even though the RL training length is set to 52K.</p> <h3 id="training-free-length-extrapolation">Training-free Length Extrapolation</h3> <p>To address this, we introduce length extrapolation technique in long reasoning traces generation, which follows the principle of ‚Äútrain shorter, test longer.‚Äù By adjusting the model‚Äôs <a href="https://arxiv.org/abs/2104.09864">Rotary Position Embeddings (RoPE)</a>, this method allows the model to <strong>maintain its performance</strong> on sequences <strong>longer than</strong> those seen during training, effectively compensating for insufficient long-context training.</p> <p>For ease of implementation, we adopt the Yarn method with a scaling factor of <code class="language-plaintext highlighter-rouge">1.5</code>. While Yarn recommends adjusting the attention temperature during extrapolation, we find that this modification‚Äîthough beneficial for long-context retrieval tasks‚Äîis detrimental for generating long reasoning sequences.</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"rope_scaling"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"attn_factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.5</span><span class="p">,</span><span class="w">
    </span><span class="nl">"rope_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yarn"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2011-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2011-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2011-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2011.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By applying Yarn at inference time‚Äîwith no retraining required‚Äîwe boost the accuracy on responses longer than 32K from <strong>26% to over 50%!</strong></p> <p>This discovery suggests that CoT extrapolation is a powerful tool for the <strong>later stages</strong> of training advanced reasoning models, especially when increasing rollout length becomes unaffordable. We also note that the accuracy improvements are concentrated on the more difficult problems in the dataset.</p> <h3 id="extrapolation-benefits-inference-time-scaling">Extrapolation Benefits Inference-Time Scaling</h3> <p>The graph below illustrates the inference-time scaling capabilities unlocked by Yarn on the AIME 24/25 datasets. The blue line represents <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> with Yarn, while the orange line shows the baseline performance without it.</p> <p>As the chart demonstrates, Polaris-4B-Preview with Yarn (blue line) significantly outperforms its base model, Qwen3-4B, once the context length exceeds 48K. Its performance continues to grow as the length increases toward 96K. In contrast, the model without Yarn (yellow line) shows its performance plateauing after 64K with almost no further gains.</p> <p>This confirms that applying an extrapolation technique like Yarn at inference time unlocks the model‚Äôs potential to scale its reasoning abilities to much longer contexts, overcoming the limitations imposed by practical RL training constraints.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2012-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2012-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2012-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2012.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="4-exploration-efficiency"><em>4. Exploration Efficiency</em></h2> <p>The success of long CoT training hinges on efficient exploration at the frontier of reward sparsity. In Polaris, exploration efficiency is enhanced through multi-stage training, accompanied with techniques that address response-level and sample reward sparsity. Specifically, we found that:</p> <ul> <li>the previously proposed <strong>‚ÄúThink Shorter, then Longer‚Äù</strong> paradigm does not generalize to all reasoning models; directly training with longer responses can often yield better performance;</li> <li>dynamical sampling can be done easy with the proposed <strong>Rollout Rescue Mechanism</strong> and <strong>Intra-Batch Informative Substitution</strong> techniques.</li> </ul> <h3 id="multi-stage-training">Multi-Stage Training</h3> <p>One of the biggest challenges in optimizing long CoT models with RL is the excessively long output, which results in slow training. To improve training efficiency, we incorporate multi-stage training in all our released models. Specifically, we use shorter context windows in earlier stages. Once the model‚Äôs performance converges, we increase the length of the context windows in the next stage.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2013-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2013-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2013-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2013.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="is-think-shorter-then-longer-necessary"><strong>Is ‚ÄúThink Shorter, then Longer‚Äù necessary?</strong></h3> <p>While effective, for multi-stage training it is critical to select the appropriate response length at the first stage:</p> <ul> <li>Not all models are both equally token-efficient: We found that training at a small response length works well for <code class="language-plaintext highlighter-rouge">DeepSeek-R1-Distill-Qwen-7B</code> but not for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>. Specifically, we observe drastic performance drop for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> even at a response length of 24K and response clip ratio of &lt;15%. Such performance degeneration is irreversible at later stages.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2014-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2014-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2014-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2014.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>It is usually safer to directly allow the model to ‚Äúthink longer‚Äù from the beginning: For <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, we observed steadily increasing performance with a 40K response length from scratch, in stark contrast with 24K and 24K‚Üí40K schemes.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2015-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2015-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2015-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2015.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Takeaway</strong>: When computational resources allow, start directly with the maximum decoding length suggested by the official repository.</p> <h3 id="rollout-rescue-mechanism">Rollout Rescue Mechanism</h3> <p>POLARIS uses a small rollout size (8) for cost savings, but this raises the chance of zero-reward batches on hard prompts. To balance positive examples with minimal engineering, we maintain a per-example offline buffer (‚Äúsink‚Äù):</p> <ol> <li>If all 8 rollouts fail (accuracy 0/8) and a correct rollout was observed in earlier epochs, store that response in the sink (evicting the previous one).</li> <li>In later epochs, whenever a new batch yields 0/8 for that example, randomly swap one failed rollout with the buffered response.</li> </ol> <p>This lightweight strategy reduces zero-reward data dramatically and speeds up convergence, without retry loops.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2016-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2016-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2016-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2016.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="intra-batch-informative-substitution">Intra-Batch Informative Substitution</h3> <p>In GRPO, examples with all-correct or all-incorrect rollouts produce no advantage. Rather than complex dynamic sampling, we apply a simple in-batch swap:</p> <ol> <li>Within each batch, select samples that have a mix of correct and incorrect rollouts (nonzero advantage).</li> <li>Randomly duplicate these informative samples to replace those that yield zero advantage.</li> </ol> <p>This ensures every training example contributes a learning signal, matching DAPO‚Äôs benefits but requiring only a few tensor index operations‚Äîno extra rollouts or data-pipeline changes.</p> <h2 id="5-from-dapo-and-grpo"><strong><em>5. From DAPO and GRPO+</em></strong></h2> <p>We‚Äôve incorporated several key strategies from <a href="https://dapo-sia.github.io/">DAPO</a> and <a href="https://www.notion.so/1cf81902c14680b3bee5eb349a512a51?pvs=21">GRPO+</a> into our training process for the following reasons:</p> <ul> <li><strong>No Entropy Loss (from GRPO+):</strong> We remove the entropy loss term to prevent training instability. While intend to encourage exploration, we note it can cause entropy to grow uncontrollably, leading to a training collapse. Our primary motivation is to ensure a more stable and reliable training process.</li> <li><strong>No KL Loss (from DAPO):</strong> We eliminate the KL loss to allow our model to explore beyond the constraints of the original SFT model. This also speeds up training, as we no longer need to compute log probabilities for a reference model.</li> <li><strong>Clip High (from DAPO):</strong> We increase the upper clipping bound in the surrogate loss function to encourage more aggressive exploration. This adjustment helps stabilize entropy and has been shown to improve model performance by allowing the policy to take larger, more beneficial update steps.</li> </ul> <h2 id="6-reward-function"><em>6. Reward Function</em></h2> <p>The reward function used in this work is the same as DeepscaleR, we employ an Outcome Reward Model (ORM) which returns:</p> <ul> <li><code class="language-plaintext highlighter-rouge">1</code> - If the LLM‚Äôs answer passes basic LaTeX/Sympy checks.</li> <li><code class="language-plaintext highlighter-rouge">0</code> - If the LLM‚Äôs answer is incorrect or formatted incorrectly (e.g. missing <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code> delimiters).</li> </ul> <h1 id="evaluation"><strong><em>Evaluation</em></strong></h1> <p>Our model needs to use a <strong>higher</strong> <strong>sampling temperature</strong> and <strong>a longer response length</strong> than Qwen3; all other settings are the same. For AIME24 and AIME25, we report the average performance of 32 runs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">90000</span>
    <span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">example input:</code> <code class="language-plaintext highlighter-rouge">&lt;|im_start|&gt;user\nEvery morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop. Let's think step by step and output the final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></p> <p>The evaluation scripts based on <a href="https://github.com/volcengine/verl">Verl</a> have been released on our GitHub. You can also use your own scripts for testing, but please note: our model‚Äôs output length has been significantly boosted. If your max response length is set too small, the performance may not even reach the level of the original <strong><code class="language-plaintext highlighter-rouge">Qwen3-4B</code></strong> due to the truncation mechanism. Therefore, please ensure the testing length is at least 64K. The graph showing performance changes with response length is available in the <strong>‚ÄúInference-Time Length Scaling‚Äù</strong> section.</p> <table> <thead> <tr> <th><strong>Models</strong></th> <th><strong>AIME24 avg@32</strong></th> <th><strong>AIME25 avg@32</strong></th> <th><strong>Minerva Math avg@4</strong></th> <th><strong>Olympiad Bench avg@4</strong></th> <th><strong>AMC23 avg@8</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">DeepScaleR-1.5B</code></strong></td> <td>43.1</td> <td>27.2</td> <td>34.6</td> <td>40.7</td> <td>50.6</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Qwen3-1.7B</code></strong></td> <td>48.3</td> <td>36.8</td> <td>34.9</td> <td>55.1</td> <td>75.6</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-1.7B-Preview</code></strong></td> <td><strong>66.9</strong></td> <td><strong>53.0</strong></td> <td><strong>38.9</strong></td> <td><strong>63.8</strong></td> <td><strong>85.8</strong></td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code></strong></td> <td>55.0</td> <td>39.7</td> <td>36.7</td> <td>56.8</td> <td>81.9</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">AReal-boba-RL-7B</code></strong></td> <td>61.9</td> <td>48.3</td> <td>39.5</td> <td>61.9</td> <td>86.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Skywork-OR1-7B-Math</code></strong></td> <td>69.8</td> <td>52.3</td> <td><strong>40.8</strong></td> <td>63.2</td> <td>85.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-7B-Preview</code></strong></td> <td><strong>72.6</strong></td> <td><strong>52.6</strong></td> <td>40.2</td> <td><strong>65.4</strong></td> <td><strong>89.0</strong></td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-32B</code></strong></td> <td>72.6</td> <td>54.9</td> <td>42.1</td> <td>59.4</td> <td>84.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-32B</code></strong></td> <td>81.4</td> <td>72.9</td> <td>44.2</td> <td>66.7</td> <td>92.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-4B</code></strong></td> <td>73.8</td> <td>65.6</td> <td>43.6</td> <td>62.2</td> <td>87.2</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-4B-Preview</code></strong></td> <td><strong>81.2</strong></td> <td><strong>79.4</strong></td> <td><strong>44.0</strong></td> <td><strong>69.1</strong></td> <td><strong>94.8</strong></td> </tr> </tbody> </table> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Polaris2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{POLARIS: A Post-Training Recipe for Scaling Reinforcement Learning on Advanced Reasoning Models}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/Polaris}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Chenxin and Xie, Zhihui and Li, Xiaonan and Li, Lei and Zhang, Jun and Gong, Shansan and Zhong, Ming and Xu, Jingjing and Qiu, Xipeng and Wang, Mingxuan and Kong, Lingpeng}</span>
    <span class="nv">year</span> <span class="err">=</span> <span class="err">{2025</span><span class="p">}</span>
<span class="c">}</span>
</code></pre></div></div>]]></content><author><name>Chenxin An</name></author><category term="reasoning-models"/><category term="scaling-RL"/><summary type="html"><![CDATA[Introducing Polaris-4B-Preview and Polaris-7B-Preview, the most powerful open-recipe reasoning models to date.]]></summary></entry><entry><title type="html">Dream 7B</title><link href="https://hkunlp.github.io/blog/2025/dream/" rel="alternate" type="text/html" title="Dream 7B"/><published>2025-04-02T00:00:00+00:00</published><updated>2025-04-02T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/dream</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/dream/"><![CDATA[<p><strong>Team:</strong> Jiacheng Ye*, Zhihui Xie*, Lin Zheng*, Jiahui Gao*, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Huawei Noah‚Äôs Ark Lab</p> <h2 id="introducing-dream-7b">Introducing Dream 7B</h2> <p>In a joint effort with Huawei Noah‚Äôs Ark Lab, we release <strong>Dream 7B</strong> (<ins>D</ins>iffusion <ins>rea</ins>soning <ins>m</ins>odel), the most powerful open diffusion large language model to date.</p> <p>In short, Dream 7B:</p> <ul> <li>consistently outperforms existing diffusion language models by a large margin;</li> <li>matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;</li> <li>demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/overall_performance-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/overall_performance.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of language models on general, math, coding, and planning tasks.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/main_tab-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/main_tab.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of language models on standard evaluation benchmarks. * indicates Dream 7B, LLaDA 8B, Qwen2.5 7B and LLaMA3 8B are evaluated under the same protocol. The best results are bolded and the second best are underlined.</figcaption> </figure> </div> </div> <p>We release the weights of the base and instruct models in:</p> <ul> <li>Base model: <a href="https://huggingface.co/Dream-org/Dream-v0-Base-7B"><strong>Dream-org/Dream-v0-Base-7B</strong></a></li> <li>SFT model: <a href="https://huggingface.co/Dream-org/Dream-v0-Instruct-7B"><strong>Dream-org/Dream-v0-Instruct-7B</strong></a></li> <li>Codebase: <a href="https://github.com/HKUNLP/Dream"><strong>GitHub</strong></a></li> </ul> <h2 id="why-diffusion-for-text-generation">Why Diffusion for Text Generation?</h2> <p>The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, transforming numerous applications across industries. Currently, autoregressive (AR) models dominate the landscape of text generation, with virtually all leading LLMs (e.g., GPT-4, DeepSeek, Claude) relying on this same sequential left-to-right architecture. While these models have demonstrated remarkable capabilities, a fundamental question emerges: what architectural paradigms might define the next generation of LLMs? This question becomes increasingly relevant as we observe certain limitations in AR models at scale, including challenges with complex reasoning, long-term planning, and maintaining coherence across extended contexts<d-cite key="bubeck2023sparks,dziri2024faith,bachmann2024pitfalls,ye2024beyond"></d-cite>. These limitations are particularly crucial for emerging applications such as embodied AI, autonomous agents, and long-horizon decision-making systems, where sustained reasoning and contextual understanding are essential for success.</p> <p>Discrete diffusion models (DMs) have gained attention as a promising alternative for sequence generation since their introduction to the text domain <d-cite key="hoogeboom2021argmax,austin2021structured,campbell2022continuous"></d-cite>. Unlike AR models that generate tokens sequentially, discrete DMs dynamically refine the full sequence in parallel starting from a fully noised state. This fundamental architectural difference unlocks several significant advantages:</p> <ul> <li><strong>Bidirectional contextual modeling</strong>¬†enables richer integration of information from both directions, substantially enhancing global coherence across the generated text.</li> <li><strong>Flexible controllable generation</strong>¬†capabilities arise naturally through the iterative refinement process.</li> <li><strong>Potential for fundamental sampling acceleration</strong> through novel architectures and training objectives that enable efficient direct mapping from noise to data <d-cite key="song2023consistency"></d-cite>.</li> </ul> <p>Recently, significant advancements have highlighted diffusion‚Äôs growing potential in language tasks. DiffuLLaMA<d-cite key="gong2024scaling"></d-cite> and LLaDA<d-cite key="nie2025large"></d-cite> scaled diffusion language models to 7B parameters, while <a href="https://www.inceptionlabs.ai/news">Mercury Coder</a>, as a commercial implementation, has demonstrated remarkable inference efficiency in code generation. This rapid progress, combined with the inherent architectural advantages of diffusion language modeling, positions these models as a promising direction for overcoming the fundamental limitations of autoregressive approaches.</p> <h2 id="training">Training</h2> <p>Dream 7B builds upon <a href="https://ikekonglp.github.io/dreams.html">our team‚Äôs prior effort</a><d-footnote><a href="https://ikekonglp.github.io/dreams.html">https://ikekonglp.github.io/dreams.html</a></d-footnote> in the diffusion language model area, drawing from RDM<d-cite key="Zheng2023ARD"></d-cite>‚Äôs theoretical foundation and DiffuLLaMA<d-cite key="gong2024scaling"></d-cite>‚Äôs adaptation strategy. We adopt a mask diffusion paradigm with the model architecture shown below. Our training data spans from text to math and code, mainly sourced from¬†<a href="https://huggingface.co/datasets/allenai/dolma">Dolma v1.7</a>,¬†<a href="https://huggingface.co/collections/OpenCoder-LLM/opencoder-datasets-672e6db6a0fed24bd69ef1c2">OpenCoder</a>, and¬†<a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline</a>, with several pre-processing and curation pipelines. Following a carefully designed training process, we pretrain Dream 7B using a mixture of the aforementioned corpus, totaling 580 billion tokens. The pretraining was done on 96 NVIDIA H800 GPUs for 256 hours. The pretraining process went smoothly overall, with occasional node anomalies, and we did not experience any unrecoverable loss spikes.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/model-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/model-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/model-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/model.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of autoregressive modeling and diffusion modeling in Dream. Dream predicts all the masked tokens in a shifted manner, allowing for maximumly architectural alignment and weight initialization with AR models.</figcaption> </figure> </div> </div> <p>We extensively studied the design choices on the 1B level and identified many valuable components, such as weight initialization from AR models (e.g., Qwen2.5<d-cite key="yang2024qwen2"></d-cite> and LLaMA3<d-cite key="grattafiori2024llama"></d-cite>) and a context-adaptive token-level noise rescheduling, which enables the effective training of Dream 7B.</p> <h3 id="ar-initialization">AR initialization</h3> <p>Building on our previous work DiffuLLaMA<d-cite key="gong2024scaling"></d-cite>, we discovered that using the weights from the existing autoregressive (AR) model serves as a non-trivial initialization for the diffusion language model. We find this design is more effective than training the diffusion language model from scratch, particularly during the early stages of training, as illustrated in the figure below.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/from_scratch_adapt-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/from_scratch_adapt.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: loss comparison of the from-scratch and AR-initialization with LLaMA3.2 1B training on the Dream 1B models with 200B tokens. AR initialization also experiences a high loss at the beginning due to the transition from causal attention to full attention; however, this loss remains lower compared to training from scratch throughout the training.</figcaption> </figure> </div> </div> <p>Dream 7B is finally initialized with weights from <a href="https://huggingface.co/Qwen/Qwen2.5-7B">Qwen2.5 7B</a>. During the training process, we find the learning rate to be especially important. If it‚Äôs set too high, it can quickly wash away the left-to-right knowledge in the initial weights, providing little help in the diffusion training, while if it‚Äôs set too low, it can hinder diffusion training. We meticulously selected this parameter along with the other training parameters.</p> <p>Thanks to the existing left-to-right knowledge in the AR model, the diffusion model‚Äôs any-order learning can be accelerated, significantly reducing the tokens and computation required for pretraining.</p> <h3 id="context-adaptive-token-level-noise-rescheduling">Context-adaptive Token-level Noise Rescheduling</h3> <p>The selection of each token in a sequence depends on its context, yet we observed that previous diffusion training approaches fail to adequately account for this aspect. Specifically, in conventional discrete diffusion training, a timestep¬†<em>t</em>¬†is sampled to determine the sentence-level noise level, after which the model performs denoising. However, since the learning ultimately operates at the token level, the actual noise level for each token does not strictly align with¬†<em>t</em> due to the application of discrete noise. This resulted in ineffective learning of tokens with varying levels of contextual information.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/reweighting-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/reweighting.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: illustration of the context-adaptive token-level noise rescheduling mechanism. Dream re-decides a token-level timestep t for each mask token by measuring its context informationness.</figcaption> </figure> </div> </div> <p>To address this, we introduce a¬†context-adaptive token-level noise rescheduling mechanism¬†that dynamically reassigns the noise level for each token based on the corrupted context after noise injection. This mechanism provides more fine-grained and precise guidance for the learning process of individual tokens.</p> <h2 id="planning-ability">Planning Ability</h2> <p>In our previous work<d-cite key="ye2024beyond,ye2025implicit"></d-cite>, we demonstrated that text diffusion exhibits superior planning capabilities in the small-scale, task-specific context. However, it remains uncertain whether a general, scaled diffusion model possesses similar abilities. Now, with Dream 7B, we can better answer this question.</p> <p>We evaluated Dream on the Countdown and Sudoku tasks from <d-cite key="ye2024beyond"></d-cite>, where we can flexibly control the planning difficulty. Our comparison included Dream 7B alongside <a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base">LLaDA 8B</a>, <a href="https://huggingface.co/Qwen/Qwen2.5-7B">Qwen2.5 7B</a>, and <a href="https://huggingface.co/meta-llama/Llama-3.1-8B">LLaMA3 8B</a>, together with the latest <a href="https://www.deepseek.com/">Deepseek V3 671B (0324)</a> for reference. All models were assessed in a few-shot setting without any training on these tasks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/sudoku_cd-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/sudoku_cd.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: results on Countdown and Sudoku when varying planning difficulty.</figcaption> </figure> </div> </div> <p>It is evident that Dream outperforms other similar-sized baseline models. Remarkably, both diffusion models significantly surpass the two AR models and, at times, even the latest DeepSeek V3, despite its orders of magnitude more parameters. The intuition behind is that diffusion language models are more effective for solving problems with multiple constraints or for achieving specific objectives.</p> <p>Here are some examples of Qwen 2.5 7B and Dream 7B in three planning tasks:</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/cases-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/cases-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/cases-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/cases.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: generation examples from Qwen2.5 7B and Dream 7B.</figcaption> </figure> </div> </div> <h2 id="inference-flexibility">Inference Flexibility</h2> <p>Diffusion models offer more flexible inference compared to AR models in the following two main aspects.</p> <h3 id="arbitrary-order">Arbitrary Order</h3> <p>Diffusion models are not constrained to sequential (e.g., left-to-right) generation, enabling outputs to be synthesized in arbitrary orders‚Äîthis allows for more diverse user queries.</p> <ul> <li><strong>Completion</strong></li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/fig_gsm_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: a completion example of Dream-7B-instruct.</figcaption> </figure> </div> </div> <ul> <li><strong>Infilling</strong></li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_infill_1.gif-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/fig_infill_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: an infilling example of Dream-7B-instruct with an exact ending sentence.</figcaption> </figure> </div> </div> <ul> <li> <p><strong>Controlling the decoding behavior</strong></p> <p>Different queries may have preferences for the order in which the responses are generated. One can also adjust the decoding hyperparameters to control the decoding behavior, shifting it from more left-to-right like an AR model to more random-order generation.</p> </li> </ul> <div class="row mt-1"> <div class="col-sm-4 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_1.gif-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/fig_code_1.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: configured to decode more in a left-to-right way like an AR model.</figcaption> </figure> </div> <div class="col-sm-4 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_2.gif-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/fig_code_2.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: configured to add some randomness in the decoding order.</figcaption> </figure> </div> <div class="col-sm-4 mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/fig_code_3.gif-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/fig_code_3.gif" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: configured for fully randomness in the decoding order.</figcaption> </figure> </div> </div> <h3 id="quality-speed-trade-off">Quality-speed Trade-off</h3> <p>In the above cases, we show one token is generated per step. However, the number of generated tokens per step (controlled by diffusion steps) can be adjusted dynamically, providing a tunable trade-off between speed and quality: fewer steps yield faster but coarser results, while more steps produce higher-quality outputs at greater computational cost. This introduces an additional dimension for inference-time scaling <d-cite key="snell2024scaling,muennighoff2025s1,geiping2025scaling"></d-cite> that complements rather than replaces techniques like long chain-of-thought reasoning employed in large language models such as o1 and r1. This adjustable computation-quality tradeoff represents a unique advantage over traditional AR frameworks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/quality_speed-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/quality_speed.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: quality-speed comparison on the Countdown task for Dream 7B and Qwen2.5 7B. By adjusting the diffusion timesteps, the performance of Dream can be flexibly tuned for either speed or quality.</figcaption> </figure> </div> </div> <h2 id="supervised-fine-tuning">Supervised Fine-tuning</h2> <p>As a preliminary step in post-training diffusion language models, we perform supervised fine-tuning to align Dream with user instructions. Specifically, we curate a dataset with 1.8M pairs from Tulu 3<d-cite key="lambert2024t"></d-cite> and SmolLM2<d-cite key="allal2025smollm2smolgoesbig"></d-cite>, fine-tuning Dream for three epochs. The results highlight Dream‚Äôs potential to match autoregressive models in performance. Looking forward, we plan to explore more advanced post-training recipes for diffusion language models.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-04-02-dream-img/sft-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-04-02-dream-img/sft-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-04-02-dream-img/sft-1400.webp"/> <img src="/assets/img/2025-04-02-dream-img/sft.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: supervised fine-tuning results.</figcaption> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>We introduce Dream, a new family of efficient, scalable, and flexible diffusion language models with carefully selected training recipes. It performs comparably to the best autoregressive models of similar size in general, mathematical, and coding tasks while especially showcasing advanced planning abilities and flexible inference capabilities.</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">dream2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Dream 7B}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/dream}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Jiacheng Ye</name></author><category term="language-models"/><category term="diffusion-models"/><summary type="html"><![CDATA[Introducing Dream 7B, the most powerful open diffusion large language model to date.]]></summary></entry><entry><title type="html">EvaByte: Efficient Byte-level Language Models at Scale</title><link href="https://hkunlp.github.io/blog/2025/evabyte/" rel="alternate" type="text/html" title="EvaByte: Efficient Byte-level Language Models at Scale"/><published>2025-01-21T00:00:00+00:00</published><updated>2025-01-21T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/evabyte</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/evabyte/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} \newcommand{\mbpsi}{\mathbold{\psi}} \newcommand{\mcM}{\mathcal{M}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} $$ </div> <p><strong>Full team:</strong> Lin Zheng, Xueliang Zhao, Guangtao Wang, Chen Wu, David Dong, Angela Wang, Mingran Wang, Yun Du, Haige Bo, Amol Sharma, Bo Li, Kejie Zhang, Changran Hu, Urmish Thakker, and Lingpeng Kong</p> <h2 id="introducing-evabyte">Introducing EvaByte</h2> <p>In a collaborative effort between the University of Hong Kong and SambaNova Systems, we introduce <strong>EvaByte</strong>, a 6.5B state-of-the-art <strong>byte-level language model</strong> featuring an improved architecture and powered by EVA ‚Äì an efficient attention mechanism designed for scalability and performance.</p> <p>Trained on 1.5T bytes of natural language text, math, and code using the performant SambaNova SN30 RDU system, EvaByte demonstrates that efficient byte-level processing at scale is not just possible, but practically advantageous ‚Äì rivaling modern open-source tokenizer-based LMs <d-cite key="groeneveld2024olmo,li2024dclm,zhang2024mapneo"></d-cite> despite using 5x less training data, excelling in coding tasks, and decoding up to 2x faster. Its token-free design also brings added <strong>flexibility</strong>, avoiding tokenizer quirks while naturally extending to <a href="#case-study-multimodal-learning">multimodal applications</a> without any architecture tweaks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: scaling analysis between average task performance and training set size.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/main_table_v2.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of language models on standard evaluation benchmarks. ‚Ä° the number of tokens measured by Llama 3 tokenizer, corresponding to 1.5T training bytes. ‚Ä†Low scores are caused by failing to generate Python functions and repeat the input under EvalPlus prompt format.</figcaption> </figure> </div> </div> <p>To our knowledge, EvaByte is the first open-source byte-level model without tokenization that yet matches the performance of modern tokenizer-based LMs. Check out the model weights and code here:</p> <ul> <li>Base model before annealing: <a href="https://huggingface.co/EvaByte/EvaByte-Phase1"><strong>EvaByte/EvaByte-Phase1</strong></a></li> <li>Base model: <a href="https://huggingface.co/EvaByte/EvaByte"><strong>EvaByte/EvaByte</strong></a></li> <li>SFT model: <a href="https://huggingface.co/EvaByte/EvaByte-SFT"><strong>EvaByte/EvaByte-SFT</strong></a></li> <li>Codebase: <a href="https://github.com/OpenEvaByte/evabyte"><strong>GitHub</strong></a></li> </ul> <h2 id="byte-level-modeling-with-improved-architectures">Byte-level Modeling with Improved Architectures</h2> <p>Tokenization is a fundamental step in modern large language models, deciding how input is represented in Transformers. Although it efficiently compresses raw text into shorter sequences, tokenization comes with its own baggage ‚Äì it is an externally trained, detached component that can introduce complex biases and edge-case quirks, like the prompt boundary problem <d-cite key="microsoft2023guidance,lundberg2023tokenhealing,dagan2024getting,athiwaratkun2024token,vieira2024language"></d-cite>, undertrained tokens <d-cite key="rumbelow2023solidgoldmagikarp,land2024fishing,wang2024tokenizationmatters,yang2024rethinking,yang2024problematictokens"></d-cite>, and even pretraining data mixture leaks <d-cite key="hayase2024datamixture"></d-cite>.</p> <p>Byte-level modeling is an approach that inherently eliminates biases introduced by tokenization, although directly operating on bytes at scale is not easy <d-cite key="clark2022canine,xue2022byt5,tay2022charformer,yu2023megabyte,slagle2024spacebyte,wang2024mambabyte,kallini2024mrt5"></d-cite>: </p> <div class="row mt-0"> <div class="col-sm-10 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: correspondence between tokens and bytes, as measured by the GPT-4o tokenizer.</figcaption> </figure> </div> </div> <ul> <li>Byte sequences are naturally longer ‚Äì 3.8x longer than their tokenized counterparts in our training corpus ‚Äì leading to more than 3.8x computational overhead under standard Transformer architectures.</li> <li>Inference becomes more challenging due to the inherently long and sequential nature of byte-level predictions.</li> <li>Training byte-level models is less stable as we observed in our <a href="#training">experiments</a>.</li> </ul> <p>We address these hurdles with a streamlined architecture featuring two improvements: <strong>multibyte prediction</strong> and <strong>the efficient attention mechanism, EVA</strong>.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/arch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: an overview of the EvaByte architecture.</figcaption> </figure> </div> </div> <p>Although vanilla byte-level language models typically run much slower than tokenizer-based LMs, with the improved architecture, we have achieved a significant speed boost for byte models ‚Äì <strong>5-10x faster</strong> decoding compared to vanilla architectures and even <strong>up to 2x faster</strong> than tokenizer-based LMs, making byte-level models a practical choice for real-world applications.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: <b>bytes per second</b> (ü†Ö) measured by generating 512 bytes (or tokens) with a batch size of 1 on one H800 GPU using the HF native generate() interface.</figcaption> </figure> </div> </div> <h3 id="multibyte-prediction">Multibyte Prediction</h3> <p>We draw inspiration from recent work <d-cite key="stern2018blockwise,qi2020prophetnet,cai2024medusa,gloeckle2024multitoken"></d-cite> and equip our model with multiple prediction heads, allowing it to predict several future bytes simultaneously. During training, we average the cross-entropy losses from different output heads as the primary training objective. These heads learn very effectively ‚Äì their predictions are often highly accurate and sometimes even outperform the immediate next byte prediction, as shown in the figure below.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: multi-choice task performance across different prediction heads. Each head corresponds to using the likelihood from the immediate next byte prediction (Head 1), second-next byte prediction (Head 2), and so forth.</figcaption> </figure> </div> </div> <p>Multibyte prediction adds almost no training overhead, thanks to the particularly small vocabulary size. <d-footnote>Our model uses 8 prediction heads and a vocabulary size of 320, including 256 byte values and 64 special tokens.</d-footnote> However, it greatly speeds up inference with <strong>self-speculative decoding</strong>, where multiple heads are combined via Medusa-like tree attention <d-cite key="cai2024medusa"></d-cite> and enable the model to predict multiple bytes in one decoding step.</p> <h3 id="efficient-attention-with-eva">Efficient Attention with EVA</h3> <p>However, multibyte prediction alone is not enough to speed up the byte-level model: the self-attention mechanism quickly becomes the major bottleneck as the context length grows. To address this, we build our model on <strong>EVA</strong> <d-cite key="zheng2023eva"></d-cite>, an improved version of <strong>linearized attention</strong> <d-cite key="katharopoulos2020transformers_are_rnns,peng2021rfa,choromanski2021rethinking"></d-cite>. Linearized attention approximates exact self-attention by designing feature maps $\phi(\cdot)$ such that</p> <div style="font-size: 0.9em; auto; text-align: center; max-width: 100%;"> \begin{equation} \frac{\sum_{m=1}^n\exp\left(\mbq_{n}^\top \mbk_{m} \right)\mbv_{m}^\top}{\sum_{m'=1}^n \exp\left(\mbq_{n}^\top \mbk_{m'} \right)} \approx \frac{\sum_{m=1}^n \phi(\mbq_n)^\top \phi(\mbk_m)\mbv_{m}^\top}{\sum_{m'=1}^n\phi(\mbq_{n'})^\top \phi(\mbk_{m'})} = \frac{\phi(\mbq_n)^\top \sum_{m=1}^n \phi(\mbk_m)\mbv_{m}^\top}{\phi(\mbq_{n'})^\top \sum_{m'=1}^n\phi(\mbk_{m'})} \notag. \end{equation} </div> <p>By linearizing $\exp(\cdot)$, one can rearrange the order of computation and achieve linear complexity in sequence length. This approach admits the form of a linear RNN, maintaining a global hidden state. With gating mechanisms and decay coefficients <d-cite key="peng2021rfa,qin2024hgrn2,sun2023retnet,yang2024gla"></d-cite>, it also connects to recent state-space models like Mamba and Mamba-2 <d-cite key="gu2024mamba,dao2024mamba2"></d-cite>. Conventional linearized attention compresses past tokens into a single global hidden state, unlike standard attention, which explicitly caches every token.</p> <p>EVA takes a middle ground by <strong>distributing</strong> the global state into multiple local memory slots. By splitting key-value pairs into consecutive chunks and applying linearization <strong>separately</strong> on each chunk, EVA maintains a local hidden state for each chunk and aggregates them together to produce the final output. This expands the design space of linearized attention mechanisms, simplifies implementation, and directly benefits from hardware-optimized kernels for standard attention mechanisms.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/attn_sketch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: computation graphs for standard attention (<strong>left</strong>), linearized attention (<strong>middle</strong>), and EVA (<strong>right</strong>). Symbols: $\times$ denotes (multiple) matrix multiplication and $\sum$ represents sum reduction.</figcaption> </figure> </div> </div> <h2 id="training">Training</h2> <p>We pretrain EvaByte on a corpus of 1.5T bytes spanning from text to math and code, mainly sourced from <a href="https://huggingface.co/datasets/allenai/dolma">Dolma v1.7</a>, <a href="https://huggingface.co/datasets/bigcode/the-stack-v2-train-smol-ids">The Stack v2</a>, <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-Edu</a>, and <a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline</a>. We constantly refined the data mix by tweaking the proportions or swapping in new sources mid-flight. After training on 1.2T bytes, we conduct two independent annealing runs (100B and 200B bytes respectively), where the learning rate is linearly decayed from 1e-4 to 0 and the checkpoints are merged via model soup. <d-cite key="wortsman22modelsoups"></d-cite></p> <p>EvaByte is trained with a batch size of 8M bytes and 32K context length on 256 SambaNova SN30-2 RDUs. We observed non-trivial instability during pretraining:</p> <ul> <li><strong>Byte-level collapses</strong>: Occasionally, intermediate checkpoints would produce bizarre typos (e.g., <code class="language-plaintext highlighter-rouge">e</code> in generated outputs turning into an <code class="language-plaintext highlighter-rouge">i</code>) when prompted to perform generation tasks; interestingly, these glitches resolved themselves after a few thousand training steps and never appeared near the end of training.</li> </ul> <figure style="width: 92%; margin: 0 auto;"> <figcaption>A snapshot of code generation at an intermediate checkpoint with bizarre typos.</figcaption> <pre style=" font-size: 12px; font-family:monospace;color: rgb(0, 0, 0); background-color: rgb(254, 252, 252); font-weight: 100; white-space: pre-wrap; word-wrap: break-word; margin: 0; line-height: 1.25; ">
<span style="color: rgb(0, 0, 255); font-weight: 400;">from</span> typing <span style="color: rgb(0, 0, 255); font-weight: 400;">import</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>, <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>

<span style="color: rgb(0, 0, 255); font-weight: 400;">def</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">sum_product</span>(<span style="color: rgb(0, 0, 0); font-weight: 400;">numbers: <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]</span>) -&gt; <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>, <span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]:
    <span style="color: rgb(163, 21, 21); font-weight: 400;">""" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    &gt;&gt;&gt; sum_product([])
    (0, 1)
    &gt;&gt;&gt; sum_product([1, 2, 3, 4])
    (10, 24)
    """</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> = <span style="color: rgb(0, 0, 0); font-weight: 400;">0</span>
    product = <span style="color: rgb(0, 0, 0); font-weight: 400;">1</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">for</span> number <span style="color: rgb(0, 0, 255); font-weight: 400;">in</span> numb<span style="background-color: #ffb6c1;">i</span>rs:
        <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> += numb<span style="background-color: #ffb6c1;">i</span>r
        product *= numb<span style="background-color: #ffb6c1;">i</span>r
    <span style="color: rgb(0, 0, 255); font-weight: 400;">return</span> (<span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span>, product)
</pre> </figure> <ul> <li><strong>Loss spikes</strong>: The most helpful techniques for stabilizing training through our experiments include <ul> <li>Lowering Adam epsilon $\epsilon$ from 1e-8 to 1e-12.</li> <li>Skipping batches that lead to spikes to keep the model in sane state.</li> <li>Periodically resetting Adam optimizer states to zero with quickly re-warming up the learning rate to remove bad out-of-track estimates.</li> </ul> <p>Other attempts, like freezing embedding parameters or applying weighted average over different prediction heads, offered little improvement.</p> </li> </ul> <h2 id="empirical-results">Empirical Results</h2> <p>Let‚Äôs dive into how EvaByte performs in practice. We compare EvaByte‚Äôs intermediate checkpoints against recent language models (OLMo-1.7-7B and OLMo-2-7B), trained on the roughly same amount of data. We observe the EvaByte checkpoint at 1.22T bytes (roughly 0.4T tokens) consistently outperforms them by a large margin.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of intermediate checkpoints on standard benchmarks.</figcaption> </figure> </div> </div> <p></p> <p>We also tracked EvaByte‚Äôs task performance throughout pretraining and observed a consistent <strong>upward trend with no signs of plateauing</strong>. Interestingly, EvaByte excels at coding tasks (e.g., HumanEval and MBPP), even though we intentionally reduced the proportion of code data in the later stages of training. One possible reason is that removing tokenization might eliminate domain-specific biases, enabling more efficient parallel learning across domains. A deeper investigation into this behavior is planned for future work.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>We take EvaByte a step further with <strong>supervised fine-tuning</strong>. Following DCLM <d-cite key="li2024dclm"></d-cite>, OLMo-2 <d-cite key="ai22024olmo2"></d-cite>, TULU 3 <d-cite key="lambert2024tulu3"></d-cite>, and OpenCoder <d-cite key="huang2024opencoder"></d-cite>, we curate a data mix from Tulu 3, OpenHermes 2.5, and OpenCoder, fine-tune EvaByte for 2 epochs, and achieve results on par with recent open LMs.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/sft_table.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of instruct models. ‚Ä† Evaluated by us. * Following Tulu 3, we evaluate the Pass@10 rate for HumanEval with 20 samples at temperature 0.8.</figcaption> </figure> </div> </div> <h3 id="flexibility">Flexibility</h3> <p>As mentioned at the beginning, we demonstrate below that byte-level modeling naturally avoids tokenization quirks and edge-case behaviors, such as the <strong>prompt boundary problem</strong>, where tokenizer-based LMs behave inconsistently around prompt boundaries. EvaByte resolves these cases seamlessly and delivers more predictable results.</p> <figure> <figcaption style="font-size: 100%; font-family: monospace; padding-bottom: 10px;"> <span style="background-color: #a8dcfb; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> prompt &nbsp;&nbsp;&nbsp; <span style="background-color: #B2FBA8; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> correct completion &nbsp;&nbsp;&nbsp; <span style="background-color: #ffb6c1; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> incorrect completion </figcaption> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>EvaByte</strong>: outputs from different prompt boundaries converge. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #B2FBA8;">""\n    if not strings:\n        return None\n    longest = strings...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #B2FBA8;">"\n    if not strings:\n        return None\n    longest = strings[...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #B2FBA8;">\n    if not strings:\n        return None\n    longest = strings[0...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #B2FBA8;">   if not strings:\n        return None\n    longest = strings[0]\n...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #B2FBA8;">  if not strings:\n        return None\n    longest = strings[0]\n ...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest = strings[0]\n  ...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #B2FBA8;">if not strings:\n        return None\n    longest = strings[0]\n   ...</span>
</pre> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>Qwen2.5-7B</strong>: different prompt boundaries lead to diverging and unexpected outputs. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #ffb6c1;">&gt;&gt;&gt; longest([\'a\', \'bb\', \'ccc\', \'dddd\'])\n    \'dddd\'\n    """\n    i...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #ffb6c1;">"""\n    if not strings:\n        return None\n    longest_string =...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #ffb6c1;"></span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #ffb6c1;"> if not strings:\n    return None\n  longest = strings[0]\n  for st...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #ffb6c1;"> # if not strings:\n    #    return None\n    # longest = strings[...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest_string = string...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #ffb6c1;"> if len(strings) == 0 None:\n        return None\n    else:\n      ...</span>
</pre> </figure> <h2 id="case-study-multimodal-learning">Case Study: Multimodal Learning</h2> <p>EvaByte is also flexible to extend to multimodal tasks, treating image data as just another byte stream according to some protocol, such as JPEG, PNG, etc. We follow prior work <d-cite key="perez2024compressed,han2024jpeglm"></d-cite> and use JPEG format, keeping image byte sequence length manageable (a 384 x 384 image takes around 10K JPEG bytes). Thanks to byte-level modeling, EvaByte can seamlessly interleave image with text bytes for vision-language training <strong>without any architectural tweaks</strong>.</p> <p>Our multimodal dataset spans around 2T bytes, primarily filtered from OBELICS <d-cite key="laurencon2023obelics"></d-cite>, LLaVA-ReCap <d-cite key="li2024llavanext-ablations"></d-cite>, and DataComp <d-cite key="gadre2023datacomp,mobileclip2024"></d-cite>. After 20K fine-tuning steps on the data mix (~3M images), starting from a pre-trained text-only checkpoint, EvaByte can zero-shot generate captions and answer basic visual questions. <d-footnote> EvaByte can also generate images though a bit fuzzily. </d-footnote> The multimodal capabilities of EvaByte are actively evolving, with current work addressing specific alignment challenges (such as spatial relationships between computer components). Scaling and refining these multimodal abilities represents one of our most promising research directions.</p> <table style="width: 100%; border-collapse: collapse;"> <caption style="font-size: 0.9em; color: gray;">Table: after fine-tuning on limited multimodal data, EvaByte demonstrates basic capabilities in image captioning and visual question answering.</caption> <thead> <tr> <th style="text-align: center; width: 30%;">Image</th> <th style="text-align: center; width: 70%;">Text</th> </tr> </thead> <tbody> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a close-up view of a laptop keyboard. The keyboard has a black color scheme with white lettering on the keys. The keys are arranged in a standard QWERTY layout, which is commonly used for English language keyboards. The keys are labeled with the standard Western letters and symbols, including the alphabet, numbers, and punctuation marks. The laptop appears to be a modern model, possibly a MacBook, given the design and the visible trackpad to the right of the keyboard. The trackpad is a flat, rectangular surface with a grid pattern, typical of MacBook trackpads. The laptop is open, and the keyboard is the main focus of the image. The background is blurred, but it seems to be an indoor setting with a wooden surface, possibly a table or desk. The lighting in the image is soft, suggesting an indoor environment with ambient lighting.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a black dog swimming in a body of water. The dog appears to be a medium to large breed, with a glossy black coat. It is captured mid-swim, with its head above the water and its body partially submerged. The water around the dog is rippling, indicating movement, and the surface of the water reflects the light, suggesting it is a sunny day. There are no visible texts or distinguishing marks that provide additional context about the location or the dog's identity. The style of the image is a candid, real-life photograph, capturing a moment of the dog's activity.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> How many throw pillows are on the bed?<br/> <strong>A:</strong> three </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> Which iconic landmark is on the picture?<br/> <strong>A:</strong> The Eiffel Tower </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> What 2 colors are the flowers?<br/> <strong>A:</strong> red and yellow </td> </tr> </tbody> </table> <h2 id="comparison-to-byte-latent-transformers-blts">Comparison to Byte Latent Transformers (BLTs)</h2> <p>A recent concurrent work, Byte Latent Transformers (BLTs) <d-cite key="pagnoni2024blt"></d-cite>, also explores tokenization-free language models and offers an in-depth analysis of BLTs‚Äô behavior at scale. BLTs introduce an elegant framework that first encodes byte sequences into patches and then processes them globally.</p> <p>The main difference between BLTs and EvaByte lies in the <strong>architecture</strong>: BLTs use patchification and propose entropy patching to dynamically group bytes. While this approach adjusts compute allocation based on data complexity and reduces context length, it still relies on external models to determine patch boundaries. The majority of compute ends up focused on patch-level modeling, detached from the byte stream, similar to tokenizer-based models.</p> <p>In contrast, <strong>EvaByte keeps things simple</strong>: it directly operates on bytes with a flat Transformer-like model without needing to invoke external modules or group inputs. Empirically, EvaByte achieves better performance than BLTs even with 3-4x fewer training bytes, as shown in the table below. Besides, EvaByte is more flexible and scales easily to multimodal data, while BLTs require retraining or swapping out the auxiliary language model used for entropy patching.</p> <div class="row mt-1"> <div class="col-sm-11 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Table: we closely follow the evaluation setup in BLTs, testing zero-shot task performance on Arc-e, Arc-c, HellaSwag, PIQA, and HumanEval; 3-shot for the original MBPP split; and 5-shot for MMLU.</figcaption> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>We introduce EvaByte, a new family of efficient, scalable, and flexible byte-level language models. The ability to rival tokenization-based LMs with 5x less data while being faster highlights the significant potential of lower-level language modeling within the EvaByte architecture. Future research directions include further refining the model‚Äôs architecture to improve both its capacity and efficiency, analyzing in depth how lower-level language models scale with increasing sizes and data volume, as well as extending the context length to seamlessly process diverse data types ‚Äì images, videos, and audio ‚Äì simultaneously.</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">evabyte</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{EvaByte: Efficient Byte-level Language Models at Scale}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/evabyte}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin Zheng and Xueliang Zhao and Guangtao Wang and Chen Wu and David Dong and Angela Wang and Mingran Wang and Yun Du and Haige Bo and Amol Sharma and Bo Li and Kejie Zhang and Changran Hu and Urmish Thakker and Lingpeng Kong}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Lin Zheng</name></author><category term="language-models"/><category term="efficient-attention"/><summary type="html"><![CDATA[Introducing EvaByte, an efficient and strong byte-level language model]]></summary></entry><entry><title type="html">Randomized Attention: a Generalized Random Feature Attention Algorithm</title><link href="https://hkunlp.github.io/blog/2022/lara/" rel="alternate" type="text/html" title="Randomized Attention: a Generalized Random Feature Attention Algorithm"/><published>2022-07-12T00:00:00+00:00</published><updated>2022-07-12T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2022/lara</id><content type="html" xml:base="https://hkunlp.github.io/blog/2022/lara/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} $$ </div> <h2 id="overview">Overview</h2> <p>This blog post introduces a new perspective to understanding the <strong>Random Feature Attention (RFA)</strong> mechanism. We show that <strong>1)</strong> the conventional softmax attention can be equivalently rewritten as an <strong>expectation over RFAs</strong>, and that <strong>2)</strong> RFA is in fact a <strong>self-normalized importance sampler</strong> to estimate conventional softmax attention. This new perspective grounds the heuristic RFA approximation and also sheds light on how to generalize further and improve RFAs. More details can be found in our ICML paper <d-cite key="lara"></d-cite>.</p> <h2 id="attention">Attention</h2> <p>The attention mechanism <d-cite key="bahdanau2014neural,vaswani2017attention"></d-cite> has become a ubiquitous building block in modern deep learning models and brought great success across various domains, including natural language processing (NLP), computer vision (CV), bioinformatics, reinforcement learning, etc. Attention mechanisms take three different kinds of inputs: a set of $N$ query vectors $\mbQ \in \R^{N \times D}$, $M$ key vectors $\mbK \in \R^{M \times D}$ and value vectors $\mbV \in \R^{M \times D}$.<d-footnote> In this work we focus on self-attention, where $M=N$ and all of queries, keys and values are obtained by projecting tokens of the same input sequence. </d-footnote></p> <div class="row mt-3"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-sketch-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/attn-sketch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For each query $\mbq_n$, conventional softmax attention computes the following quantity,<d-footnote> We omit the commonly used scaling factor $1 / \sqrt{d}$ for simplicity as it can be merged into the computation of queries or keys.</d-footnote> \begin{equation} \mathsf{SoftmaxAttn}\left(\mbq_{n},\mbK,\mbV\right)\coloneqq\sum_{m=1}^M\frac{\exp\left(\mbq_{n}^\top \mbk_{m} \right)}{\sum_{m‚Äô=1}^M \exp\left(\mbq_{n}^\top \mbk_{m‚Äô} \right)} \mbv_{m}^{\top}. \end{equation} Intuitively, softmax attention first compares the query against each key and then computes the average over value vectors weighted by the normalized query-key similarities. It is effective in capturing long-term dependencies across sequence elements and producing contextualized representations; however, it suffers from <strong>quadratic</strong> time and memory complexity due to the explicit computation of all $NM$ query-key pairs.</p> <div class="row mt-3"> <div class="col-sm-8 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/attn-quadratic-complexity.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="random-feature-attention">Random Feature Attention</h2> <p>To reduce the computational complexity of softmax attention, researchers proposed to use <strong>random features (RF)</strong> <d-cite key="random-features"></d-cite> to linearize softmax attention. In particular, they make use of the following identity to rewrite the <strong>exponential kernel</strong> as an expectation, \begin{equation} \exp(\mbx^\top \mby) = \mathbb{E}_{\omega \sim \mathcal{N}(\omega;0,\mathbf{I})}\left[\xi(\mbx,\omega)^\top\xi(\mby, \omega)\right], \label{eqn:identity} \end{equation} where $\xi(\cdot, \cdot): \R^D \times \R^D \rightarrow \R^l$, $l\geq 1$, is a non-linear <strong>randomized mapping</strong> projecting the input vector to a $l$-dimensional vector via a randomly drawn $\omega \sim \mathcal{N}(\omega;0,\mathbf{I})$. There are several parameterization choices of $\xi(\mbx,\omega)$ for the identity to hold; in this work we focus on the positive type $\xi(\mbx,\omega) = \exp{\left(\omega^\top \mbx - \frac{1}{2}\norm{\mbx}^2\right)}$ as proposed in <d-cite key="choromanski2021rethinking"></d-cite>.<d-footnote> A classical choice of randomized mappings is to let $\xi(\mbx,\omega) = \exp{\left( \frac{1}{2}\norm{\mbx}^2\right)}\left[\sin{\left(\omega^\top \mbx\right)},\cos{\left(\omega^\top \mbx\right)}\right]^\top$ <d-cite key="random-features,peng2021rfa"></d-cite>. Besides, there are other advanced randomized mappings enjoying more appealing properties; see <d-cite key="choromanski2022hybrid,likhosherstov2022chefs"></d-cite> for a more in-depth study. </d-footnote></p> <p> To estimate the expectation in \eqref{eqn:identity}, one can draw multiple Monte Carlo samples from $\mathcal{N}(\omega;0,\mathbf{I})$ such that $\exp(\mbx^{\top} \mby) \approx \frac{1}{S}\sum_{s=1}^S \xi(\mbx,\omega_s)^{\top}\xi(\mby, \omega_s)$. By substituting such approximation into the softmax attention, we obtain <b>random feature attention (RFA)</b> <d-cite key="peng2021rfa"></d-cite> (also called <b>Performer</b> <d-cite key="choromanski2021rethinking"></d-cite>), $$ \begin{align} \frac{\sum_{m=1}^M\exp\left(\mbq_{n}^\top \mbk_{m} \right)\mbv_{m}^{\top}}{\sum_{m'=1}^M \exp\left(\mbq_{n}^\top \mbk_{m'} \right)} &amp;\approx \frac{\sum_{m=1}^M \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{m'=1}^M\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\xi(\mbk_{m'}, \omega_s)} \notag \\ &amp;=\frac{ \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)} \label{eqn:rfa}\\ &amp;\coloneqq \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) \notag. \end{align} $$ Thanks to the linearized formulation, one can first pre-compute the corresponding key-value statistics $\sum_{m=1}^M\xi(\mbk_{m},\omega_s)\mbv_{m}^{\top}$ and $\sum_{m=1}^M\xi(\mbk_{m},\omega_s)$ once, and then reuse them for each query. Consequently, it achieves <b>linear</b> complexity in both time and memory with respect to the sequence length. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/comparison-between-softmax-and-rfa.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </p> <div class="row mt-3" style="border: 1px solid #000;background-color: #ebebeb; border-radius: 10px; padding-top: 30px"> <p> <b> <font size="+2"> Why it is called random feature attention? </font> </b> <br/> This is due to the fact that the sample average can be written as $\mbphi(\mbx,\mbw)^\top \mbphi(\mby,\mbw)$, where $\mbphi(\mbx,\mbw) \coloneqq 1/\sqrt{S}[\xi(\mbx,\omega_1), \dots, \xi(\mbx, \omega_S)]^\top \in \R^{lS}$. The $\mbphi(\cdot,\cdot)$ can be considered as a feature map transforming the input vector to a new vector representation; as a result, $\mbphi(\cdot,\cdot)$ are conveniently referred to as <b>random features</b> <d-cite key="random-features"></d-cite>.</p> <div class="col-sm-3 col-lg-3 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/rfa-as-concat-mappings.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-biasedness-of-rfa">The Biasedness of RFA</h2> <p>RFA suffers from significant performance degradation across various tasks, as observed in many previous studies (e.g., see <a href="#comp-tb">here</a>). Although computationally efficient, the reduced modeling capacity of RFA greatly limits its practical usage. Researchers try to improve the performance from different perspectives, such as</p> <ul> <li>developing better random feature types <d-cite key="likhosherstov2022chefs, choromanski2022hybrid, chowdhury2021learning"></d-cite>,</li> <li>connecting it with fast weight programmers <d-cite key="pmlr-v139-schlag21a, irie2021going"></d-cite>,</li> <li>incorporating relative information <d-cite key="spe, luo2021stable, ripple, zhen2022cosformer"></d-cite>, and so on.</li> </ul> <p>In this work, we explore an orthogonal axis by re-examining the estimation bias of RFA. Our key observation is that RFA is a heuristic approximation to the <strong>whole</strong> softmax attention. Although the estimation of individual <strong>exponential kernels</strong> is unbiased, such estimation of a ratio of exponential kernels is not unbiased anymore. This is due to the non-linearity of ratios,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 mb-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/illustrate-why-rfa-is-biased.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Such bias not only incurs a potentially large approximation gap between RFA and softmax attention but also bottlenecks the effectiveness of unbiased kernel estimation.</p> <div class="row mt-3"> <p> Our work aims to address the following research question:<br/> <b>Given that we already know how to unbiasedly estimate exponential kernels, how do we construct an unbiased estimator for the whole softmax attention?</b> </p> <div class="col-sm mt-3 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-motivation-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/lara-motivation.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="randomized-attention-an-unbiased-estimator-for-softmax-attention">Randomized Attention: An Unbiased Estimator for Softmax Attention</h2> <p> We answer this question in the affirmative and prove that softmax attention can be rewritten as an expectation of simple RFAs, $$ \begin{equation} \mathsf{SoftmaxAttn}(\mbq_n, \mbK,\mbV) = \sum_{m} \frac{\exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m} \right)}{\sum_{m'} \exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m'} \right)} \mathbf{v}_{m}^{\top} = \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right],\label{eqn:softmax_as_expectation} \end{equation} $$ where $$ \begin{align} p_n(\omega) &amp;= \sum_{m=1}^M \frac{\exp\left( \mbq_n^\top\mbk_m \right)}{\sum_{m'=1}^M\exp\left( \mbq_n^\top\mbk_{m'} \right)} \mathcal{N}(\omega; \mbq_n + \mbk_m, \mathbf{I}), \label{eqn:ra-density} \\ f_n(\omega) &amp;= \frac{\xi(\mbq_n,\omega)^\top \sum_{m=1}^M \xi(\mbk_m, \omega) \mbv_{m}^{\top}}{\xi(\mbq_n,\omega)^\top \sum_{m'=1}^M \xi(\mbk_{m'}, \omega)} \label{eqn:ra-function}. \end{align} $$ Intuitively, <ul> <li>$p_n(\omega)$ is a Gaussian mixture distribution whose component weights are exactly attention scores, while </li> <li>$f_n(\omega)$ is an <b>RFA</b>, except that the linearized similarity between queries and keys is computed via individual <b>randomized mappings</b> (or equivalently, <b>one-dimensional</b> random features).</li> </ul></p> <p> Notably, our result can be viewed as a neat generalization of random feature approximation, which exhibits a high degree of symmetry: $$ \begin{align} \exp(\mbq_n^\top \mbk_m)\mbv_m^\top &amp;= \mathbb{E}_{q(\omega)}\left[\xi(\mbq_n,\omega)^\top\xi(\mbk_m, \omega)\mbv_m^\top\right] \notag \\ \sum_{m} \frac{\exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m} \right)}{\sum_{m'} \exp \left(\mathbf{q}_{n}^\top \mathbf{k}_{m'} \right)} \mathbf{v}_{m}^{\top} &amp;= \mathbb{E}_{p_n(\omega)}\left[\frac{\xi(\mbq_n,\omega)^\top \sum_{m=1}^M \xi(\mbk_m, \omega) \mbv_{m}^{\top}}{\xi(\mbq_n,\omega)^\top \sum_{m'=1}^M \xi(\mbk_{m'}, \omega)}\right].\notag \end{align} $$ Another implication is that we can construct a Monte Carlo estimator to approximate softmax attention in an <b>unbiased</b> way. By drawing $\omega_n \sim p_n(\omega)$, we obtain $$ \begin{align} \mathsf{SoftmaxAttn}(\mbq_n, \mbK,\mbV) &amp;\approx \frac{\xi(\mbq_n,\omega_n)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_n) \mbv_{m}^{\top}}{ \xi(\mbq_n,\omega_n)^\top \sum_{m'=1}^M\xi(\mbk_{m'}, \omega_n)} \label{eqn:ra}\\ &amp;\coloneqq \mathsf{RA}\left(\mbq_{n},\mbK,\mbV\right) \notag \end{align} $$ We name such estimator <b>Randomized Attention (RA)</b> since it computes similarity scores with individual <b>randomized mappings (instead of concatenated features)</b>. To the best of our knowledge, this is the first result that generalizes the unbiased <b>kernel</b> estimation to unbiased <b>attention</b> estimation. </p> <div class="row mt-3" style="border: 1px solid #000; background-color: #ebebeb; border-radius: 10px; padding-top: 20px"> <p> <b> Remark: </b> The proof of \eqref{eqn:softmax_as_expectation} is done by first reverse-engineering the formulation of RFA, equating it with self-normalized importance sampling (see below) and then completing the square of Gaussians to derive the density $p_n(\omega)$. The function $f_n(\omega)$ can be solved by substituting the density $p_n(\omega)$ into the equation. See the paper for a detailed proof.</p> </div> <h2 id="rfa-as-a-self-normalized-importance-sampler">RFA as a Self-normalized Importance Sampler</h2> <p>The analysis above further reveals that RFA is a specific <strong>self-normalized importance sampling</strong> estimator to softmax attention.</p> <h3 id="self-normalized-importance-sampling-snis">Self-normalized Importance Sampling (SNIS)</h3> <p> <b>Importance sampling (IS)</b> is a general approach to approximating expectation $\mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right]$ when it is difficult to directly draw samples from $p_n(\omega)$. In importance sampling, we use a <b>proposal</b> distribution $q(\omega)$ to draw samples and estimate the quantity as $$ \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right] = \mathbb{E}_{\omega \sim q(\omega)}\left[\frac{p_n(\omega)}{q(\omega)}f_n(\omega)\right] \approx \frac{1}{S} \sum_{s=1}^S \frac{p_n(\omega)}{q(\omega)} f_n(\omega_s), $$ where $\omega_1, \dots, \omega_S \sim q(\omega)$. The <b>self-normalized importance sampling (SNIS)</b> is defined as $$ \begin{equation} \mathbb{E}_{p_n(\omega)}\left[f_n(\omega)\right] \approx \frac{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}f(\omega_s)}{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}} = \sum_{s=1}^S\frac{\frac{p_n(\omega_s)}{q(\omega_s)}}{\sum_{s=1}^S\frac{p_n(\omega_s)}{q(\omega_s)}}f(\omega_s). \label{eqn:snis} \end{equation} $$ The name <b>self-normalized</b> comes from the fact that the importance weights $p_n(\omega)/q(\omega)$ are explicitly normalized and sum to 1. </p> <h3 id="rfa-as-snis">RFA as SNIS</h3> <p> Our key finding here is that the formulation of RFA can be exactly derived from SNIS. Supposing $p_n(\omega)$ and $f_n(\omega)$ are given in \eqref{eqn:ra-density} and \eqref{eqn:ra-function} respectively, and $q(\omega) = \mathcal{N}(\omega;0,\mathbf{I})$, we have $$ \begin{align} \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{s=1}^S\textcolor{strings}{\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}}{\sum_{s=1}^S \textcolor{keywords}{\xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)}} = \frac{ \sum_{s=1}^S\textcolor{strings}{\frac{p_n(\omega_s)}{q(\omega_s)} f(\omega_s)}}{ \sum_{s=1}^S\textcolor{keywords}{\frac{p_n(\omega_s)}{q(\omega_s)}}}. \label{eqn:rfa-as-snis} \end{align} $$ This formulation provides a new understanding of RFA: it is just a specific instantiation of <b>SNIS</b> estimators for softmax attention, whose proposal distribution $q(\omega)$ is chosen to be standard Gaussian. This reveals one of the possible reasons why RFA does not work well in practice: <b>The plain standard Gaussian proposal in RFA is far away from the true Gaussian mixture (as in RA), which might lead to a large approximation gap.</b> More importantly, this view implies that we can generalize and extend RFA by using other proposal distributions or adopting other estimating schemes! </p> <h2 id="lara-generalizing-both-ra-and-rfa">LARA: Generalizing Both RA and RFA</h2> <p>So far, we have two types of estimators available for approximating softmax attention: unbiased RA and biased RFA. Besides the theoretical biasedness, how do they differ in terms of practical modeling behavior? We list a comprehensive comparison to better illustrate their main differences.</p> <ul> <li>In terms of <strong>expressiveness</strong>: <ul> <li>‚úîÔ∏è RA directly draws from the true distribution $p_n(\omega)$ \eqref{eqn:ra-density}. This makes the mechanism <strong>adaptive</strong> and <strong>query-specific</strong>, since the sampling distribution depends on the query vector. As a result, RA can <strong>specialize</strong> to each query and process the whole sequence at a finer-grained level.</li> <li>‚ùå RFA suffers from limited capacity, since there is a large discrepancy between the used proposal $q(\omega) = \mathcal{N}(\omega;0,\mathbf{I})$ and the true distribution $p_n(\omega)$; furthermore, $q(\omega)$ captures no contextual information, leading to low sample efficiency.</li> </ul> </li> <li>In terms of <strong>efficiency</strong>: <ul> <li>‚ùå RA suffers from <strong>quadratic</strong> computational costs. Since the sampling distribution is distinct for each query, we have to draw at least $\mathcal{O}(N)$ samples for all queries; at the same time, we needs to sum over all $M$ key-value pairs at each sampled $\omega_n$ (see \eqref{eqn:ra}), which leads to $\mathcal{O}(NM)$ complexity overall, as is the case for softmax attention;</li> <li>‚úîÔ∏è RFA uses SNIS to avoid computing or sampling from $p_n(\omega)$, which require quadratic-time computation. Drawing from a simpler proposal distribution allows samples to be shared across queries and still resulting in a $\mathcal{O}(M)$ complexity.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/lara-as-combining-ra-and-rfa.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">A diagram of LARA that combines the strengths of both approaches.</figcaption> </figure> </div> </div> <p>Motivated by the comparison, we propose <b>LineAr-time Randomized Attention (LARA)</b> that attempts to get the best of both worlds, combining both the efficiency of RFA and the expressiveness of RA.</p> <ul> <li>On the one hand, LARA inherits the <strong>SNIS</strong> formulation as in RFA to achieve <strong>linear complexity</strong>;</li> <li>On the other hand, its expressiveness is significantly improved towards RA via the following modifications: <ul> <li>The proposal distribution is data-dependent so that LARA can be <strong>adaptive</strong> with respect to the contextual information;</li> <li>LARA adopts <strong>multiple</strong> proposal distributions, which can be combined in a <strong>query-specific</strong> manner.</li> </ul> </li> </ul> <p> LARA takes the following form $$ \begin{align} \mathsf{LARA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{c=1}^C\alpha_{nc}(\omega_c)\frac{p_n(\omega_c)}{q_c(\omega_c)} f_n(\omega_c)}{\sum_{c=1}^C\alpha_{nc}(\omega_c)\frac{p_n(\omega_c)}{q_c(\omega_c)}}.\label{eqn:lara} \end{align} $$ Here, <ul> <li>$q_1(\omega), \dots, q_C(\omega)$ are $C$ different proposals that depend on different query and key information;</li> <li>$\{\alpha_{nc}(\cdot)\}_{c=1}^C$ is a set of weights that controls the contribution of different proposals, which is <b>exclusive</b> to each query and satisfies $\sum_{c=1}^C \alpha_{nc}(\omega) = 1$;</li> <li>as long as $C$ is smaller than $N$, the computational complexity of LARA is linear, that is, $\mathcal{O}(CN+CM)$.</li> </ul> </p> <div class="row mt-3" style="border: 1px solid #000; background-color: #ebebeb; border-radius: 10px; padding-top: 20px"> <p> <b> Remark: </b> We provide a detailed discussion about the parameterization of our proposal distributions in Appendix G.3 of our paper. To summarize, we find the key is to let different proposals depend on different sets of query information so that they could be as query-specific as possible. A good default is to divide the whole sequence into $C$ chunks, compute the mean of queries $\{\widetilde{\mbq}_c\}_{c=1}^C$ and keys $\{\widetilde{\mbk}_c\}_{c=1}^C$ within the same chunk, and set $q_c(\omega) = \mcN(\omega;\widetilde{\mbq}_c + \widetilde{\mbk}_c, \mathbf{I})$. We find this choice works well across various benchmarks. </p> </div> <h3 id="a-unified-view-of-lara-ra-and-rfa">A Unified View of LARA, RA, and RFA</h3> <p> LARA can be equivalently written in a similar way to RA and RFA. We spell it out here to see a systematic comparison among RA, LARA, and RFA: $$ \begin{align} \mathsf{RA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\xi(\mbq_n,\omega_n)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_n) \mbv_{m}^{\top}}{ \xi(\mbq_n,\omega_n)^\top \sum_{m'=1}^M\xi(\mbk_{m'}, \omega_n)}, &amp;&amp;\textcolor{keywords}{\omega_n \sim p_n(\omega)}\notag\\ \mathsf{LARA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{\sum_{c=1}^C \textcolor{strings}{\alpha'_{nc}(\omega_c)} \xi(\mbq_n,\omega_c)^\top \sum_{m=1}^M\xi(\mbk_m, \omega_c) \mbv_{m}^{\top}}{\sum_{c=1}^C \textcolor{strings}{\alpha'_{nc}(\omega_c)} \xi(\mbq_n,\omega_c)^\top \sum_{m=1}^M \xi(\mbk_{m}, \omega_c)}, &amp;&amp;\textcolor{keywords}{\omega_c \sim q_c(\omega)}\notag\\ \mathsf{RFA}\left(\mbq_{n},\mbK,\mbV\right) &amp;= \frac{ \sum_{s=1}^S\xi(\mbq_n,\omega_s)^{\top}\sum_{m=1}^M\xi(\mbk_m, \omega_s)\mbv_{m}^{\top}}{\sum_{s=1}^S \xi(\mbq_n,\omega_s)^{\top}\sum_{m'=1}^M\xi(\mbk_{m'}, \omega_s)}, &amp;&amp;\textcolor{keywords}{\omega_1,\dots,\omega_S \sim \mcN(\omega;0, \mathbf{I})} \notag \end{align} $$ where we denote $\alpha'_{nc}(\omega_c) \coloneqq \alpha_{nc}(\omega_c)\mcN(\omega_c;0, \mathbf{I})/q_c(\omega_c)$ to simplify the notation. Note that their major difference lies in the choice of sampling distributions.<br/> LARA is not designed to be a simple interpolation between RA and RFA; instead, it is a generalized estimation framework that includes both RA and RFA as its special cases. To see this, <ul> <li>LARA is equivalent to RA if $C = N$, $q_n(\omega) = p_n(\omega)$ and $\alpha_{nc}(\omega) = \delta_{nc}$ (that is, $\alpha_{nc}(\omega) = 1$ if $n = c$ and $0$ otherwise);</li> <li>LARA is equivalent to RFA if $q_c(\omega) = \mcN(\omega;0,\mathbf{I})$ and $\alpha_{nc}(\cdot)$ is constant for all $c = 1,\dots,C$ and $n = 1,\dots,N$.</li> </ul> With general proposals and weighting functions, LARA approximates softmax attention in a query-specific manner as in RA while achieving linear complexity as in RFA, effectively combining the advantages of both estimators. It is also easy to implement with a couple of lines of code. </p> <h2 id="experimental-results">Experimental Results</h2> <p>To demonstrate the effectiveness of our approach, we first visualize the approximation error of LARA to the true softmax attention outputs. Our unbiased estimate, RA, achieves the lowest MSE among these three methods and gets very close to the true softmax attention; RFA (Performer) soon plateaus at large approximation error and does not improve even with more samples, possibly due to low sample efficiency. On the other hand, LARA exhibits much lower MSE than RFA and the approximation error continually decreases as the number of proposals increases.</p> <div class="row mt-3"> <div class="col-sm-9 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/approx_error_deit_plot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Mean Squared Error (MSE) between the true softmax attention and different RF approximations under different numbers of samples (lower is better), which are evaluated on DeiT.</figcaption> </figure> </div> </div> <p>We further verify the improved performance of LARA by conducting experiments across a wide range of data modalities, including images, videos, natural language texts, and a long-sequence benchmark. From the <a name="comp-tb"> Table </a> below, we observe that</p> <ul> <li>RA performs competitively with conventional softmax attention, and</li> <li>LARA greatly improves the performance of RFA and significantly reduces the performance gap between RFA and softmax attention.</li> </ul> <table> <thead> <tr> <th style="text-align: left"><font size="2"> Model </font></th> <th style="text-align: center"><font size="2"> Complexity </font></th> <th style="text-align: center"><font size="2"> Image Classification on ImageNet (ü†Ö) </font></th> <th style="text-align: center"><font size="2"> Video Recognition on SSv2 (ü†Ö) </font></th> <th style="text-align: center"><font size="2"> Machine Translation on WMT (ü†Ö) </font></th> <th style="text-align: center"><font size="2"> Long Range Arena suite (ü†Ö) </font></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Softmax</td> <td style="text-align: center">$\mathcal{O}(N^2)$</td> <td style="text-align: center">79.9</td> <td style="text-align: center">66.5</td> <td style="text-align: center">27.5</td> <td style="text-align: center">59.08</td> </tr> <tr> <td style="text-align: left">RA</td> <td style="text-align: center">$\mathcal{O}(N^2)$</td> <td style="text-align: center">80.0</td> <td style="text-align: center">64.9</td> <td style="text-align: center">27.8</td> <td style="text-align: center">59.30</td> </tr> <tr> <td style="text-align: left">RFA</td> <td style="text-align: center">$\mathcal{O}(N)$</td> <td style="text-align: center">74.3</td> <td style="text-align: center">53.1</td> <td style="text-align: center">23.7</td> <td style="text-align: center">57.63</td> </tr> <tr> <td style="text-align: left">LARA</td> <td style="text-align: center">$\mathcal{O}(N)$</td> <td style="text-align: center"><strong>79.5</strong></td> <td style="text-align: center"><strong>63.7</strong></td> <td style="text-align: center"><strong>27.0</strong></td> <td style="text-align: center"><strong>59.12</strong></td> </tr> </tbody> </table> <p>LARA also enjoys much better scalability and could achieve SOTA results for image classification when applying it to advanced transformer architectures.</p> <div class="row mt-3"> <div class="col-sm-7 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/imagenet-sota-lara.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">SOTA results on ImageNet-1k across various model architectures.</figcaption> </figure> </div> </div> <p>Finally, we evaluate the empirical efficiency of different attention methods. We note that RA runs almost twice slower than softmax attention, while its linear variant LARA runs much faster and brings marginal computational overheads compared to RFA.</p> <div class="row mt-3"> <div class="col-sm mb-0 mt-0 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-07-12-lara-imgs/mem-time-comparison-1400.webp"/> <img src="/assets/img/2022-07-12-lara-imgs/mem-time-comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Empirical memory consumption (left) and running time (right) of different attention mechanisms under different sequence lengths. Metrics are measured relative to the true softmax attention.</figcaption> </figure> </div> </div> <h2 id="conclusion-and-limitation">Conclusion and Limitation</h2> <p>In this work, we generalize the random feature attention (RFA) algorithm in two aspects:</p> <ul> <li>On the one hand, we generalize the unbiased <strong>kernel</strong> estimation (RFA) to unbiased <strong>attention</strong> estimation (RA);</li> <li>On the other hand, we uncover the self-normalized importance sampling nature of RFA and propose a general framework <strong>LARA</strong> that includes both RA and RFA as its special cases.</li> </ul> <p>LARA greatly improves the performance of RFA while also maintaining its efficiency of RFA. Our framework provides a novel perspective for understanding and improving RF-based attention approximation, which is also orthogonal to most previous work. At the same time, there are several limitations of our approach:</p> <ul> <li>The choice of proposal distributions is sub-optimal. Although our modeling strategy enables a query-specific treatment as desired in RA, it needs to evaluate all the $C$ samples for each key-value pair, incurring a potential waste of computing. Future work might include more sophisticated implementations that use variable sample sets for different key-value pairs.</li> <li>Currently, LARA is still a biased (but consistent) estimator for softmax attention. It would be interesting to adopt advanced sampling strategies (e.g., Markov chain Monte Carlo (MCMC), etc.) to derive lower-bias lower-variance estimators.</li> <li>The proposal distribution should be carefully designed when dealing with auto-regressive modeling since it is non-trivial to apply the causal masking mechanism to LARA as in conventional softmax attention. Adapting the importance sampling formulation of LARA to support arbitrary auto-regressive modeling is also an interesting research direction.</li> </ul> ]]></content><author><name>Lin Zheng</name></author><category term="attention"/><category term="transformers"/><summary type="html"><![CDATA[A blog post on novel perspectives to understand random feature attention]]></summary></entry></feed>