@article{elman1990rnn,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  year={1990},
}
@inproceedings{qi2020prophetnet,
    title = "{P}rophet{N}et: Predicting Future N-gram for Sequence-to-{S}equence{P}re-training",
    author = "Qi, Weizhen  and
      Yan, Yu  and
      Gong, Yeyun  and
      Liu, Dayiheng  and
      Duan, Nan  and
      Chen, Jiusheng  and
      Zhang, Ruofei  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    url = "https://aclanthology.org/2020.findings-emnlp.217",
}
@inproceedings{cai2024medusa,
    title={Medusa: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
    author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=PEpbUobfJv}
}
@article{stern2018blockwise,
  title={Blockwise parallel decoding for deep autoregressive models},
  author={Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}
@article{gloeckle2024multitoken,
  title={Better \& faster large language models via multi-token prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2404.19737},
  year={2024}
}

@inproceedings{li2023contrastive,
    title = "Contrastive Decoding: Open-ended Text Generation as Optimization",
    author = "Li, Xiang Lisa  and
      Holtzman, Ari  and
      Fried, Daniel  and
      Liang, Percy  and
      Eisner, Jason  and
      Hashimoto, Tatsunori  and
      Zettlemoyer, Luke  and
      Lewis, Mike",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.687",
}
@article{ho2022diffusioncfg,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}
@inproceedings{xu2024lemur,
    title={Lemur: Harmonizing Natural Language and Code for Language Agents},
    author={Yiheng Xu and Hongjin SU and Chen Xing and Boyu Mi and Qian Liu and Weijia Shi and Binyuan Hui and Fan Zhou and Yitao Liu and Tianbao Xie and Zhoujun Cheng and Siheng Zhao and Lingpeng Kong and Bailin Wang and Caiming Xiong and Tao Yu},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=hNhwSmtXRh}
}
@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Mesnard, Gemma Team: Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}
@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Riviere, Gemma Team: Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}
@article{reid2024gemini15,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@article{yang2024rethinking,
  title={Rethinking tokenization: Crafting better tokenizers for large language models},
  author={Yang, Jinbiao},
  journal={International Journal of Chinese Linguistics},
  year={2024},
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={and Anil, Gemini Team: Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{li2024dclm,
  title={DataComp-LM: In search of the next generation of training sets for language models},
  author={Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and others},
  journal={arXiv preprint arXiv:2406.11794},
  year={2024}
}
@article{blakeney2024pretrainanneal,
  title={Does your data spark joy? Performance gains from domain upsampling at the end of training},
  author={Blakeney, Cody and Paul, Mansheej and Larsen, Brett W and Owen, Sean and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2406.03476},
  year={2024}
}
@inproceedings{deletang2024lmiscompression,
    title={Language Modeling Is Compression},
    author={Gregoire Deletang and Anian Ruoss and Paul-Ambroise Duquenne and Elliot Catt and Tim Genewein and Christopher Mattern and Jordi Grau-Moya and Li Kevin Wenliang and Matthew Aitchison and Laurent Orseau and Marcus Hutter and Joel Veness},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=jznbgiynus}
}
@article{huang2024compression,
  title={Compression represents intelligence linearly},
  author={Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian},
  journal={arXiv preprint arXiv:2404.09937},
  year={2024}
}
@article{gu2024chared,
  title={CharED: Character-wise Ensemble Decoding for Large Language Models},
  author={Gu, Kevin and Tuecke, Eva and Katz, Dmitriy and Horesh, Raya and Alvarez-Melis, David and Yurochkin, Mikhail},
  journal={arXiv preprint arXiv:2407.11009},
  year={2024}
}
@article{muennighoff2024olmoe,
  title={OLMoE: Open Mixture-of-Experts Language Models},
  author={Muennighoff, Niklas and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Morrison, Jacob and Min, Sewon and Shi, Weijia and Walsh, Pete and Tafjord, Oyvind and Lambert, Nathan and others},
  journal={arXiv preprint arXiv:2409.02060},
  year={2024}
}
@inproceedings{liu2023character,
    title = "Character-Aware Models Improve Visual Text Rendering",
    author = "Liu, Rosanne  and
      Garrette, Dan  and
      Saharia, Chitwan  and
      Chan, William  and
      Roberts, Adam  and
      Narang, Sharan  and
      Blok, Irina  and
      Mical, Rj  and
      Norouzi, Mohammad  and
      Constant, Noah",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.900",
    pages = "16270--16297",
}
@article{liu2024glyphbyt5,
  title={Glyph-byt5: A customized text encoder for accurate visual text rendering},
  author={Liu, Zeyu and Liang, Weicong and Liang, Zhanhao and Luo, Chong and Li, Ji and Huang, Gao and Yuan, Yuhui},
  journal={arXiv preprint arXiv:2403.09622},
  year={2024}
}
@article{liu2024glyphbyt5v2,
  title={Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering},
  author={Liu, Zeyu and Liang, Weicong and Zhao, Yiming and Chen, Bohan and Li, Ji and Yuan, Yuhui},
  journal={arXiv preprint arXiv:2406.10208},
  year={2024}
}
@inproceedings{
chen2023textdiffuser,
title={TextDiffuser: Diffusion Models as Text Painters},
author={Jingye Chen and Yupan Huang and Tengchao Lv and Lei Cui and Qifeng Chen and Furu Wei},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=ke3RgcDmfO}
}

@inproceedings{alrfou2019character,
  title={Character-level language modeling with deeper self-attention},
  author={Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2019}
}
@article{yuan2023speculativecontrastive,
  title={Speculative contrastive decoding},
  author={Yuan, Hongyi and Lu, Keming and Huang, Fei and Yuan, Zheng and Zhou, Chang},
  journal={arXiv preprint arXiv:2311.08981},
  year={2023}
}
@article{kim2023elementwise,
  title={Elementwise Language Representation},
  author={Kim, Dunam and Kim, Jeeeun},
  journal={arXiv preprint arXiv:2302.13475},
  year={2023}
}
@inproceedings{sutskever2011generating,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={1017--1024},
  year={2011}
}
@article{graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}
@article{radford2017learning,
  title={Learning to generate reviews and discovering sentiment},
  author={Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1704.01444},
  year={2017}
}
@inproceedings{hwang2017character,
  title={Character-level language modeling with hierarchical recurrent neural networks},
  author={Hwang, Kyuyeon and Sung, Wonyong},
  booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5720--5724},
  year={2017},
  organization={IEEE}
}
@inproceedings{
chung2017hierarchical,
title={Hierarchical Multiscale Recurrent Neural Networks},
author={Junyoung Chung and Sungjin Ahn and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1di0sfgl}
}
@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}
@article{
horton2024bytes,
title={Bytes Are All You Need: Transformers Operating Directly On File Bytes},
author={Maxwell Horton and Sachin Mehta and Ali Farhadi and Mohammad Rastegari},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=RkaqxxAOfN}
}
@article{lester2024training,
  title={Training LLMs over Neurally Compressed Text},
  author={Lester, Brian and Lee, Jaehoon and Alemi, Alex and Pennington, Jeffrey and Roberts, Adam and Sohl-Dickstein, Jascha and Constant, Noah},
  journal={arXiv preprint arXiv:2404.03626},
  year={2024}
}
@inproceedings{
ge2024icae,
title={In-context Autoencoder for Context Compression in a Large Language Model},
author={Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uREj4ZuGJE}
}
@article{han2024jpeglm,
  title={JPEG-LM: LLMs as Image Generators with Canonical Codec Representations},
  author={Han, Xiaochuang and Ghazvininejad, Marjan and Koh, Pang Wei and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2408.08459},
  year={2024}
}
@inproceedings{jiang2023zipclassification,
    title = "{``}Low-Resource{''} Text Classification: A Parameter-Free Classification Method with Compressors",
    author = "Jiang, Zhiying  and
      Yang, Matthew  and
      Tsirlin, Mikhail  and
      Tang, Raphael  and
      Dai, Yiqin  and
      Lin, Jimmy",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
    url = "https://aclanthology.org/2023.findings-acl.426"
}

@article{tai2024pixar,
  title={PIXAR: Auto-Regressive Language Modeling in Pixel Space},
  author={Tai, Yintao and Liao, Xiyang and Suglia, Alessandro and Vergari, Antonio},
  journal={arXiv preprint arXiv:2401.03321},
  year={2024}
}
@inproceedings{
rust2023pixel,
title={Language Modelling with Pixels},
author={Phillip Rust and Jonas F. Lotz and Emanuele Bugliarello and Elizabeth Salesky and Miryam de Lhoneux and Desmond Elliott},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=FkSp8VW8RjH}
}
@inproceedings{lotz2023textrendering,
    title = "Text Rendering Strategies for Pixel Language Models",
    author = "Lotz, Jonas  and
      Salesky, Elizabeth  and
      Rust, Phillip  and
      Elliott, Desmond",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.628",
}
@article{li2023glyphdiffusion,
  title={Glyphdiffusion: Text generation as image generation},
  author={Li, Junyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2304.12519},
  year={2023}
}

@InProceedings{lee23pix2struct,
  title = 	 {{P}ix2{S}truct: Screenshot Parsing as Pretraining for Visual Language Understanding},
  author =       {Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/lee23g.html}
}

@article{gao2024improving,
  title={Improving Language Understanding from Screenshots},
  author={Gao, Tianyu and Wang, Zirui and Bhaskar, Adithya and Chen, Danqi},
  journal={arXiv preprint arXiv:2402.14073},
  year={2024}
}
@inproceedings{salesky2023multilingualpixel,
    title = "Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer",
    author = "Salesky, Elizabeth  and
      Verma, Neha  and
      Koehn, Philipp  and
      Post, Matt",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.854",
}
@inproceedings{salesky2021robust,
    title = "Robust Open-Vocabulary Translation from Visual Text Representations",
    author = "Salesky, Elizabeth  and
      Etter, David  and
      Post, Matt",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.576"
}
@article{land2024fishing,
  title={Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models},
  author={Land, Sander and Bartolo, Max},
  journal={arXiv preprint arXiv:2405.05417},
  year={2024}
}

@inproceedings{gao-etal-2020-character,
    title = "Character-Level Translation with Self-attention",
    author = "Gao, Yingqiang  and
      Nikolov, Nikola I.  and
      Hu, Yuhuang  and
      Hahnloser, Richard H.R.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.145",
    doi = "10.18653/v1/2020.acl-main.145",
    pages = "1591--1604",
}
@article{xue2022byt5,
    title = "{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
    author = "Xue, Linting  and
      Barua, Aditya  and
      Constant, Noah  and
      Al-Rfou, Rami  and
      Narang, Sharan  and
      Kale, Mihir  and
      Roberts, Adam  and
      Raffel, Colin",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2022",
    url = "https://aclanthology.org/2022.tacl-1.17",
}
@article{clark2022canine,
    title = "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
    author = "Clark, Jonathan H.  and
      Garrette, Dan  and
      Turc, Iulia  and
      Wieting, John",
    journal = "Transactions of the Association for Computational Linguistics",
    year = "2022",
    url = "https://aclanthology.org/2022.tacl-1.5",
}
@inproceedings{chevalier2023autocompressors,
    title = "Adapting Language Models to Compress Contexts",
    author = "Chevalier, Alexis  and
      Wettig, Alexander  and
      Ajith, Anirudh  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.232"
}

@article{choe2019bridging,
  title={Bridging the gap for tokenizer-free language models},
  author={Choe, Dokook and Al-Rfou, Rami and Guo, Mandy and Lee, Heeyoung and Constant, Noah},
  journal={arXiv preprint arXiv:1908.10322},
  year={2019}
}
@inproceedings{
yu2023megabyte,
title={{MEGABYTE}: Predicting Million-byte Sequences with Multiscale Transformers},
author={Lili Yu and Daniel Simig and Colin Flaherty and Armen Aghajanyan and Luke Zettlemoyer and Mike Lewis},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=JTmO2V9Xpz}
}
@article{wang2024mambabyte,
  title={Mambabyte: Token-free selective state space model},
  author={Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
  journal={arXiv preprint arXiv:2401.13660},
  year={2024}
}
@article{slagle2024spacebyte,
  title={SpaceByte: Towards Deleting Tokenization from Large Language Modeling},
  author={Slagle, Kevin},
  journal={arXiv preprint arXiv:2404.14408},
  year={2024}
}
@article{perez2024compressed,
  title={Compressed-Language Models for Understanding Compressed File Formats: a JPEG Exploration},
  author={P{\'e}rez, Juan C and Pardo, Alejandro and Soldan, Mattia and Itani, Hani and Leon-Alcazar, Juan and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2405.17146},
  year={2024}
}
@article{wu2024bgpt,
  title={Beyond Language Models: Byte Models are Digital World Simulators},
  author={Wu, Shangda and Tan, Xu and Wang, Zili and Wang, Rui and Li, Xiaobing and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.19155},
  year={2024}
}
@article{phan2024understanding,
  title={Understanding and Mitigating Tokenization Bias in Language Models},
  author={Phan, Buu and Havasi, Marton and Muckley, Matthew and Ullrich, Karen},
  journal={arXiv preprint arXiv:2406.16829},
  year={2024}
}
@inproceedings{nawrot2023dynamicpool,
    title = "Efficient Transformers with Dynamic Token Pooling",
    author = "Nawrot, Piotr  and
      Chorowski, Jan  and
      Lancucki, Adrian  and
      Ponti, Edoardo Maria",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.353"
}
@inproceedings{nawrot2022hourglass,
    title = "Hierarchical Transformers Are More Efficient Language Models",
    author = "Nawrot, Piotr  and
      Tworkowski, Szymon  and
      Tyrolski, Micha{\l}  and
      Kaiser, Lukasz  and
      Wu, Yuhuai  and
      Szegedy, Christian  and
      Michalewski, Henryk",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    year = "2022",
    url = "https://aclanthology.org/2022.findings-naacl.117"
}
@inproceedings{
rae2020compressive,
title={Compressive Transformers for Long-Range Sequence Modelling},
author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylKikSYDH}
}
@InProceedings{zheng22blara,
  title = 	 {Linear Complexity Randomized Self-attention Mechanism},
  author =       {Zheng, Lin and Wang, Chong and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v162/zheng22b.html},
}
@inproceedings{
zheng2023eva,
title={Efficient Attention via Control Variates},
author={Lin Zheng and Jianbo Yuan and Chong Wang and Lingpeng Kong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=G-uNfHKrj46}
}
@inproceedings{
peng2021rfa,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@article{baker2019control,
  title={Control variates for stochastic gradient MCMC},
  author={Baker, Jack and Fearnhead, Paul and Fox, Emily B and Nemeth, Christopher},
  journal={Statistics and Computing},
  volume={29},
  number={3},
  pages={599--615},
  year={2019},
  publisher={Springer}
}
@article{greensmith2004variance,
  title={Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={9},
  year={2004},
  publisher={Citeseer}
}
@inproceedings{
grathwohl2018backpropagation,
title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoff Roeder and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyzKd1bCW},
}
@article{geffner2018using,
  title={Using large ensembles of control variates for variational inference},
  author={Geffner, Tomas and Domke, Justin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{hesterberg1995snis,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1269620},
 abstract = {Importance sampling uses observations from one distribution to estimate for another distribution by weighting the observations. Including the target distribution as one component of a mixture distribution bounds the weights and makes importance sampling more reliable. The usual importance-sampling estimate is a weighted average with weights that do not sum to 1. We discuss simple normalization and other, more efficient normalization methods. These innovations make importance sampling useful in a wider variety of problems. We demonstrate with a case study of oil-inventory reliability at a large utility.},
 author = {Tim Hesterberg},
 journal = {Technometrics},
 number = {2},
 pages = {185--194},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Weighted Average Importance Sampling and Defensive Mixture Distributions},
 urldate = {2022-05-07},
 volume = {37},
 year = {1995}
}
@inproceedings{veach1995optimally,
  title={Optimally combining sampling techniques for Monte Carlo rendering},
  author={Veach, Eric and Guibas, Leonidas J},
  booktitle={Proceedings of the 22nd annual conference on Computer graphics and interactive techniques},
  pages={419--428},
  year={1995}
}
@inproceedings{wang2023recode,
    title = "{R}e{C}ode: Robustness Evaluation of Code Generation Models",
    author = "Wang, Shiqi  and
      Li, Zheng  and
      Qian, Haifeng  and
      Yang, Chenghao  and
      Wang, Zijian  and
      Shang, Mingyue  and
      Kumar, Varun  and
      Tan, Samson  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Nallapati, Ramesh  and
      Ramanathan, Murali Krishna  and
      Roth, Dan  and
      Xiang, Bing",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.773"
}
@article{akhauri2024attamba,
  title={Attamba: Attending To Multi-Token States},
  author={Akhauri, Yash and Huda, Safeen and Abdelfattah, Mohamed S},
  journal={arXiv preprint arXiv:2411.17685},
  year={2024}
}
@article{lambert2024tulu3,
  title={TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}
@inproceedings{repeat-augment,
  title={Augment Your Batch: Improving Generalization Through Instance Repetition},
  author={Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8129--8138},
  year={2020}
}
@inproceedings{cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6023--6032},
  year={2019}
}
@inproceedings{adamw,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}
@inproceedings{stochastic-depth,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}
@inproceedings{random-augment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}
@article{pvt,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={arXiv preprint arXiv:2102.12122},
  year={2021}
}
@article{pvtv2,
  title={Pvtv2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={arXiv preprint arXiv:2106.13797},
  year={2021}
}
@article{dai2020funnel,
  title={Funnel-transformer: Filtering out sequential redundancy for efficient language processing},
  author={Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={4271--4282},
  year={2020}
}
@article{xiong2021simple,
  title={Simple Local Attentions Remain Competitive for Long-Context Tasks},
  author={Xiong, Wenhan and O{\u{g}}uz, Barlas and Gupta, Anchit and Chen, Xilun and Liskovich, Diana and Levy, Omer and Yih, Wen-tau and Mehdad, Yashar},
  journal={arXiv preprint arXiv:2112.07210},
  year={2021}
}
@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{kasai2021t2r,
    title = "Finetuning Pretrained Transformers into {RNN}s",
    author = "Kasai, Jungo  and
      Peng, Hao  and
      Zhang, Yizhe  and
      Yogatama, Dani  and
      Ilharco, Gabriel  and
      Pappas, Nikolaos  and
      Mao, Yi  and
      Chen, Weizhu  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.830",
    doi = "10.18653/v1/2021.emnlp-main.830",
    pages = "10630--10643",
}
@inproceedings{ott-etal-2018-scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9"
}
@inproceedings{ott-etal-2019-fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53"
}
@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}
@article{
wei2022emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=yzkSU5zdwD},
note={Survey Certification}
}
@article{kaplan2020scalinglaw,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hoffmann2022chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@misc{openai2024o1,
  title={Learning to Reason with LLMs},
  author={OpenAI},
  journal = "https://openai.com/",
  url={https://openai.com/index/learning-to-reason-with-llms/},
  year={2024}
}
@misc{radford2018gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}
@misc{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 year = {2020}
}
@article{openai2023gpt4,
  title={{GPT}-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{rabe2021self,
  title={Self-attention Does Not Need {$\mathcal{O}(n^2)$} Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}
@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}
@inproceedings{random-erasing,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={13001--13008},
  year={2020}
}
@article{qin2022devil,
  title={The Devil in Linear Transformer},
  author={Qin, Zhen and Han, XiaoDong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
  journal={arXiv preprint arXiv:2210.10340},
  year={2022}
}
@article{cos-lr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@article{liu2022ecoformer,
  title={EcoFormer: Energy-Saving Attention with Linear Complexity},
  author={Liu, Jing and Pan, Zizheng and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2209.09004},
  year={2022}
}
@article{convit,
  title={Convit: Improving vision transformers with soft convolutional inductive biases},
  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  journal={arXiv preprint arXiv:2103.10697},
  year={2021}
}
@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:2106.14881},
  year={2021}
}
@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}
@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}
@inproceedings{gupta-berant-2021-value,
    title = "Value-aware Approximate Attention",
    author = "Gupta, Ankit  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.753",
    pages = "9567--9574",
}

@inproceedings{random-features,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2008}
}

@InProceedings{mixture-reparam,
  title = 	 { Automatic Differentiation Variational Inference with Mixtures },
  author =       {Morningstar, Warren and Vikram, Sharad and Ham, Cusuh and Gallagher, Andrew and Dillon, Joshua},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3250--3258},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/morningstar21b/morningstar21b.pdf},
  url = 	 {https://proceedings.mlr.press/v130/morningstar21b.html},
}
@article{owen2000safe,
  title={Safe and effective importance sampling},
  author={Owen, Art and Zhou, Yi},
  journal={Journal of the American Statistical Association},
  volume={95},
  number={449},
  pages={135--143},
  year={2000},
  publisher={Taylor \& Francis}
}
@inproceedings{stickingtheland2017nips,
 author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference},
 url = {https://proceedings.neurips.cc/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf},
 volume = {30},
 year = {2017}
}


@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}
@article{lin2021survey,
  title={A Survey of Transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2106.04554},
  year={2021}
}
@article{snelson2006sparse,
  title={Sparse Gaussian processes using pseudo-inputs},
  author={Snelson, Edward and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  volume={18},
  pages={1257},
  year={2006},
  publisher={Citeseer}
}
@article{ma2021luna,
  title={Luna: Linear Unified Nested Attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2106.01540},
  year={2021}
}
@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International Conference on Machine Learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}
@article{tay2019compositional,
  title={Compositional de-attention networks},
  author={Tay, Yi and Luu, Anh Tuan and Zhang, Aston and Wang, Shuohang and Hui, Siu Cheung},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}
@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}
@inproceedings{yang2019xlnet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {5753--5763},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{Yu2019vqa,
author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
title = {Deep Modular Co-Attention Networks for Visual Question Answering},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{fan2020bayesian,
  title={Bayesian Attention Modules},
  author={Fan, Xinjie and Zhang, Shujian and Chen, Bo and Zhou, Mingyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article {Rivese2016239118,
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	number = {15},
	elocation-id = {e2016239118},
	year = {2021},
	doi = {10.1073/pnas.2016239118},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/15/e2016239118},
	eprint = {https://www.pnas.org/content/118/15/e2016239118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
@article{jaegle2021perceiverio,
  title={Perceiver io: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}
@article{likhosherstov2022chefs,
  title={Chefs' Random Tables: Non-Trigonometric Random Features},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Dubey, Avinava and Liu, Frederick and Sarlos, Tamas and Weller, Adrian},
  journal={arXiv preprint arXiv:2205.15317},
  year={2022}
}

@InProceedings{mra-attention,
  title = 	 {Multi Resolution Analysis ({MRA}) for Approximate Self-Attention},
  author =       {Zeng, Zhanpeng and Pal, Sourav and Kline, Jeffery and Fung, Glenn M and Singh, Vikas},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25955--25972},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zeng22a/zeng22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zeng22a.html},
}
@InProceedings{schlag21a,
  title = 	 {Linear Transformers Are Secretly Fast Weight Programmers},
  author =       {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9355--9366},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/schlag21a.html}
}

@inproceedings{h-transformer,
    title = "{H}-Transformer-1{D}: Fast One-Dimensional Hierarchical Attention for Sequences",
    author = "Zhu, Zhenhai  and
      Soricut, Radu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.294",
    doi = "10.18653/v1/2021.acl-long.294",
    pages = "3801--3815"
}
@article{madaan2022treeformer,
  title={Treeformer: Dense Gradient Trees for Efficient Attention Computation},
  author={Madaan, Lovish and Bhojanapalli, Srinadh and Jain, Himanshu and Jain, Prateek},
  journal={arXiv preprint arXiv:2208.09015},
  year={2022}
}
@article{chowdhury2022learning,
title={Learning the Transformer Kernel},
author={Sankalan Pal Chowdhury and Adamos Solomou and Kumar Avinava Dubey and Mrinmaya Sachan},
journal={Transactions on Machine Learning Research},
year={2022},
url={https://openreview.net/forum?id=tLIBAEYjcv},
note={}
}
@inproceedings{
irie2021going,
title={Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
author={Kazuki Irie and Imanol Schlag and R{\'o}bert Csord{\'a}s and J{\"u}rgen Schmidhuber},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ot2ORiBqTa1}
}
@InProceedings{liutkus2021spe,
  title = 	 {Relative Positional Encoding for Transformers with Linear Complexity},
  author =       {Liutkus, Antoine and C\'{\i}fka, Ond{\v{r}}ej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7067--7079},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/liutkus21a/liutkus21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/liutkus21a.html},
}
@inproceedings{chen-2021-permuteformer,
    title = "{P}ermute{F}ormer: Efficient Relative Position Encoding for Long Sequences",
    author = "Chen, Peng",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.828",
    doi = "10.18653/v1/2021.emnlp-main.828",
    pages = "10606--10618"
}

@InProceedings{ripple,
  title = 	 {Ripple Attention for Visual Perception with Sub-quadratic Complexity},
  author =       {Zheng, Lin and Pan, Huijie and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26993--27010},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22a/zheng22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22a.html}
}
@inproceedings{
zhen2022cosformer,
title={cosFormer: Rethinking Softmax In Attention},
author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@inproceedings{
luo2021stable,
title={Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding},
author={Shengjie Luo and Shanda Li and Tianle Cai and Di He and Dinglan Peng and Shuxin Zheng and Guolin Ke and Liwei Wang and Tie-Yan Liu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=X7XNPor93uG}
}
@article{tu2022maxvit,
  title={Maxvit: Multi-axis vision transformer},
  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  journal={arXiv preprint arXiv:2204.01697},
  year={2022}
}
@article{focal,
  title={Focal Attention for Long-Range Interactions in Vision Transformers},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{regnet,
  title={Designing network design spaces},
  author={Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10428--10436},
  year={2020}
}
@article{vil,
  title={Multi-scale vision longformer: A new vision transformer for high-resolution image encoding},
  author={Zhang, Pengchuan and Dai, Xiyang and Yang, Jianwei and Xiao, Bin and Yuan, Lu and Zhang, Lei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2103.15358},
  year={2021}
}
@article{regionvit,
  title={RegionViT: Regional-to-Local Attention for Vision Transformers},
  author={Chen, Chun-Fu and Panda, Rameswar and Fan, Quanfu},
  journal={arXiv preprint arXiv:2106.02689},
  year={2021}
}
@article{cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  journal={arXiv preprint arXiv:2103.15808},
  year={2021}
}
@article{li2022uniformer,
  title={Uniformer: Unifying convolution and self-attention for visual recognition},
  author={Li, Kunchang and Wang, Yali and Zhang, Junhao and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2201.09450},
  year={2022}
}
@inproceedings{
choromanski2022hybrid,
title={Hybrid Random Features},
author={Krzysztof Marcin Choromanski and Han Lin and Haoxian Chen and Arijit Sehanobish and Yuanzhe Ma and Deepali Jain and Jake Varley and Andy Zeng and Michael S Ryoo and Valerii Likhosherstov and Dmitry Kalashnikov and Vikas Sindhwani and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=EMigfE6ZeS}
}
@inproceedings{
kasai2021deepNMT,
title={Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation},
author={Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah Smith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KpfasTaLUpq}
}
@inproceedings{sennrich-etal-2016-bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{ott2018scalingNMT,
  title={Scaling neural machine translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1806.00187},
  year={2018}
}
@inproceedings{bojar2014wmt,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}
@inproceedings{ainslie2020etc,
  title={ETC: Encoding Long and Structured Inputs in Transformers},
  author={Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={268--284},
  year={2020}
}
@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}
@inproceedings{maas2011imdb,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}
@article{radev2013aan,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  number={4},
  pages={919--944},
  year={2013},
  publisher={Springer}
}
@article{krizhevsky2009cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@inproceedings{linsley2018pathfinder,
  title={Learning long-range spatial dependencies with horizontal gated recurrent units},
  author={Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charlie and Serre, Thomas},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={152--164},
  year={2018}
}
@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}
@inproceedings{
liu2018generating,
title={Generating Wikipedia by Summarizing Long Sequences},
author={Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hyg0vbWC-},
}
@inproceedings{zaheer2020bigbird,
 author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17283--17297},
 publisher = {Curran Associates, Inc.},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{tay2021synthesizer,
  title={Synthesizer: Rethinking self-attention for transformer models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  booktitle={International Conference on Machine Learning},
  pages={10183--10192},
  year={2021},
  organization={PMLR}
}
@inproceedings{zeng2021yolo,
  title={You only sample (almost) once: Linear cost self-attention via bernoulli sampling},
  author={Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya and Acharya, Shailesh and Fung, Glenn M and Singh, Vikas},
  booktitle={International Conference on Machine Learning},
  pages={12321--12332},
  year={2021},
  organization={PMLR}
}
@inproceedings{katharopoulos2020transformers_are_rnns,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@article{lee2021fnet,
  title={FNet: Mixing Tokens with Fourier Transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}
@article{peng2021abc,
  title={ABC: Attention with Bounded-memory Control},
  author={Peng, Hao and Kasai, Jungo and Pappas, Nikolaos and Yogatama, Dani and Wu, Zhaofeng and Kong, Lingpeng and Schwartz, Roy and Smith, Noah A},
  journal={arXiv preprint arXiv:2110.02488},
  year={2021}
}
@article{books3,
  title={Books3},
  author={Shawn Presser},
  url={https://twitter.com/theshawwn/status/1320282149329784833},
  year={2020}
}
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@inproceedings{
liu2018actiondependent,
title={Action-dependent Control Variates for Policy Optimization via Stein Identity},
author={Hao Liu and Yihao Feng and Yi Mao and Dengyong Zhou and Jian Peng and Qiang Liu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1mCp-ZRZ},
}
@article{zhu2021long-short,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.03130},
  year={2017}
}
@inproceedings{bbvi,
  title={Black box variational inference},
  author={Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  booktitle={Artificial intelligence and statistics},
  pages={814--822},
  year={2014},
  organization={PMLR}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={14138--14148},
  year={2021}
}
@article{ren2021combiner,
  title={Combiner: Full attention transformer with sparse computation cost},
  author={Ren, Hongyu and Dai, Hanjun and Dai, Zihang and Yang, Mengjiao and Leskovec, Jure and Schuurmans, Dale and Dai, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{limisiewicz2024myte,
  title={MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling},
  author={Limisiewicz, Tomasz and Blevins, Terra and Gonen, Hila and Ahia, Orevaoghene and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2403.10691},
  year={2024}
}
@inproceedings{
tay2022charformer,
title={Charformer: Fast Character Transformers via Gradient-based Subword Tokenization},
author={Yi Tay and Vinh Q. Tran and Sebastian Ruder and Jai Gupta and Hyung Won Chung and Dara Bahri and Zhen Qin and Simon Baumgartner and Cong Yu and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JtBRnrlOEFN}
}
@article{schmidt2024tokenization,
  title={Tokenization Is More Than Compression},
  author={Schmidt, Craig W and Reddy, Varshini and Zhang, Haoran and Alameddine, Alec and Uzan, Omri and Pinter, Yuval and Tanner, Chris},
  journal={arXiv preprint arXiv:2402.18376},
  year={2024}
}
@inproceedings{ali2024tokenizer,
    title = "Tokenizer Choice For {LLM} Training: Negligible or Crucial?",
    author = {Ali, Mehdi  and
      Fromm, Michael  and
      Thellmann, Klaudia  and
      Rutmann, Richard  and
      L{\"u}bbering, Max  and
      Leveling, Johannes  and
      Klug, Katrin  and
      Ebert, Jan  and
      Doll, Niclas  and
      Buschhoff, Jasper  and
      Jain, Charvi  and
      Weber, Alexander  and
      Jurkschat, Lena  and
      Abdelwahab, Hammam  and
      John, Chelsea  and
      Ortiz Suarez, Pedro  and
      Ostendorff, Malte  and
      Weinbach, Samuel  and
      Sifa, Rafet  and
      Kesselheim, Stefan  and
      Flores-Herr, Nicolas},
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-naacl.247"
}
@article{wortsman2023smallforlargeinstabilities,
  title={Small-scale proxies for large-scale transformer training instabilities},
  author={Wortsman, Mitchell and Liu, Peter J and Xiao, Lechao and Everett, Katie and Alemi, Alex and Adlam, Ben and Co-Reyes, John D and Gur, Izzeddin and Kumar, Abhishek and Novak, Roman and others},
  journal={arXiv preprint arXiv:2309.14322},
  year={2023}
}
@inproceedings{wortsman2023stable,
title={Stable and low-precision training for large-scale vision-language models},
author={Mitchell Wortsman and Tim Dettmers and Luke Zettlemoyer and Ari S. Morcos and Ali Farhadi and Ludwig Schmidt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=sqqASmpA2R}
}
@article{wang2024adamwwd,
  title={How to set AdamW's weight decay as you scale model and dataset size},
  author={Wang, Xi and Aitchison, Laurence},
  journal={arXiv preprint arXiv:2405.13698},
  year={2024}
}
@article{molybog2023adaminstability,
  title={A theory on adam instability in large-scale machine learning},
  author={Molybog, Igor and Albert, Peter and Chen, Moya and DeVito, Zachary and Esiobu, David and Goyal, Naman and Koura, Punit Singh and Narang, Sharan and Poulton, Andrew and Silva, Ruan and others},
  journal={arXiv preprint arXiv:2304.09871},
  year={2023}
}
@InProceedings{park2023rgbnomore,
    author    = {Park, Jeongsoo and Johnson, Justin},
    title     = {RGB No More: Minimally-Decoded JPEG Vision Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {22334-22346}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{athiwaratkun2024token,
  title={Token Alignment via Character Matching for Subword Completion},
  author={Athiwaratkun, Ben and Wang, Shiqi and Shang, Mingyue and Tian, Yuchen and Wang, Zijian and Gonugondla, Sujan Kumar and Gouda, Sanjay Krishna and Kwiatowski, Rob and Nallapati, Ramesh and Xiang, Bing},
  journal={arXiv preprint arXiv:2403.08688},
  year={2024}
}
@article{fleshman2023toucan,
  title={Toucan: Token-Aware Character Level Language Modeling},
  author={Fleshman, William and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2311.08620},
  year={2023}
}
@inproceedings{godey2022manta,
    title = "{MANT}a: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling",
    author = "Godey, Nathan  and
      Castagn{\'e}, Roman  and
      de la Clergerie, {\'E}ric  and
      Sagot, Beno{\^\i}t",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    year = "2022",
    url = "https://aclanthology.org/2022.findings-emnlp.207"
}

@article{ahia2024magnet,
  title={MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization},
  author={Ahia, Orevaoghene and Kumar, Sachin and Gonen, Hila and Hoffman, Valentin and Limisiewicz, Tomasz and Tsvetkov, Yulia and Smith, Noah A},
  journal={arXiv preprint arXiv:2407.08818},
  year={2024}
}
@article{hayase2024datamixture,
  title={Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?},
  author={Hayase, Jonathan and Liu, Alisa and Choi, Yejin and Oh, Sewoong and Smith, Noah A},
  journal={arXiv preprint arXiv:2407.16607},
  year={2024}
}
@inproceedings{chen2021skyformer,
  title={Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr$\backslash$" om Method},
  author={Chen, Yifan and Zeng, Qi and Ji, Heng and Yang, Yun},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}
@inproceedings{lu2021soft,
  title={SOFT: Softmax-free Transformer with Linear Complexity},
  author={Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}
@article{hua2022flash,
  title={Transformer Quality in Linear Time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V},
  journal={arXiv preprint arXiv:2202.10447},
  year={2022}
}
@inproceedings{rebar,
 author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models},
 url = {https://proceedings.neurips.cc/paper/2017/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{paisley2012vi,
author = {Paisley, John and Blei, David M. and Jordan, Michael I.},
title = {Variational Bayesian Inference with Stochastic Search},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1363â€“1370},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}
@inproceedings{chong2013var,
 author = {Wang, Chong and Chen, Xi and Smola, Alexander J and Xing, Eric P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Variance Reduction for Stochastic Gradient Optimization},
 url = {https://proceedings.neurips.cc/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf},
 volume = {26},
 year = {2013}
}
@inproceedings{
vlassis2021control,
title={Control Variates for Slate Off-Policy Evaluation},
author={Nikos Vlassis and Ashok Chandrashekar and Fernando Amat and Nathan Kallus},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=e9_UPqMNfi}
}
@article{hutchins2022blockrnn,
       author = {{Hutchins}, DeLesley and {Schlag}, Imanol and {Wu}, Yuhuai and {Dyer}, Ethan and {Neyshabur}, Behnam},
        title = "{Block-Recurrent Transformers}",
      journal = {arXiv preprint arXiv:2203.07852},
         year = 2022,
}
@article{nguyen2021fmmformer,
  title={Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention},
  author={Nguyen, Tan and Suliafu, Vai and Osher, Stanley and Chen, Long and Wang, Bao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{chen2021scatterbrain,
  title={Scatterbrain: Unifying sparse and low-rank attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}
@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}
@article{cohen2022adaptive,
  title={Adaptive gradient methods at the edge of stability},
  author={Cohen, Jeremy M and Ghorbani, Behrooz and Krishnan, Shankar and Agarwal, Naman and Medapati, Sourabh and Badura, Michal and Suo, Daniel and Cardoze, David and Nado, Zachary and Dahl, George E and others},
  journal={arXiv preprint arXiv:2207.14484},
  year={2022}
}
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  year={2018},
}
@InProceedings{jaegle2021perceiver,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html}
}
@inproceedings{
gu2022s4,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}
@InProceedings{flowformer,
  title = 	 {Flowformer: Linearizing Transformers with Conservation Flows},
  author =       {Wu, Haixu and Wu, Jialong and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v162/wu22m.html}
}

@inproceedings{
chen2022pixelated,
title={Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
author={Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Nfl-iXa-y7R}
}

@InProceedings{pmlr-v162-wang22l,
  title = 	 {What Dense Graph Do You Need for Self-Attention?},
  author =       {Wang, Yuxin and Lee, Chu-Tak and Guo, Qipeng and Yin, Zhangyue and Zhou, Yunhua and Huang, Xuanjing and Qiu, Xipeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v162/wang22l.html}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{ou2022learning,
  title={Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference},
  author={Ou, Zijing and Xu, Tingyang and Su, Qinliang and Li, Yingzhen and Zhao, Peilin and Bian, Yatao},
  journal={arXiv preprint arXiv:2203.01693},
  year={2022}
}


@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{roy2021routing,
    title = "Efficient Content-Based Sparse Attention with Routing Transformers",
    author = "Roy, Aurko  and
      Saffar, Mohammad  and
      Vaswani, Ashish  and
      Grangier, David",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    url = "https://aclanthology.org/2021.tacl-1.4",
    doi = "10.1162/tacl_a_00353",
    pages = "53--68"
}
@article{daras2020smyrf,
  title={SMYRF-Efficient Attention using Asymmetric Clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@InProceedings{yi2020sinkhorn,
  title = 	 {Sparse {S}inkhorn Attention},
  author =       {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9438--9447},
  year = 	 {2020},
  editor = 	 {III, Hal DaumÃ© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tay20a/tay20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tay20a.html}
}

@inproceedings{
Kitaev2020reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}
@inproceedings{carion2020detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={arXiv preprint arXiv:2106.01345},
  year={2021}
}
@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
@InProceedings{touvron21adeit,
  title = 	 {Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10347--10357},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/touvron21a.html}
}

@inproceedings{
dosovitskiy2021vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}
@article{raffel2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@inproceedings{Pearl1989ProbabilisticRI,
  title={Probabilistic reasoning in intelligent systems - networks of plausible inference},
  author={J. Pearl},
  booktitle={Morgan Kaufmann series in representation and reasoning},
  year={1989}
}
@article{Wellman1993ExplainingA,
  title={Explaining 'Explaining Away'},
  author={Michael P. Wellman and M. Henrion},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={1993},
  volume={15},
  pages={287-292}
}
@article{abc,
       author = {{Peng}, Hao and {Kasai}, Jungo and {Pappas}, Nikolaos and {Yogatama}, Dani and {Wu}, Zhaofeng and {Kong}, Lingpeng and {Schwartz}, Roy and {Smith}, Noah A.},
        title = "{ABC: Attention with Bounded-memory Control}",
      journal = {arXiv e-prints arXiv:2110.02488},
         year = 2021,
}
@inproceedings{
yang2018breaking,
title={Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
author={Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkwZSG-CZ},
}
@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}
@InProceedings{vae3, 
  title = {Doubly Stochastic Variational Bayes for non-Conjugate Inference}, 
  author = {Michalis Titsias and Miguel LÃ¡zaro-Gredilla}, 
  booktitle = {Proceedings of the 31st International Conference on Machine Learning}, 
  pages = {1971--1979}, 
  year = {2014}, 
  volume = {32}, 
}
@InProceedings{vae2, 
  title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models}, 
  author = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra}, 
  booktitle = {Proceedings of the 31st International Conference on Machine Learning}, 
  pages = {1278--1286}, 
  year = {2014}, 
  volume = {32}, 
}
@article{vae1,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1312.6114K},
}
@article{blei2017variational,
  title={Variational inference: A review for statisticians},
  author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  journal={Journal of the American statistical Association},
  volume={112},
  number={518},
  pages={859--877},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{em,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}
@article{liu2020very,
  title={Very deep transformers for neural machine translation},
  author={Liu, Xiaodong and Duh, Kevin and Liu, Liyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2008.07772},
  year={2020}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{snderby2016ladder,
 author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {3738--3746},
 publisher = {Curran Associates, Inc.},
 title = {Ladder Variational Autoencoders},
 url = {https://proceedings.neurips.cc/paper/2016/file/6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf},
 volume = {29},
 year = {2016}
}

@InProceedings{nag2013, 
title = {On the importance of initialization and momentum in deep learning}, 
author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, 
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1139--1147}, 
year = {2013}, 
url = {http://proceedings.mlr.press/v28/sutskever13.html}
}
@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}
@inproceedings{
baevski2018adaptive,
title={Adaptive Input Representations for Neural Language Modeling},
author={Alexei Baevski and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByxZX20qFQ},
}
@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}
@article{cohen2008logistic,
  title={Logistic normal priors for unsupervised probabilistic grammar induction},
  author={Cohen, Shay and Gimpel, Kevin and Smith, Noah A},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  pages={321--328},
  year={2008}
}
@article{blei2006correlated,
  title={Correlated topic models},
  author={Blei, David and Lafferty, John},
  journal={Advances in neural information processing systems},
  volume={18},
  pages={147},
  year={2006}
}
@inproceedings{ranganath2016hierarchical,
  title={Hierarchical variational models},
  author={Ranganath, Rajesh and Tran, Dustin and Blei, David},
  booktitle={International Conference on Machine Learning},
  pages={324--333},
  year={2016}
}
@inproceedings{michel-16heads-2019-neurips,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {14014--14024},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}
@inproceedings{xie2016diversity,
  title={Diversity-promoting bayesian learning of latent variable models},
  author={Xie, Pengtao and Zhu, Jun and Xing, Eric},
  booktitle={International Conference on Machine Learning},
  pages={59--68},
  year={2016}
}
@InProceedings{liu2021swin,
    author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
    title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10012-10022}
}
@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}
@article{deng2018latent,
  title={Latent alignment and variational attention},
  author={Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9712--9724},
  year={2018}
}
@article{cordonnier2020multi,
  title={Multi-Head Attention: Collaborate Instead of Concatenate},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.16362},
  year={2020}
}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@inproceedings{mikolov2010ptb,
  author = {Mikolov, Tomas and KarafiÃ¡t, Martin and Burget, LukÃ¡s and CernockÃ½, Jan and Khudanpur, Sanjeev},
  booktitle = {INTERSPEECH},
  pages = {1045-1048},
  publisher = {ISCA},
  title = {Recurrent neural network based language model.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10},
  year = {2010}
}

@inproceedings{bojar2014findings,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}
@inproceedings{cettolo2014report,
  title={Report on the 11th iwslt evaluation campaign, iwslt 2014},
  author={Cettolo, Mauro and Niehues, Jan and St{\"u}ker, Sebastian and Bentivogli, Luisa and Federico, Marcello},
  booktitle={Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam},
  volume={57},
  year={2014}
}
@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={A. Radford and Jeffrey Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}
@inproceedings{mha2019neurips,
 author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {68--80},
 publisher = {Curran Associates, Inc.},
 title = {Stand-Alone Self-Attention in Vision Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14014--14024},
  year={2019}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@ARTICLE{he2020realformer,
       author = {{He}, Ruining and {Ravula}, Anirudh and {Kanagal}, Bhargav and {Ainslie}, Joshua},
        title = "{RealFormer: Transformer Likes Residual Attention}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2020,
        month = dec,
          eid = {arXiv:2012.11747},
        pages = {arXiv:2012.11747},
archivePrefix = {arXiv},
       eprint = {2012.11747},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201211747H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

 @article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{el2021xcit,
  title={XCiT: Cross-Covariance Image Transformers},
  author={El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and others},
  journal={arXiv preprint arXiv:2106.09681},
  year={2021}
}

@article{botev2024recurrentgemma,
  title={RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author={Botev, Aleksandar and De, Soham and Smith, Samuel L and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and others},
  journal={arXiv preprint arXiv:2404.07839},
  year={2024}
}
@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}
@article{team2024jamba1-5,
  title={Jamba-1.5: Hybrid Transformer-Mamba Models at Scale},
  author={Lenz, Jamba Team: Barak and Arazi, Alan and Bergman, Amir and Manevich, Avshalom and Peleg, Barak and Aviram, Ben and Almagor, Chen and Fridman, Clara and Padnos, Dan and others},
  journal={arXiv preprint arXiv:2408.12570},
  year={2024}
}
@article{waleffe2024empiricalmamba,
  title={An Empirical Study of Mamba-based Language Models},
  author={Waleffe, Roger and Byeon, Wonmin and Riach, Duncan and Norick, Brandon and Korthikanti, Vijay and Dao, Tri and Gu, Albert and Hatamizadeh, Ali and Singh, Sudhakar and Narayanan, Deepak and others},
  journal={arXiv preprint arXiv:2406.07887},
  year={2024}
}
@article{zuo2024falconmamba,
  title={Falcon Mamba: The First Competitive Attention-free 7B Language Model}, 
  author={Jingwei Zuo and Maksim Velikanov and Dhia Eddine Rhaiem and Ilyas Chahed and Younes Belkada and Guillaume Kunsch and Hakim Hacid},
  journal={arXiv preprint arXiv:2410.05355},
  year={2024},
}
@article{ren2024samba,
  title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
  author={Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
  journal={arXiv preprint arXiv:2406.07522},
  year={2024}
}
@article{peng2024rwkv-eagle-finch,
  title={Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Ferdinan, Teddy and Hou, Haowen and Kazienko, Przemys{\l}aw and others},
  journal={arXiv preprint arXiv:2404.05892},
  year={2024}
}
@article{glorioso2024zamba,
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}

@article{mercat2024mamba-rw,
  title={Linearizing Large Language Models},
  author={Mercat, Jean and Vasiljevic, Igor and Keh, Sedrick and Arora, Kushal and Dave, Achal and Gaidon, Adrien and Kollar, Thomas},
  journal={arXiv preprint arXiv:2405.06640},
  year={2024}
}
@article{wang2024mamba-in-llama,
  title={The mamba in the llama: Distilling and accelerating hybrid models},
  author={Wang, Junxiong and Paliotta, Daniele and May, Avner and Rush, Alexander M and Dao, Tri},
  journal={arXiv preprint arXiv:2408.15237},
  year={2024}
}
@article{bick2024transformers-to-ssms,
  title={Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models},
  author={Bick, Aviv and Li, Kevin Y and Xing, Eric P and Kolter, J Zico and Gu, Albert},
  journal={arXiv preprint arXiv:2408.10189},
  year={2024}
}

@article{gu2024olmes,
  title={{OLMES}: A Standard for Language Model Evaluations},
  author={Gu, Yuling and Tafjord, Oyvind and Kuehl, Bailey and Haddad, Dany and Dodge, Jesse and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2406.08446},
  year={2024}
}

@article{clark2018arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@inproceedings{clark2019boolq,
    title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    author = "Clark, Christopher  and
      Lee, Kenton  and
      Chang, Ming-Wei  and
      Kwiatkowski, Tom  and
      Collins, Michael  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    url = "https://aclanthology.org/N19-1300",
}
@inproceedings{sap2019socialiqa,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    url = "https://aclanthology.org/D19-1454",
}
@inproceedings{zellers2019hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    url = "https://aclanthology.org/P19-1472",
}
@inproceedings{talmor2019commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    url = "https://aclanthology.org/N19-1421",
}
@inproceedings{mihaylov2018openbookqa,
    title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    author = "Mihaylov, Todor  and
      Clark, Peter  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    year = "2018",
    url = "https://aclanthology.org/D18-1260",
}

@inproceedings{
    hendrycks2021mmlu,
    title={Measuring Massive Multitask Language Understanding},
    author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@article{bisk2020piqa, 
    title={PIQA: Reasoning about Physical Commonsense in Natural Language},  
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6239},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Bisk, Yonatan and Zellers, Rowan and Le bras, Ronan and Gao, Jianfeng and Choi, Yejin}, 
    year={2020} 
}
@article{sakaguchi2020winograde, 
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6399}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin}, 
    year={2020},
}
@inproceedings{
hendrycks2021math,
title={Measuring Mathematical Problem Solving With the {MATH} Dataset},
author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=7Bywt2mQsCe}
}
@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{chen2021humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{austin2021mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@inproceedings{
liu2023evalplus,
title={Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and LINGMING ZHANG},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=1qvx610Cu7}
}
@article{yehudai2024can,
  title={When Can Transformers Count to n?},
  author={Yehudai, Gilad and Kaplan, Haim and Ghandeharioun, Asma and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2407.15160},
  year={2024}
}
@article{wang2024stringllm,
  title={StringLLM: Understanding the String Processing Capability of Large Language Models},
  author={Wang, Xilong and Fu, Hao and Wang, Jindong and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2410.01208},
  year={2024}
}
@article{shin2024large,
  title={Large language models lack understanding of character composition of words},
  author={Shin, Andrew and Kaneko, Kunitake},
  journal={arXiv preprint arXiv:2405.11357},
  year={2024}
}
@article{xu2024llm,
  title={LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems},
  author={Xu, Nan and Ma, Xuezhe},
  journal={arXiv preprint arXiv:2410.14166},
  year={2024}
}
@article{chai2024tokenization,
  title={Tokenization Falling Short: The Curse of Tokenization},
  author={Chai, Yekun and Fang, Yewei and Peng, Qiwei and Li, Xuhong},
  journal={arXiv preprint arXiv:2406.11687},
  year={2024}
}
@article{pagnoni2024blt,
  title={Byte Latent Transformer: Patches Scale Better Than Tokens},
  author={Pagnoni, Artidoro and Pasunuru, Ram and Rodriguez, Pedro and Nguyen, John and Muller, Benjamin and Li, Margaret and Zhou, Chunting and Yu, Lili and Weston, Jason and Zettlemoyer, Luke and others},
  journal={arXiv preprint arXiv:2412.09871},
  year={2024}
}
@misc{lozhkov2024starcoder2,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas KrauÃŸ and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos MuÃ±oz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      journal={arXiv preprint arXiv:2402.19173},
      year={2024}
}

@article{phan2024exactbyteprob,
  title={Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles},
  author={Phan, Buu and Amos, Brandon and Gat, Itai and Havasi, Marton and Muckley, Matthew and Ullrich, Karen},
  journal={arXiv preprint arXiv:2410.09303},
  year={2024}
}
@article{lundberg2023tokenhealing,
  title={The art of prompt design: Prompt boundaries and token healing},
  author={Lundberg, Scott and Ribeiro, Marco Tulio},
  journal={Medium},
  year={2023},
  url={https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38}
}
@article{dagan2024getting,
  title={Getting the most out of your tokenizer for pre-training and domain adaptation},
  author={Dagan, Gautier and Synnaeve, Gabriel and Roziere, Baptiste},
  journal={arXiv preprint arXiv:2402.01035},
  year={2024}
}
@inproceedings{pimentel2024exactwordprob,
    title = "How to Compute the Probability of a Word",
    author = "Pimentel, Tiago  and
      Meister, Clara",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    year = "2024",
    url = "https://aclanthology.org/2024.emnlp-main.1020"
}
@article{vieira2024language,
  title={From language models over tokens to language models over characters},
  author={Vieira, Tim and LeBrun, Ben and Giulianelli, Mario and Gastaldi, Juan Luis and DuSell, Brian and Terilla, John and O'Donnell, Timothy J and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2412.03719},
  year={2024}
}
@misc{microsoft2023guidance,
   author = {Microsoft},
   title = {Guidance},
   url = {https://github.com/microsoft/guidance},
   year={2023}
}
@misc{rumbelow2023solidgoldmagikarp,
  title={SolidGoldMagikarp (plus, prompt generation)},
  author={Rumbelow, Jessica and Watkins, Matthew},
  booktitle={AI ALIGNMENT FORUM},
  url={https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation},
  year={2023}
}
@article{yang2024problematictokens,
  title={Problematic Tokens: Tokenizer Bias in Large Language Models},
  author={Yang, Jin and Wang, Zhiqiang and Lin, Yanbin and Zhao, Zunduo},
  journal={arXiv preprint arXiv:2406.11214},
  year={2024}
}
@article{wang2024tokenizationmatters,
  title={Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization},
  author={Wang, Dixuan and Li, Yanda and Jiang, Junyuan and Ding, Zepeng and Jiang, Guochao and Liang, Jiaqing and Yang, Deqing},
  journal={arXiv preprint arXiv:2405.17067},
  year={2024}
}
@inproceedings{geiping2024coercing,
title={Coercing {LLM}s to do and reveal (almost) anything},
author={Jonas Geiping and Alex Stein and Manli Shu and Khalid Saifullah and Yuxin Wen and Tom Goldstein},
booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
year={2024},
url={https://openreview.net/forum?id=Y5inHAjMu0}
}
@article{huang2024opencoder,
  title={Opencoder: The open cookbook for top-tier code large language models},
  author={Huang, Siming and Cheng, Tianhao and Liu, Jason Klein and Hao, Jiaran and Song, Liuyihan and Xu, Yang and Yang, J and Liu, JH and Zhang, Chenchen and Chai, Linzheng and others},
  journal={arXiv preprint arXiv:2411.04905},
  year={2024}
}
@misc{ai22024olmo2,
  author = {AI2},
  title = {OLMo 2: The best fully open language model to date},
  year = {2024},
  url = {https://allenai.org/blog/olmo2},
}

@inproceedings{schuhmann2022laionb,
title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}
}
@inproceedings{gadre2023datacomp,
title={DataComp: In search of the next generation of multimodal datasets},
author={Samir Yitzhak Gadre and Gabriel Ilharco and Alex Fang and Jonathan Hayase and Georgios Smyrnis and Thao Nguyen and Ryan Marten and Mitchell Wortsman and Dhruba Ghosh and Jieyu Zhang and Eyal Orgad and Rahim Entezari and Giannis Daras and Sarah M Pratt and Vivek Ramanujan and Yonatan Bitton and Kalyani Marathe and Stephen Mussmann and Richard Vencu and Mehdi Cherti and Ranjay Krishna and Pang Wei Koh and Olga Saukh and Alexander Ratner and Shuran Song and Hannaneh Hajishirzi and Ali Farhadi and Romain Beaumont and Sewoong Oh and Alex Dimakis and Jenia Jitsev and Yair Carmon and Vaishaal Shankar and Ludwig Schmidt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=dVaWCDMBof}
}
@InProceedings{mobileclip2024,
  author = {Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel},
  title = {MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
}
@inproceedings{laurencon2023obelics,
title={{OBELICS}: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Leo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=SKN2hflBIZ}
}
@misc{li2024llavanext-ablations,
    title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
    url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
    author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
    year={2024}
}
@InProceedings{wortsman22modelsoups,
  title = 	 {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author =       {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v162/wortsman22a.html}
}
@InProceedings{yang2024gla,
  title = 	 {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author =       {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024},
  url = 	 {https://proceedings.mlr.press/v235/yang24ab.html}
}
@inproceedings{
qin2023hgrn,
title={Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
author={Zhen Qin and Songlin Yang and Yiran Zhong},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=P1TCHxJwLB}
}
@inproceedings{
qin2024hgrn2,
title={{HGRN}2: Gated Linear {RNN}s with State Expansion},
author={Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=y6SqbJfCSk}
}
@inproceedings{
qin2022cosformer,
title={cosFormer: Rethinking Softmax In Attention},
author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}
@inproceedings{qin2022devil,
    title = "The Devil in Linear Transformer",
    author = "Qin, Zhen  and
      Han, Xiaodong  and
      Sun, Weixuan  and
      Li, Dongxu  and
      Kong, Lingpeng  and
      Barnes, Nick  and
      Zhong, Yiran",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
}
@article{sun2023retnet,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}
@article{kacham2023polysketchformer,
  title={Polysketchformer: Fast transformers via sketches for polynomial kernels},
  author={Kacham, Praneeth and Mirrokni, Vahab and Zhong, Peilin},
  journal={arXiv preprint arXiv:2310.01655},
  year={2023}
}
@inproceedings{
zhang2024hedgehog,
title={The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
author={Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Re},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=4g02l2N2Nx}
}

@InProceedings{arora2024based,
  title = 	 {Simple linear attention language models balance the recall-throughput tradeoff},
  author =       {Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zou, James and Rudra, Atri and Re, Christopher},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024},
  url = 	 {https://proceedings.mlr.press/v235/arora24a.html},
}
@inproceedings{mao2022fastweights,
    title = "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
    author = "Mao, Huanru Henry",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    url = "https://aclanthology.org/2022.emnlp-main.697/"
}

@InProceedings{dao2024mamba2,
  title = 	 {Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author =       {Dao, Tri and Gu, Albert},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024},
  url = 	 {https://proceedings.mlr.press/v235/dao24a.html},
}
@inproceedings{gu2024mamba,
title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
author={Albert Gu and Tri Dao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=tEYskw1VY2}
}
@inproceedings{
beck2024xlstm,
title={x{LSTM}: Extended Long Short-Term Memory},
author={Maximilian Beck and Korbinian P{\"o}ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael K Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ARAxPPIAhq}
}
@inproceedings{zhang2024gsa,
title={Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
author={Yu Zhang and Songlin Yang and Rui-Jie Zhu and Yue Zhang and Leyang Cui and Yiqiao Wang and Bolun Wang and Freda Shi and Bailin Wang and Wei Bi and Peng Zhou and Guohong Fu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=jY4PhQibmg}
}
@inproceedings{
gu2022s4,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}
@article{
de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  year={2024}
}
@inproceedings{
gu2021lssl,
title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers},
author={Albert Gu and Isys Johnson and Karan Goel and Khaled Kamal Saab and Tri Dao and Atri Rudra and Christopher Re},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=yWd42CWN3c}
}
@inproceedings{
smith2023s5,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}
@inproceedings{
ma2023mega,
title={Mega: Moving Average Equipped Gated Attention},
author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qNLe3iq2El}
}
@article{ma2024megalodon,
  title={Megalodon: Efficient llm pretraining and inference with unlimited context length},
  author={Ma, Xuezhe and Yang, Xiaomeng and Xiong, Wenhan and Chen, Beidi and Yu, Lili and Zhang, Hao and May, Jonathan and Zettlemoyer, Luke and Levy, Omer and Zhou, Chunting},
  journal={arXiv preprint arXiv:2404.08801},
  year={2024}
}
@inproceedings{
hasani2023liquid,
title={Liquid Structural State-Space Models},
author={Ramin Hasani and Mathias Lechner and Tsun-Hsuan Wang and Makram Chahine and Alexander Amini and Daniela Rus},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=g4OTKRKfS7R}
}
@inproceedings{
gupta2022dss,
title={Diagonal State Spaces are as Effective as Structured State Spaces},
author={Ankit Gupta and Albert Gu and Jonathan Berant},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=RjS0j6tsSrf}
}
@inproceedings{
likhosherstov2023favorsharp,
title={Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel},
author={Valerii Likhosherstov and Krzysztof Marcin Choromanski and Kumar Avinava Dubey and Frederick Liu and Tamas Sarlos and Adrian Weller},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S0xrBMFihS}
}
@inproceedings{
likhosherstov2022chefs,
title={Chefs' Random Tables: Non-Trigonometric Random Features},
author={Valerii Likhosherstov and Krzysztof Marcin Choromanski and Kumar Avinava Dubey and Frederick Liu and Tamas Sarlos and Adrian Weller},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=vRwCvlvd8eA}
}

@InProceedings{choromanski2024flt,
  title = 	 {Learning a {F}ourier Transform for Linear Relative Positional Encodings in Transformers},
  author =       {Choromanski, Krzysztof and Li, Shanda and Likhosherstov, Valerii and Avinava Dubey, Kumar and Luo, Shengjie and He, Di and Yang, Yiming and Sarlos, Tamas and Weingarten, Thomas and Weller, Adrian},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2024},
  url = 	 {https://proceedings.mlr.press/v238/choromanski24a.html},
}

@InProceedings{reid2023simrf,
  title = 	 {Simplex Random Features},
  author =       {Reid, Isaac and Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Weller, Adrian},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/reid23a.html},
}

@InProceedings{choromanski2023taming,
  title = 	 {Taming graph kernels with random features},
  author =       {Choromanski, Krzysztof Marcin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/choromanski23a.html},
}
@article{an2024string,
  title={Why Does the Effective Context Length of LLMs Fall Short?},
  author={An, Chenxin and Zhang, Jun and Zhong, Ming and Li, Lei and Gong, Shansan and Luo, Yao and Xu, Jingjing and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2410.18745},
  year={2024}
}
@inproceedings{an2024leval,
    title = "{L}-Eval: Instituting Standardized Evaluation for Long Context Language Models",
    author = "An, Chenxin  and
      Gong, Shansan  and
      Zhong, Ming  and
      Zhao, Xingjian  and
      Li, Mukai  and
      Zhang, Jun  and
      Kong, Lingpeng  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.776/",
}
@article{an2024dca,
  title={Training-free long-context scaling of large language models},
  author={An, Chenxin and Huang, Fei and Zhang, Jun and Gong, Shansan and Qiu, Xipeng and Zhou, Chang and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2402.17463},
  year={2024}
}
@inproceedings{zhang2023cab,
  title={Cab: comprehensive attention benchmarking on long sequence modeling},
  author={Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  year={2023},
}
@article{yang2024deltanet,
  title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  journal={arXiv preprint arXiv:2406.06484},
  year={2024}
}
@article{munkhdalai2024infini-attn,
  title={Leave no context behind: Efficient infinite context transformers with infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  year={2024}
}
@article{kallini2024mrt5,
  title={MrT5: Dynamic Token Merging for Efficient Byte-level Language Models},
  author={Kallini, Julie and Murty, Shikhar and Manning, Christopher D and Potts, Christopher and Csord{\'a}s, R{\'o}bert},
  journal={arXiv preprint arXiv:2410.20771},
  year={2024}
}
@article{zhang2024mapneo,
  title={Map-neo: Highly capable and transparent bilingual large language model series},
  author={Zhang, Ge and Qu, Scott and Liu, Jiaheng and Zhang, Chenchen and Lin, Chenghua and Yu, Chou Leuang and Pan, Danny and Cheng, Esther and Liu, Jie and Lin, Qunshu and others},
  journal={arXiv preprint arXiv:2405.19327},
  year={2024}
}
@article{olmo20242olmo2,
  title={2 OLMo 2 Furious}, 
  author={Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V. Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},
  journal={arXiv preprint arXiv:2501.00656},
  year={2025},
}
@inproceedings{groeneveld2024olmo,
    title = "{OLM}o: Accelerating the Science of Language Models",
    author = "Groeneveld, Dirk  and
      Beltagy, Iz  and
      Walsh, Evan  and
      Bhagia, Akshita  and
      Kinney, Rodney  and
      Tafjord, Oyvind  and
      Jha, Ananya  and
      Ivison, Hamish  and
      Magnusson, Ian  and
      Wang, Yizhong  and
      Arora, Shane  and
      Atkinson, David  and
      Authur, Russell  and
      Chandu, Khyathi  and
      Cohan, Arman  and
      Dumas, Jennifer  and
      Elazar, Yanai  and
      Gu, Yuling  and
      Hessel, Jack  and
      Khot, Tushar  and
      Merrill, William  and
      Morrison, Jacob  and
      Muennighoff, Niklas  and
      Naik, Aakanksha  and
      Nam, Crystal  and
      Peters, Matthew  and
      Pyatkin, Valentina  and
      Ravichander, Abhilasha  and
      Schwenk, Dustin  and
      Shah, Saurabh  and
      Smith, William  and
      Strubell, Emma  and
      Subramani, Nishant  and
      Wortsman, Mitchell  and
      Dasigi, Pradeep  and
      Lambert, Nathan  and
      Richardson, Kyle  and
      Zettlemoyer, Luke  and
      Dodge, Jesse  and
      Lo, Kyle  and
      Soldaini, Luca  and
      Smith, Noah  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.841/",
}
